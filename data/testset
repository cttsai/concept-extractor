<abstract> [A Lexical Discrimination A] With The Italian Version Of [T WordNet T] . We present a prototype of the Italian version of  [T WORDNET T] , a general computational lexical resource . Some relevant extensions are discussed to make it usable for [A parsing A] : in particular we add [T verbal selectional restrictions T] to make lexical discrimination effective . Italian [T WORDNET T] has been coupled with a [A parser A] and a number of experiments have been performed to individuate the methodology with the best trade-off between disambiguation rate and        precision . Results confirm intuitive hypothesis on the role of [T selectional restrictions T] and show evidences for a WORDNET-Iike organization of lexical senses . </abstract>
<abstract> A [T Surface-Based T] Approach To Identifying [A Discourse Markers And Elementary Textual Units A] In Unrestricted Texts . I present a [T surface-based T]  algorithm that employs knowledge of [T cue phrase T] usages in order to determine automatically [A clause boundaries and discourse markers A] in unrestricted natural language texts . The knowledge was derived from a comprehensive corpus analysis . </abstract>
<abstract> [A Direct Orthographical Mapping For Machine Transliteration A] . [A Machine transliteration\/back-transliteration A] plays an important role in many multilingual speech and language applications . In this paper , a novel framework for [A machine transliteration\/backtransliteration A] that allows us to carry out direct [A orthographical mapping A] ( DOM ) between two different languages is presented . Under this framework , a [T joint source-channel T] transliteration model , also called [T n-gram T] transliteration model ( [T ngram T] TM ) , is further proposed to model the [A transliteration A]       process . We evaluate the proposed methods through several [A transliteration\/backtransliteration A] experiments for English\/Chinese and English\/Japanese language pairs . Our study reveals that the proposed method not only reduces an extensive system development effort but also improves the [A transliteration A] accuracy significantly . </abstract>
<abstract> Exploiting [T Headword Dependency T] And [T Predictive Clustering T] For [A Language Modeling A] . This paper presents several practical ways of incorporating [T linguistic structure T] into [A   language models A] . A [T headword detector T] is first applied to detect the headword of each phrase in a sentence . A [T permuted headword trigram  T] model ( [T PHTM T] ) is then generated from the annotated corpus . Finally , [T PHTM T] is extended to a [T cluster PHTM  T] ( [T C-PHTM T] )  by defining clusters for similar words in the corpus . We evaluated the proposed models on the realistic application of [A Japanese Kana-Kanji conversion A] . Experiments show that [T C-PHTM T] achieves 15 % error rate reduction over the word trigram model . This demonstrates that the use of simple methods such as the [T headword trigram T] and [T predictive clustering T] can effectively capture long distance word dependency , and substantially outperform a word trigram model . </abstract>
<abstract> Towards A Simple And Accurate [T Statistical  T] Approach To Learning  [A Translation Relationships Among Words A] . We report on a project to derive [A word translation A]  relationships automatically from parallel corpora . Our effort is distinguished by the use of simpler , faster models than those used in previous high-accuracy approaches . Our methods achieve accuracy on [A singleword translations A] that seems comparable to any work previously reported , up to nearly 60 % coverage of word types , and they perform particularly well on a class of multi-word compounds of special interest to our translation effort . </abstract>
<abstract> Towards [A Spoken-Document Retrieval A] For The Internet : [T Lattice Indexing T] For [A Large-Scale Web-Search Architectures A] . [A Large-scale web-search engines A] are generally designed for linear text . The linear text representation is suboptimal for audio search , where accuracy can be significantly improved if the search includes alternate recognition candidates , commonly represented as word lattices . This paper proposes a method for [T indexing word lattices T] that is suitable for [A large-scale web-search engines A] , requiring only limited code changes . The proposed method , called [T Time-based Merging for Indexing  T] ( [T TMI T] ) , first converts the word lattice to a posterior-probability representation and then merges word hypotheses with similar time boundaries to reduce the index size . Four alternative approximations are presented , which differ in index size and the strictness of the phrase-matching constraints . Results are presented for three types of typical web audio content , podcasts , video clips , and online lectures , for [A phrase spotting and relevance ranking A] . Using [T TMI indexes T] that are only five times larger than corresponding lineartext indexes , [A phrase spotting A] was improved over searching top-1 transcripts by 25-35 % , and relevance ranking by 14 % , at only a small loss compared to unindexed lattice search . </abstract>
<abstract> A Japanese  [A Predicate Argument Structure Analysis A] using [T Decision Lists T] . This paper describes a new automatic method for Japanese [A predicate argument structure analysis A] . The method learns relevant features to assign [A case roles to the argument of the target predicate A] using the features of the words located closest to the target predicate under various constraints such as [T dependency types , words , semantic categories , parts of speech , functional words and predicate voices T] . We constructed [T decision lists T] in which these featuresweresortedbytheirlearnedweights . Using our method , we integrated the tasks of [A semantic role labeling A] and [A zero-pronoun identification A] , and achieved a 17 % improvement compared with a baseline method in a sentence level performance analysis . </abstract>
<abstract> [T Statistical  T][A Machine Translation A] Using [T Coercive Two-Level Syntactic Transduction T] . We define , implement and evaluate a novel model for [T statistical  T][A  machine translation A] , which is based on [T shallow syntactic analysis ( part-of-speech tagging and phrase chunking ) T] in both the source and target languages . It is able to model long-distance constituent motion and other syntactic phenomena without requiring a full parse in either language . We also examine aspects of [T lexical transfer T] , suggesting and exploring a concept of translation coercion across [T parts of speech T] , as well as a transfer model based on [T lemma-to-lemma T] translation probabilities , which holds promise for improving [A machine translation A] of low-density languages . Experiments are performed in both Arabic-to-English and French-to-English translation demonstrating the efficacy of the proposed techniques . Performance is automatically evaluated via the Bleu score metric . </abstract>
<abstract> Modelling User Satisfaction And Student Learning In A [A Spoken Dialogue Tutoring  A] System With Generic , Tutoring , And [T User Affect T] Parameters . We investigate using the PARADISE framework to develop [T predictive T] models of system performance in our [A spoken dialogue tutoring  A] system . We represent performance with two metrics : user satisfaction and student learning . We train and test [T predictive T] models of these metrics in our tutoring system corpora . We predict user satisfaction with 2 parameter types : 1 ) system-generic , and 2 ) tutoringspeci c. To predict student learning , we also use a third type : 3 )[T  user affect T] . Alhough generic parameters are useful predictors of user satisfaction in other PARADISE applications , overall our parameters produce less useful user satisfaction models in our system . However , generic and tutoring-speci c parameters do produce useful models of student learning in our system . [T User affect T] parameters can increase the usefulness of these models . </abstract>
<abstract> EXEMPLARS : A Practical , Extensible Framework For [A Dynamic Text Generation A] . In this paper , we present EXEMPLARS , an [T object-oriented , rule-based  T] framework designed to support practical , [A dynamic text generation A] , emphasizing its novel features compared to . existing hybrid systems that mix template-style and more sophisticated techniques . These features - . include an extensible [T classification-based text planning T]  mechanism , a definition language that is a superset of the Java language , and advanced support for [T HTMIdSGML templates T] . </abstract>
<abstract> [T Multi-View Co-Training T] of [A Transliteration A] Model . This paper discusses a new approach to training of [A transliteration A] model from unlabeled data for [A transliteration A] extraction . We start with an inquiry into the formulation of [A transliteration A] model by considering different [A transliteration A] strategies as a [T multi-view T] problem , where each view exploits a natural division of [A transliteration A] features , such as [T phonemebased T] , [T grapheme-based T] or [T hybrid T] features . Then we introduce a [T multi-view Cotraining T] algorithm , which leverages compatible and partially uncorrelated information across different views to effectively boost the model from unlabeled data . Applying this algorithm to [A transliteration A] extraction , the results show that it not only circumvents the need of data labeling , but also achieves performance close to that of supervised learning , where manual labeling is required for all training samples . </abstract>
<abstract> [A Translating Treebank Annotation For Evaluation  A] . In this paper we discuss the need for corpora with a variety of annotations to provide suitable resources to [A evaluate A] different Natural Language Processing systems and to compare them . A [T supervised machine learning  T] technique is presented for [A translating corpora between syntactic formalisms A] and is applied to the task of [A translating the Penn Treebank annotation into a Categorial Grammar annotation A] . It is compared with a current alternative approach and results indicate annotation of broader coverage using a more compact grammar . </abstract>
<abstract> A [T Compression-Based T] Algorithm For Chinese [A Word Segmentation A] . Chinese is written without using spaces or other word delimiters . Although a text may be thought of as a corresponding sequence of words , there is considerable ambiguity in the placement of boundaries . Interpreting a text as a sequence of words is beneficial for some information retrieval and storage tasks : for example,full-text search , word-based compression , and keyphrase extraction . We describe a scheme that infers appropriate positions for [A word boundaries A] using an [T adaptive language model T] that is standard in text compression . It is trained on a corpus of presegmented text , and when applied to new text , interpolates [A word boundaries A] so as to maximize the compression obtained . This simple and general method performs well with respect to specialized schemes for Chinese  [A language segmentation A] . </abstract>
<abstract> [A Ranking Paraphrases A] in [T Context T] . We present a [T vector space model T] that supports the computation of appropriate [T  vector representations T] for words in [T context T] , and apply it to a [A paraphrase ranking A] task . An evaluation on the SemEval 2007 [A lexical substitution A] task data shows promising results : the model significantly outperforms a current state of the art model , and our treatment of context is effective . </abstract>
<abstract> A Multi-Purpose Interface To An [A On-Line Dictionary A] . We argue that there are two qualitatively different modes of using a [A machine-readable dictionary A] in the context of research in computational linguistics : batch processing of the source with the purpose of collating information for subsequent use by a natural language application , and placing the dictionary on-line in an environment which supports fast interactive access to data selected on the basis of a number of [T linguistic constraints T] . While it is the former mode of dictionary use which is characteristic of most computational linguistics work to date , it is the latter which has the potential of making maximal use of the information typically found in a machine-readable dictionary . We describe the mounting of the machine-readable source of the  Longman Dictionary of Contemporary English on a single user workstation to make it available as a development tool for a number of research projects . </abstract>
<abstract> An Empirical Study Of The Influence Of [T Argument Conciseness T] On [A Argument Effectiveness A] . We have developed a system that [A generates evaluative arguments A] that are tailored to the user , properly arranged and concise . We have also developed an evaluation framework in which the [A effectiveness of evaluative arguments A] can be measured with real users . This paper presents the results of a formal experiment we have performed in our framework to verify the influence of [T argument conciseness T] on [A argument effectiveness A] </abstract>
<abstract> [A Translating A] With [T Non-Contiguous Phrases T] . This paper presents a [T phrase-based statistical T] [A  machine translation  A] method , based on [T non-contiguous phrases T] , i.e. phrases with gaps . A method for producing such phrases from a word-aligned corpora is proposed . A statistical  [A  translation A] model is also presented that deals such phrases , as well as a [T training T] method based on the maximization of [A translation A] accuracy , as measured with the NIST evaluation metric . [A Translations A] are produced by means of a [T beam-search decoder T] . Experimental results are presented , that demonstrate how the proposed method allows to better generalize from the training data . </abstract>
<abstract> What To Do When Lexicalization Fails : [A Parsing A] German With [T Suffix Analysis And Smoothing T] . In this paper , we present an [T unlexicalized  T][A parser A] for German which employs [T smoothing and suffix analysis T] to achieve a labeled bracket F-score of 76.2 , higher than previously reported results on the NEGRA corpus . In addition to the high accuracy of the model , the use of [T smoothing T] in an [T unlexicalized T] [A parser A] allows us to better examine the interplay between [T smoothing T] and [A parsing A] results . </abstract>
<abstract> A Prototype Of A [A Grammar Checker A] For Czech . This paper describes the implementation of a prototype of a [A grammar based grammar checker A] for Czech and the basic ideas behind this implementation . The demo is implemented as an independent program cooperating with Microsoft Word . The [A grammar checker A] uses [T specialized grammar formalism T] which generally enables to check errors in languages with a very high degree of word order freedom . </abstract>
<abstract> [A Machine Translation A] Based On [T NLG From XML-DB T] . The purpose of this study is to propose a new method for [A machine translation A] . Wehave proceeded through with two projects for report generation ( Kittredge and Polguere , 2000 ) : Weather Forecast and Monthly Economic Report to be produced in four languages : English , Japanese , French , and German . Their input data is stored in XML-DB . We applied a three-stage pipelined architecture ( Reiter and Dale , 2000 ) , and each stage was implemented as [T XML transformation processes T] . Weregard XML stored data as language-neutral intermediate form and employ the so-called ` [T sublanguage T]  approach  ' ( Somers , 2000 ) . The [A machine translation A] process is implemented via [T XMLDB T] as a kind of interlingua approach instead of the conventional structure transfer approach . </abstract>
<abstract> Faster [A MT Decoding A] Through [T Pervasive Laziness T] . [T Syntax-based T] [A  MT A] systems have proven effective -- the models are compelling and show good room for improvement . However , decoding involves a slow search . We present a new [T lazy-search  T] method that obtains significant speedups over a strong baseline , with no loss in Bleu . </abstract>
<abstract> [A Answering Clinical Questions A] with [T Knowledge-Based and Statistical  T] Techniques . The combination of recent developments in [A question-answering A] research and the availability of unparalleled resources developed specifically for automatic semantic processing of text in the medical domain provides a unique opportunity to explore complex question answering in the domain of clinical medicine . This article presents a system designed to satisfy the information needs of physicians practicing [A evidence-based medicine A] . We have developed a series of [T knowledge extractors T] , which employ a combination of [T knowledge-based and statistical  T]  techniques , for automatically identifying [A  clinically relevant aspects of MEDLINE abstracts A] . These extracted elements serve as the input to an algorithm that scores the relevance of citations with respect to structured representations of information needs , in accordance with the principles of [A evidencebased medicine A] . Starting with an initial list of citations retrieved by PubMed , our system can bring relevant abstracts into higher ranking positions , and from these abstracts generate responses that directly answer physicians ' questions . We describe three separate evaluations : one focused on the accuracy of the knowledge extractors , one conceptualized as a document reranking task , and finally , an evaluation of answers by two physicians . Experiments on a collection of real-world clinical questions show that our approach significantly outperforms the already competitive PubMed baseline . </abstract>
<abstract> Japanese [A Dependency Parsing A] Using a [T Tournament T] Model . In Japanese [A dependency parsing A] , Kudo 's relative preference-based method ( Kudo and Matsumoto , 2005 ) outperforms both deterministic and probabilistic CKY-based parsing methods . In Kudo 's method , for each dependent word ( or chunk ) a loglinear model estimates relative preference of all other candidate words ( or chunks ) for being as its head . This can not be considered in the deterministic parsing methods . We propose an algorithm based on a [T tournament  T] model , in which the relative preferences are directly modeled by [T one-onone games in a step-ladder tournament T] . In an evaluation experiment with Kyoto Text Corpus Version 4.0 , the proposed method outperforms previous approaches , including the relative preference-based method . </abstract>
<abstract> Enriching Automated [A  Essay Scoring A] Using [T Discourse Marking T] . Electronic Essay Rater ( e-rater ) is a prototype automated [A  essay scoring A] system built at Educational Testing Service ( ETS ) that uses [T discourse marking T] , in addition to [T syntactic information and topical content vector T] analyses to automatically assign [A essay scores A] . This paper gives a general description ore-rater as a whole , but its emphasis is on the importance of[T  discourse marking and argument partitioning T] for annotating the argument structure of an essay . We show comparisons between two [T content vector analysis T] programs used to [A predict scores A] . EsscQ \/ ` Content and ArgContent . EsscnContent assigns scores to essays by using a standard cosine correlation that treats the essay like a '' ` bag of words . '' in that it does not consider word order . [T Ark , Content T] employs a novel [T content vector analysis T] approach for score assignment based on the individual arguments in an essay . The average agreement between ArgContent scores and human rater scores is 82 % . as compared to 69 % agreement between EssavContent and the human raters . These results suggest that discourse marking enriches e-rater 's scoring capability . When e-rater uses its whole set of predictive features , agreement with human rater scores ranges from 87 ° , \/ o - 94 % across the 15 sets of essa5 responses used in this study </abstract>
<abstract> Building Parallel Corpora For [A EContent Professionals A] . This paper reports on completed work carried out in the framework of the INTERA project , and specifically , on the production of [A multilingual resources A] ( [A LRs A] ) for [A eContent A] purposes . The paper presents the methodology adopted for the development of the corpus ( acquisition and processing of the textual data ) , discusses the divergence of the initial assumptions from the actual situation met during this procedure , and concludes with a summarization of the problems attested which undermine the viability of multilingual parallel corpora construction . </abstract>
<abstract> Mining of [T Parsed T] Data to Derive [A  Deverbal Argument Structure A] . The availability of large parsed corpora and improved computing resources now make it possible to extract vast amounts of lexical data . We describe the process of extracting structured data and several methods of deriving [A  argument structure mappings for deverbal nouns A] that significantly improves upon non-lexicalized rule-based methods . For a typical model , the F-measure of performance improves from a baseline of about 0.72 to 0.81 . </abstract>
<abstract> [A Document Re-Ranking A] Based On Automatically Acquired [T  Key Terms T] In Chinese  [A Information Retrieval A] . For [A Information Retrieval A] , users are more concerned about the precision of top ranking documents in most practical situations . In this paper , we propose a method to improve the precision of top N [A  ranking documents A] by reordering the retrieved documents from the initial retrieval . To reorder documents , we first automatically extract [T Global Key Terms T] from document set , then use extracted [T Global Key Terms T] to identify [T Local Key Terms T] in a single document or query topic , finally we make use of [T Local Key Terms T] in query and documents to reorder the initial ranking documents . The experiment with NTCIR3 CLIR dataset shows that an average 10 % -11 % improvement and 2 % -5 % improvement in precision can be achieved at top 10 and 100 ranking documents level respectively . </abstract>
<abstract> [A Lexical Disambiguation A] Using [T Simulated Annealing T] . The [A resolution of lexical ambiguity A] is important for most natural language processing tasks , and a range of computational techniques have been proposed for its solution . None of these has yet proven effective on a large scale . In this paper , we describe a method for [A lexical disambiguation of text A] using the definitions in a [T machine-readable dictionary T] together with the technique of [T simulated annealing T] . The method operates on complete sentences and attempts to select the optimal combinations of word senses for all the words in the sentence simultaneously . The words in the sentences may be any of the 28,000 headwords in Longman 's Dictionary of Contemporary English ( LDOCE ) and are disambiguated relative to the senses given in LDOCE . Our initial results on a sample set of 50 sentences are comparable to those of other researchers , and the fully automatic method requires no hand coding of lexical entries , or hand tagging of text . </abstract>
<abstract> A [T Generative  T] Model for [A Parsing Natural Language to Meaning Representations A] . In this paper , we present an algorithm for learning a [T generative T] model of natural language sentences together with their [A formal meaning representations with hierarchical structures A] . The model is applied to the task of [A mapping sentences to hierarchical representations A] of their underlying meaning . We introduce [T dynamic programming T] techniques for efficient training and decoding . In experiments , we demonstrate that the model , when coupled with a [T discriminative reranking T] technique , achieves state-of-the-art performance when tested on two publicly available corpora . The generative model degrades robustly when presented with instances that are different from those seen in training . This allows a notable improvement in recall compared to previous models . </abstract>
<abstract> The Application Of [T Two-Level T] Morphology To [A Non-Concatenative German Morphology A] . Introduction In this paper 2 we describe a [T hybrid  T] system for [A morphological analysis and synthesis A] . We call it hybrid because it consists of two separate parts interacting with each other in a welldefined way . The treatment of [A morphonology and nonoconcatenative morphology A] is based on the [T two-level T] approach originally proposed by Koskenniemi ( 1983 ) . For the concatenative part of morphosyntax ( i.e. affixation ) we make use of a [T grammar based on feature-unification T] . tloth parts rely on the same morph lexicon . Combinations of [T two-level T] morphology with t ` eature-based morphosyntactic grammars have already been proposed by several authors ( c.f. llear 1988a , Carson 1988 , G6rz & Paulus 1988 , Schiller & Steffens 1990 ) to overcome the shortcomings of the continuation-classes originally proposed by Koskenniemi ( 1983 ) and Karttunen ( 1983 ) for the description of morphosyntax . But up to now no linguistically ~ ; atisfying solution has been proposed for the treatment of non-concatenative morphology in : such a framework . In this paper we describe an extension to the model which will allow for the description of such phenomena . Namely we propose to restrict the applicability of two-level rules by providing them with filters in the form of feature structures . We demonstrate how a well-known problem of German [A morphology A] , so-called `` [A Umlautung A] '' , can be described in our approach in a linguistically motivated and efficient way . </abstract>
<abstract> [A Optimal Morphology A] . [A Optimal morphology  A] ( OM ) is a [T finite state T] formalism that unifies concepts from Optimality Theory ( OT , Prince ~ : Smolensky , 1993 ) and Declarative Phonology ( DP , Scobbie , Coleman Bird , 1996 ) to describe [A morphophonological alternations in inflectional morphology A] . Candidate sets are formalized by [T inviolable lexical constraints T] which map abstract morpheme signatures to allomorphs . Phonology is implemented as [T violable rankable constraints T] selecting optimal candidates from these . Both types of constraints are realized by [T finite state transducers T] . Using phonological data from Albanian it is shown that given a finite state lexicalization of candidate outputs for word forms OM allows more natural analyses than unviolable finite state constraints do . Two possible evaluation strategies for OM grammars are considered : the global evaluation procedure from E1lisou ( 1994 ) and a simple strategy of local constraint evaluation . While the OM-specific lexicalization of candidate sets allows straightforward generation and a simple method of [A morphological parsing A] even under global evaluation , local constraint evaluation is shown to be preferable empirically and to be formally more restrictive . The first point is illustrated by an account of directionality effects in some classical Mende data . A procedure is given that generates a finite state transducer simulating the effects of local constraint evaluation . Thus local as opposed to global evaluation ( Frank & Satta , 1998 ) seems to guarantee the finite-stateness of the input-output-mapping . </abstract>
<abstract> Getting to Know Moses : Initial Experiments on German-English [T Factored  T][A Translation A] . We present results and experiences from our experiments with [T phrase-based statistical T] [A  machine translation A] using [T Moses T] . The paper is based on the idea of using an offthe-shelf [T parser T] to supply linguistic information to a [T  factored T] [A translation A] model and compare the results of German ? English translation to the shared task baseline system based on word form . We report partial results for this model and results for two simplified setups . Our best setup takes advantage of the parser ? s [T lemmatization and decompounding T] . A qualitative analysis of compound translation shows that decompounding improves [A translation A] quality . </abstract>
<abstract> [T KU T] : [A Word Sense Disambiguation A] by [T Substitution T] . Data sparsity is one of the main factors that make [A word sense disambiguation A] ( [A WSD A] ) difficult . To overcome this problem we need to find effective ways to use resources other than sense labeled data . In this paper I describe a [A WSD A] system that uses a [T statistical language model  T] based on a large unannotated corpus . The model is used to evaluate the likelihood of various substitutes for a word in a given context . These likelihoods are then used to determine the best sense for the word in novel contexts . The resulting system participated in three tasks in the SemEval 2007 workshop . The WSD of prepositions task proved to be challenging for the system , possibly illustrating some of its limitations : e.g. not all words have good substitutes . The system achieved promising results for the English lexical sample and English lexical substitution tasks . </abstract>
<abstract> A Robust And [T Hybrid Deep-Linguistic Theory T] Applied To [A Large-Scale Parsing A] . Modern statistical parsers are robust and quite fast , but their output is relatively shallow when compared to formal grammar parsers . We suggest to extend [T statistical  T] approaches to a more deep-linguistic analysis while at the same time keeping the speed and low complexity of a [T statistical  T][A parser A] . The resulting parsing architecture suggested , implemented and evaluated here ishighlyrobustandhybridonanumberof levels , combining [T statistical and rule-based T] approaches ,[T  constituency and dependency grammar , shallow and deep processing T] , full and nearfull parsing . With its parsing speed of about 300,000 words per hour and state-of-the-art performance the parser is reliable for a number of large-scale applications discussed in the article . </abstract>
<abstract> [T Unsupervised Domain Relevance Estimation T] For [A Word Sense Disambiguation A] . This paper presents [T Domain Relevance Estimation ( DRE ) T] , a fully [T unsupervised text categorization T] technique based on the statistical estimation of the relevance of a text with respect to a certain category . We use a [T pre-de ned set of categories T] ( we call them domains ) which have been previously associated to [T WORDNET  T] word senses . Given a certain domain , DRE distinguishes between relevant and non-relevant texts by means of a [T Gaussian Mixture  T] model that describes the frequency distribution of domain words inside a large-scale corpus . Then , an [T Expectation Maximization T] algorithm computes the parameters that maximize the likelihood of the model on the empirical data . The correct identi cation of the domain of the text is a crucial point for [A Domain Driven Disambiguation A] , an [T unsupervised T] [A  Word Sense Disambiguation ( WSD ) A] methodology that makes use of only domain information . Therefore , DRE has been exploited and evaluated in the context of a [A WSD task A] . Results are comparable to those of state-ofthe-art unsupervised WSD systems and show that DRE provides an important contribution . </abstract>
<abstract> Capturing [A Out-Of-Vocabulary Words A] In Arabic Text . The increasing flow of information between languages has led to a rise in the frequency of non-native or loan words , where terms of one language appear transliterated in another . Dealing with such out of vocabulary words is essential for successful cross-lingual information retrieval . For example , techniques such as stemming should not be applied indiscriminately to all words in a collection , and so before any stemming , foreign words need to be identified . In this paper , we investigate three approaches for the identification of [A  foreign words A] in Arabic text :[T  lexicons , language patterns , and n-grams T] and present that results show that [T lexicon-based T] approaches outperform the other techniques . </abstract>
<abstract> [A Generation A] As A Solution To Its Own Problem . [A Natural language generation A] technology is now ripe for commercial exploitation , but one of the remaining bottlenecks is that of providing NLG • systems with user-friendly interfaces for Specifying the content of documents to be generated . We present here a new technique we have developed for providing such interfaces : [T WYSIWYM editing T] . [T WYSIWYM ( What You See Is What You Meant ) T] makes novel use of the system 's generator to provide a natural language input device which requires no NL interpretation ... - only NL generation . </abstract>
<abstract> Automatic [A Detection Of Text Genre A] . As the text databases available to users become larger and more heterogeneous , genre becomes increasingly important for computational linguistics as a complement to topical and structural principles of classification . We propose a theory of genres as bundles of facets , which correlate with various [T surface cues T] , and argue that [A genre detection A] based on [T surface cues T] is as successful as detection based on deeper structural properties . </abstract>
<abstract> [T Hierarchical Search T] for [A Parsing A] . Both [T coarse-to-fine T] and [T A ∗ T] [A parsing A] use simple grammars to guide search in complex ones . We compare the two approaches in a common , [T agenda-based  T] framework , demonstrating the tradeoffs and relative strengths of each method . Overall , [T coarse-to-fine T] is much faster for moderate levels of search errors , but below a certain threshold [T A ∗ T] is superior . In addition , we present the first experiments on [T hierarchical A ∗ T] [A parsing A] , in which computation of heuristics is itself guided by meta-heuristics . [T Multi-level hierarchies T] are helpful in both approaches , but are more effective in the coarseto-fine case because of accumulated slack in A ∗ heuristics . </abstract>
<abstract> Application Of [T Analogical  T] Modelling To [T Example Based T] [A  Machine Translation A] . This paper describes a [T self-modelling , incremental  T] algorithm for learning [A  translation rules  A] from existing bilingual corpora . The notions of [T supracontext T] and [T subcontext T] are extended to encompass bilingual information through simultaneous analogy on both source and target sentences and juxtaposition of corresponding results . [T Analogical  T]  modelling is performed during the learning phase and translation patterns are projected in a [T multi-dimensional analogical network T] . The proposed fi'amework was evaluated on a small training corpus providing promising results . Suggestions to improve system performance are </abstract>
<abstract> Large Scale Testing Of A [A Descriptive Phrase Finder A] . This paper describes an evaluation of an existing technique that locates [A sentences containing descriptions of a query word or phrase A] . The experiments expand on previous tests by exploring the effectiveness of the system when searching from a much larger document collection . The results showed the system working significantly better than when searching over smaller collections . The improvement was such , that a more stringent definition of what constituted a correct description was devised to better measure effectiveness . The results also pointed to potentially new forms of evidence that might be used in improving the location process . Keywords Information retrieval , descriptive phrases , WWW . </abstract>
<abstract> The [T infinite HMM T] for [T unsupervised T] [A  PoS tagging A] . We extend previous work on fully [T unsupervised T] [A part-of-speech tagging A] . Using a [T non-parametric version of the HMM T] , called the [T infinite HMM ( iHMM )  T], we address the problem of choosing the number of hidden states in [T unsupervised Markov models T] for [A PoS tagging A] . We experiment with two non-parametric priors , the [T Dirichlet and Pitman-Yor processes T] , on the Wall Street Journal dataset using a parallelized implementation of an [T iHMM inference T]  algorithm . We evaluate the results with a variety of clustering evaluation metrics and achieve equivalent or better performances than previously reported . Building on this promising result we evaluate the output of the [T unsupervised T] [A PoS tagger A] as a direct replacement for the output of a fully supervised PoS tagger for the task of [A shallow parsing A] and compare the two evaluations . </abstract>
<abstract> [T Centrality Measures T] In Text Mining : Prediction Of  [A Noun Phrases A] That Appear In Abstracts . In this paper , we study different[T  centrality measures T] being used in predicting [A  noun phrases  A] appearing in the abstracts of scientific articles . Our experimental results show that centrality measures improve the accuracy of the prediction in terms of both precision and recall . We also found that the method of constructing Noun Phrase Network significantly influences the accuracy when using the centrality heuristics itself , but is negligible when it is used together with other text features in decision trees . </abstract>
<abstract> A [T Graph-theoretic  T] Model of [A Lexical Syntactic Acquisition A] . This paper presents a [T graph-theoretic  T] model of the acquisition of  [A lexical syntactic representations A] . The representations the model learns are non-categorical or graded . We propose a new evaluation methodology of [A syntactic acquisition A] in the framework of exemplar theory . When applied to the CHILDES corpus , the evaluation shows that the model 's graded syntactic representations perform better than previously proposed categorical representations . </abstract>
<abstract> [A Event Coreference A] For [A Information Extraction A] . We propose a general approach for performing [A event coreference A] and for constructing [A complex event representations A] , such as those required for [A information extraction A] tasks . Our approach is based on a representation which allows a tight coupling between [T  world or conceptual modelling and discourse modelling T] . The representation and the coreference mechanism are fully implemented within the LaSIE information extraction system where the mechanism is used for both [A object ( noun phrase ) and event coreference resolution . A] Indirect evaluation of the approach shows small , but significant benefit , for [A information extraction A] tasks . </abstract>
<abstract> [A Word Alignment And Cross-Lingual Resource Acquisition A] . Annotated corpora are valuable resources for developing Natural Language Processing applications . This work focuses on acquiring annotated data for [A multilingual processing  A] applications . We present an annotation environment that supports a web-based user-interface for [A acquiring word alignments  A] between English and Chinese as well as a visualization tool for researchers to explore the annotated data . </abstract>
<abstract> The Efficiency Of [T Multimodal T] Interaction For A [A Map-Based  A] Task . This paper compares the efficiency of using a standard direct-manipulation graphical user interface ( GUI ) with that of using the [T QuickSet pen\/voice multimodal interface T] for supporting a [A military task A] . In this task , a user places military units and control measures ( e.g. , various types of lines , obstacles , objectives ) on a map . Four military personnel designed and entered their own simulation scenarios via both interfaces . Analyses revealed that the [T multimodal  T] interface led to an average 3.5-fold speed improvement in the average entity creation time , including all error handling . The mean time to repair errors also was 4.3 times faster when interacting multimodally . Finally , all subjects reported a strong preference for [A multimodal A] interaction . These results indicate a substantial efficiency advantage for multimodal over GUI-based interaction during [A map-based A] tasks . </abstract>
<abstract> [T Lexical , Prosodic , And Syntactic Cues T] For [A Dialog Acts A] . The structure of a discourse is reflected in many aspects of its linguistic realization , including its lexical , prosodic , syntactic , and semantic nature . Multiparty dialog contains a particular kind of discourse structure , the [A dialog act ( DA ) A] . Like other types of structure , the dialog act sequence of a conversation is also reflected in its lexical , prosodic , and syntactic realization . This paper presents a preliminary investigation into the realization of a particular class of  [A  dialog acts A] which play an essential structuring role in dialog , the backchannels or acknowledgements tokens . We discuss the [T lexical , prosodic , and syntactic realization T] of these and subsumed or related [A dialog acts A] like continuers , assessments , yesanswers , agreements , and incipient-speakership . We show that [T lexical knowledge T] plays a role in distinguishing these [A dialog acts A] , despite the widespread ambiguity of words such as yeah , and that [T prosodic knowledge T] plays a role in DA identification for certain DA types , while [T lexical cues T] may be sufficient for the remainder . Finally , our investigation of the syntax of assessments suggests that at least some dialog acts have a very constrained syntactic realization , a per-dialog act ` microsyntax ' . </abstract>
<abstract> [T Counter-Training T] In Discovery Of [A Semantic Patterns A] . This paper presents a method for [T unsupervised T] discovery of [A  semantic patterns A] . [A Semantic patterns A] are useful for a variety of text understanding tasks , in particular for locating events in text for information extraction  . The method builds upon previously described approaches to [T iterative unsupervised T] [A pattern acquisition A] . One common characteristic of prior approaches is that the output of the algorithm is a continuous stream of patterns , with gradually degrading precision . Our method differs from the previous pattern acquisition algorithms in that it introduces competition among several scenarios simultaneously . This provides natural stopping criteria for the [T unsupervised learners T] , while maintaining good precision levels at termination . We discuss the results of experiments with several scenarios , and examine different aspects of the new procedure . </abstract>
<abstract> [A Detecting Emotion In Speech A] : Experiments In Three Domains . The goal of my proposed dissertation work is to help answer two fundamental questions : ( 1 ) How is emotion communicated in speech ? and ( 2 ) Does emotion modeling improve spoken dialogue applications ? In this paper I describe [T feature extraction and emotion classi cation T] experiments I have conducted and plan to conduct on three different domains : [A EPSaT , HMIHY , and ITSpoke A] . In addition , I plan to implement emotion modeling capabilities into ITSpoke and evaluate the effectiveness of doing so . </abstract>
<abstract> Improvements in [T Analogical Learning T] : Application to [A Translating Multi-Terms of the Medical Domain A] . Handling terminology is an important matter in a translation workflow . However , current Machine Translation ( MT ) systems do not yet propose anything proactive upon tools which assist in managing terminological databases . In this work , we investigate several enhancements to [T analogical learning T] and test our implementation on [A translating medical terms A] . We show that the analogical engine works equally well when translating from and into a morphologically rich language , or when dealing with language pairs written in different scripts . Combining it with a [T phrasebased statistical T] engine leads to significant improvements . </abstract>
<abstract> A [T Structural Similarity T]  Measure . This paper outlines a measure of [A language similarity A] based on [T structural similarity of surface syntactic dependency trees T] . Unlike the more traditional string-based measures , this measure tries to reflect `` deeper '' correspondences among languages . The development of this measure has been inspired by the experience from MT of syntactically similar languages . This experience shows that the lexical similarity is less important than syntactic similarity . This claim is supported by a number of examples illustrating the problems which may arise when a measure of language similarity relies too much on a simple similarity of texts in different languages . </abstract>
<abstract> [A Dependency Structure Analysis A] And [A Sentence Boundary Detection A] In [A Spontaneous Japanese A] . This paper describes a project to detect [A dependencies  A] between Japanese phrasal units called bunsetsus , and [A sentence boundaries in a spontaneous speech corpus A] . In monologues , the biggest problem with dependency structure analysis is that sentence boundaries are ambiguous . In this paper , we propose two methods for improving the accuracy of [A sentence boundary detection A] in [A spontaneous Japanese speech A] : One is based on [T statistical machine translation T] using [T dependency information T] and the other is based on [T text chunking using SVM T] . An F-measure of 84.9 was achieved for the accuracy of sentence boundary detection by using the proposed methods . The accuracy of dependency structure analysis was also improved from 75.2 % to 77.2 % by using automatically detected sentence boundaries . The accuracy of dependency structure analysis and that of sentence boundary detection were also improved by interactively using both automatically detected dependency structures and sentence boundaries . </abstract>
<abstract> [T Unsupervised T] [A  Language Model Adaptation A] Incorporating [T Named Entity Information T] . [A Language model ( LM ) adaptation A]  is important for both speech and language processing . It is often achieved by combining a generic LM with a topic-specific model that is more relevant to the target document . Unlike previous work on unsupervised LM adaptation , this paper investigates how effectively using [T named entity  T] ( [T NE T] ) information , instead of considering all the words , helps [A LM adaptation A] . We evaluate two [T latent topic analysis T] approaches in this paper , namely , [T clustering T] and [T Latent Dirichlet Allocation T] ( [T LDA T] ) . In addition , a new dynamically adapted weighting scheme for  [T  topic mixture models T] is proposed based on [T LDA T] topic analysis . Our experimental results show that the NE-driven LM adaptation framework outperforms the baseline generic LM . The best result is obtained using the [T LDA-based  T] approach by expanding the [T named entities  T] with syntactically filtered words , together with using a large number of topics , which yields a perplexity reduction of 14.23 % compared to the baseline generic LM . </abstract>
<abstract> [A Text Classification A] By [T Bootstrapping With Keywords , EM And Shrinkage T] . When applying [A text classification A] to complex tasks , it is tedious and expensive to hand-label the large amounts of training data necessary for good performance . This paper presents an alternative approach to [A text classification A] that requires no labeled documentsi instead , it uses a small set of keywords per class , a class hierarchy and a large quantity of easilyobtained unlabeled documents . The keywords are used to assign approximate labels to the unlabeled documents by termmatching . These preliminary labels become the starting point for a [T bootstrapping T] process that learns a [T naive Bayes classifier T] using [T Expectation-Maximization and hierarchical shrinkage T] . When classifying a complex data set of computer science research papers into a 70-leaf topic hierarchy , the keywords alone provide 45 % accuracy . The classifier learned by bootstrapping reaches 66 % accuracy , a level close to human agreement . </abstract>
<abstract> [T LM T] Studies On [A Filled Pauses In Spontaneous Medical Dictation A] . We investigate the optimal [T LM T] treatment of abundant [A filled pauses ( FP ) in spontaneous monologues of a professional dictation  A] task . Questions addressed here are ( 1 ) how to deal with [A FP A] in the LM history and ( 2 ) to which extent can the [T LM T] distinguish between positions with high and low [A FP A] likelihood . Our results differ partly from observations reported on dialogues . Discarding [A FP A] from all [T LM T] histories clearly improves the performance . [T Local perplexities T] , [T entropies T] and [T word rankings T] at positions following [A FP A] suggest that most [A FP A] indicate hesitations rather than restarts . Proper prediction of [A FP A] allows to distinguish [A FP A] from word positions by a doubled FP probability . Recognition experiments confirm the improvements found in our perplexity studies . </abstract>
<abstract> [A Phoneme-To-Text Transcription A] System With An Infinite Vocabulary . The [T noisy channel model T] approach is successfully applied to various natural language processing tasks . Currently the main research focus of this approach is [T adaptation  T] methods , how to capture characteristics of words and expressions in a target domain given example sentences in that domain . As a solution we describe a method enlarging the vocabulary of a [T language model T] to an almost infinite size and capturing their context information . Especially the new method is suitable for languages in which words are not delimited by whitespace . We applied our method to a [A phoneme-to-text transcription A] task in Japanese and reduced about 10 % of the errors in the results of an existing method . </abstract>
<abstract> Measuring and Predicting [T Orthographic Associations T] : Modelling the [A Similarity of Japanese Kanji A] . As human beings , our mental processes for recognizing linguistic symbols generate perceptual neighborhoods around such symbols where confusion errors occur . Such neighborhoods also provide us with conscious mental associations between symbols . This paper formalises [T orthographic  T] models for [A similarity of Japanese kanji A] , and provides a proofof-concept dictionary extension leveraging the mental associations provided by [T orthographic proximity T] . </abstract>
<abstract> Automatic Classification Of [A Verbs In Biomedical Texts A] . Lexical classes , when tailored to the application and domain in question , can provide an effective means to deal with a number of natural language processing ( NLP ) tasks . While manual construction of such classes is difficult , recent research shows that it is possible to automatically induce verb classes from [T cross-domain corpora T] with promising accuracy . We report a novel experiment where similar technology is applied to the important , challenging domain of [A biomedicine A] . We show that the resulting classification , acquired from a corpus of biomedical journal articles , is highly accurate and strongly domainspecific . It can be used to aid [A BIO-NLP A] directly or as useful material for investigating the [A syntax and semantics of verbs in biomedical texts A] . </abstract>
<abstract> [T Linguistic Profiling T] For [A Authorship Recognition And Verification A] . A new technique is introduced , [T linguistic profiling T] , in which large numbers of counts of [T  linguistic features T] are used as a text profile , which can then be compared to average profiles for groups of texts . The technique proves to be quite effective for [A authorship verification and recognition A] . The best parameter settings yield a False Accept Rate of 8.1 % at a False Reject Rate equal to zero for the verification task on a test corpus of student essays , and a 99.4 % 2-way recognition accuracy on the same corpus . </abstract>
<abstract> [A Machine Translation A] Based on [T Constraint-Based Synchronous Grammar T] . This paper proposes a variation of [T synchronous grammar T] based on the formalism of [T context-free grammar T] by generalizing the first component of productions that models the source text , named [T Constraint-based Synchronous Grammar ( CSG ) T] . Unlike other synchronous grammars , [T CSG T] allows multiple target productions to be associated to a single source production rule , which can be used to guide a parser to infer different possible translational equivalences for a recognized input string according to the feature constraints of symbols in the pattern . Furthermore , [T CSG T] is augmented with independent rewriting that allows expressing discontinuous constituents in the inference rules . It turns out that such grammar is more expressive to model the translational equivalences of parallel texts for [A machine translation A] , and in this paper , we propose the use of [T CSG T] as a basis for building a [A machine translation  A] ( [A MT A] ) system for Portuguese to Chinese translation . </abstract>
<abstract> Scaling [T Phrase-Based Statistical T] [A Machine Translation A] To Larger Corpora And Longer Phrases . In this paper we describe a novel data structure for  [T phrase-based statistical T] [A  machine translation A] which allows for the retrieval of arbitrarily long phrases while simultaneously using less memory than is required by current decoder implementations . We detail the computational complexity and average retrieval times for looking up phrase translations in our [T suffix array-based data structure T] . We show how [T sampling T] can be used to reduce the retrieval time by orders of magnitude with no loss in [A translation A] quality . </abstract>
<abstract> [A Systemic Classification A] And Its Efficiency . This paper examines the problem of [A classifying linguistic objects A] on the basis of information encoded in the [T  system network formalism T] developed by Halliday . It is shown that this problem is NP-hard , and a restriction to the formalism , which renders the classification problem soluble in polynomial time , is suggested . An algorithm for the [A unrestricted classification  A] problem , which separates a potentially expensive second stage from a more tractable first stage , is then presented . </abstract>
<abstract> Extracting [T Terminologically Relevant Collocations T] in the [A Translation of Chinese Monograph A] . This paper suggests a methodology which is aimed to extract the [T terminologically relevant collocations T] for [A translation A] purposes . Our basic idea is to use a [T hybrid  T] method which combines the [T statistical method and linguistic rules T] . The extraction system used in our work operated at three steps : ( 1 ) [T Tokenization and POS tagging T] of the corpus ; ( 2 ) [T Extraction of multi-word units using statistical measure T] ; ( 3 ) Linguistic filtering to make use of [T  syntactic patterns and stop-word list T] . As a result , [T hybrid T] method using [T linguistic filters T] proved to be a suitable method for selecting terminological collocations , it has considerably improved the precision of the extraction which is much higher than that of purely statistical method . In our test , hybrid method combining `` [T Log-likelihood ratio T] '' and `` [T linguistic rules T] '' had the best performance in the extraction . We believe that [T terminological collocations T] and phrases extracted in this way , could be used effectively either to supplement existing terminological collections or to be used in addition to traditional reference works . </abstract>
<abstract> Practical Considerations In Building A [A Multi-Lingual Authoring System A] For Business Letters . The paper describes the experiences of a multi-national consortium in an on-going project to construct a [A multilingual authoring A] tool for business letters . The consortium consists of two universities ( both with significant experience in language engineering ) , three software companies , and various potential commercial users with the organizations being located in a total of four countries . The paper covers the history of the development of the project from an academic idea but focuses on the implications of the user-requirements orientated outlook of the commercial developers and the implications of this view for the system architecture , user requirements , delivery platforms and so on . Particularly interesting consequences of the user requirements are the [T database centred architecture T] , and the constraints and opportunities this presents for development of [T grammatical components T] at both the text and sentence level . </abstract>
<abstract> Method For Improving Automatic [A Word Categorization A] . This paper presents a new approach to automatic  [A word categorization A] which improves both the efficiency of the algorithm and the quality of the formed clusters . The [T unigram and the bigram T] statistics of a corpus of about two million words are used with an efficient [T distance function T] to measure the similarities of words , and a [T greedy algorithm T] to put the words into clusters . The notions of [T fuzzy clustering T] like [T cluster prototypes , degree of membership T] are used to form up the clusters . The algorithm is of [T unsupervised T] type and the number of clusters are determined at run-time . </abstract>
<abstract> A [A Voice Enabled Procedure Browser A] For The International Space Station . Clarissa , an experimental [A voice enabled procedure browser A] that has recently been deployed on the International Space Station ( ISS ) , is to the best of our knowledge the first [A spoken dialog  A] system in space . This paper gives background on the system and the ISS procedures , then discusses the research developed to address three key problems : [A grammarbased speech recognition A] using the [T Regulus toolkit T] ; [T SVM  T] based methods for [A open microphone speech recognition A] ; and robust [T side-effect free dialogue management T] for handling undos , corrections and confirmations . </abstract>
<abstract> Automatic [A Indexing of Specialized Documents A] : Using [T Generic vs. Domain-Specific Document Representations T] . The shift from paper to electronic documents has caused the curation of information sources in large electronic databases to become more generalized . In the biomedical domain , continuing efforts aim at refining indexing tools to assist with the update and maintenance of databases such as MEDLINE ® . In this paper , we evaluate two [T statistical  T] methods of producing [A  MeSH ® indexing recommendations for the genetics literature A] , including [A recommendations involving subheadings A] , which is a novel application for the methods . We show that a [T generic representation of the documents T] yields both better precision and recall . We also find that a [T domainspecific representation T] of the documents can contribute to enhancing recall . </abstract>
<abstract> Evaluating [T Unsupervised Part-of-Speech Tagging T] for [A Grammar Induction A] . This paper explores the relationship between various measures of [T unsupervised part-of-speech tag  T] induction and the performance of both [T supervised and unsupervised T] [A parsing A] models trained on induced [T tags T] . We find that no standard tagging metrics correlate well with [T unsupervised T] [A parsing A] performance , and several metrics grounded in information theory have no strong relationship with even supervised parsing performance . </abstract>
<abstract> A Portable And Quick Japanese [A Parser A] : QJP . QJP is a portable and quick softwaxe module for Japanese processing . QJP analyzes a Japanese sentence into [A segmented morphemes\/words with tags A] and a syntactic bunsetsu kakari-uke structure based on the two strategies , a ) [T Morphological T] analysis based on [T character-types and functional-words T] and b ) [T Syntactic T] analysis  by simple treatment of [T  structural ambiguities T] and ignoring semantic information . QJP is small , fast and robust , because 1 ) dictionary size ( less than1 100KB ) and required memory size ( 260KB ) are very small , 2 ) analysis speed is fast ( more than 100 words\/see on 80486-PC ) , and 3 ) even a 100-word long sentence containing unknown words is easily processed . Using QJP and its ana ) ysis results as a base and adding other functions for processing Japanese documents , a valqety of applications can be developed on UNIX workstations or even on PCs . </abstract>
<abstract> A [A Syntactic Analysis A]  Method Of Long Japanese Sentences Based On The Detection Of [T Conjunctive Structures T] . This paper presents a [A syntactic analysis A] method that first detects [T conjunctive structures T] in a sentence by checking parallelism of two series of words and then analyzes the [A dependency structure A] of the sentence with the help of the information about the [T conjunctive structures T] . Analysis of long sentences is one of the most difficult problems in natural language processing . The main reason for this difficulty is the structural ambiguity that is common for [T conjunctive structures T] that appear in long sentences . Human beings can recognize [T conjunctive structures T] because of a certain , but sometimes subtle , similarity that exists between conjuncts . Therefore , we have developed an algorithm for calculating a similarity measure between two arbitrary series of words from the left and the right of a conjunction and selecting the two most similar series of words that can reasonably be considered as composing a [T conjunctive structure T] . This is realized using a [T dynamic programming  T] technique . A long sentence can be reduced into a shorter form by recognizing [T conjunctive structures T] . Consequently , the total [A dependency structure A]  of a sentence can be obtained by relatively simple [T head-dependent rules T] . A serious problem concerning [T conjunctive structures T] , besides the ambiguity of their scopes , is the ellipsis of some of their components . Through our [A dependency analysis A] process , we can find the ellipses and recover the omitted components . We report the results of analyzing 150Japanese sentences to illustrate the effectiveness of this method . </abstract>
<abstract> Extending A [A Thesaurus A] By [T Classifying Words T] . This paper proposes a method for extending an existing [A thesaurus A] through [T classification of new words T]  in terms of that [A thesaurus A] . New words are [T classified T] on the basis of relative probabilities of . a word belonging to a given word class , with the probabilities calculated using [T nounverb co-occurrence pairs T] . Experiments using the Japanese Bunruigoihy5 thesaurus on about 420,000 co-occurrences showed that new words can be classified correctly with a maximum accuracy of more than 80 % . </abstract>
<abstract> [T Bidirectional Inference With The Easiest-First Strategy T] For [A Tagging Sequence Data A] . This paper presents a [T bidirectional inference  T] algorithm for [A sequence labeling A] problems such as [A part-of-speech tagging A] , [A named entity recognition A] and [A text chunking A] . The algorithm can enumerate all possible decomposition structures and find the highest probability sequence together with the corresponding decomposition structure in polynomial time . We also present an efficient [T decoding T] algorithm based on the [T easiest-first strategy T] , which gives comparably good performance to full [T bidirectional inference T] with significantly lower computational cost . Experimental results of [A part-of-speech tagging A] and [A text chunking A] show that the proposed [T bidirectional inference T] methods consistently outperform unidirectional inference methods and bidirectional MEMMs give comparable performance to that achieved by state-of-the-art learning algorithms including kernel support vector machines . </abstract>
<abstract> Extracting The Unextractable : A Case Study On [A Verb-Particles A] . This paper proposes a series of techniques for extracting [A  English verb -LCB- particle constructions  A] from raw text corpora . We initially propose three basic methods , based on [T tagger T] output , [T chunker output and a chunk grammar T] , respectively , with the [T chunk grammar T] method optionally combining with an [T attachment resolution T] module to determine the [A syntactic structure of verb -LCB- preposition pairs A] in ambiguous constructs . We then combine the three methods together into a single [T classifler T] , and add in a number of extra [T lexical and frequentistic features T] , producing a flnal F-score of 0.865 over the WSJ . </abstract>
<abstract> [A Multiple Aspect Ranking A] Using the [T Good Grief Algorithm T] . We address the problem of analyzing [A multiple related opinions A] in a text . For instance , in a restaurant review such opinions may include food , ambience and service . We formulate this task as a [A multiple aspect ranking A] problem , where the goal is to produce a set of numerical scores , one for each aspect . We present an algorithm that[T  jointly learns T] [A ranking A] models for individual aspects by modeling the [T dependencies T] between assigned ranks . This algorithm guides the prediction of individual rankers by analyzing [T meta-relations  T]between opinions , such as agreement and contrast . We prove that our [T agreementbased joint T] model is more expressive than individual [A ranking A] models . Our empirical results further con rm the strength of the model : the algorithm provides signi cant improvement over both individual rankers and a state-of-the-art joint [A ranking A] model . </abstract>
<abstract> NUS at WMT09 : [T Domain Adaptation T] Experiments for [A English-Spanish Machine Translation A] of News Commentary Text . We describe the system developed by the team of the National University of Singapore for [A English to Spanish machine translation A] of News Commentary text for the WMT09 Shared Translation Task . Our approach is based on [T domain adaptation T] , combining a small in-domain News Commentary bi-text and a large out-of-domain one from the Europarl corpus , from which we built and combined two separate phrase tables . We further combined two [T language models  T] ( in-domain and out-of-domain ) , and we experimented with [T cognates T] , improved [T tokenization T] and [T recasing T] , achieving the highest lowercased NIST score of 6.963 and the second best lowercased Bleu score of 24.91 % for training without using additional external data for English-toSpanish translation at the shared task . </abstract>
<abstract> [T UPAR7 T] : A [T knowledge-based T] system for headline [A  sentiment tagging A] . For the Affective Text task at SemEval2007 , University Paris 7 's system first evaluates [A emotion and valence on all words A] of a news headline ( using enriched versions of [T SentiWordNet T] and a subset of [T WordNetAffect T] ) . We use a [T parser T] to find the [T head word T] , considering that it has a major importance . We also detect contrasts ( between [T positive and negative words T] ) that shift valence . Our [T knowledge-based T] system achieves high accuracy on [A emotion and valence annotation A] . These results show that working with [T linguistic T] techniques and a broad-coverage [T lexicon T] is a viable approach to [A sentiment analysis A] of headlines . 1 Introduction 1.1 Objectives The detection of emotional connotations in texts is a recent task in computational linguistics . Its economic stakes are promising ; for example , a company could detect , by analyzing the blogosphere , people 's opinion on its products . The goal of the SemEval task is to annotate news headlines for emotions ( using a predefined list : anger , disgust , fear , joy , sadness & surprise ) , and for valence ( positive or negative ) . A specific difficulty here is related to the small number of words available for the analysis . </abstract>
<abstract> Optimizing [A Grammars A] for Minimum Dependency Length . We examine the problem of choosing word order for a set of [A dependency trees A] so as to minimize total dependency length . We present an algorithm for computing the optimal layout of a single tree as well as a [T numerical T] method for optimizing a [A grammar A] of orderings over a set of dependency types . A [A grammar A] generated by minimizing dependency length in unordered trees from the Penn Treebank is found to agree surprisingly well with English word order , suggesting that dependency length minimization has influenced the evolution of English . </abstract>
<abstract> [T Online Learning of Relaxed CCG Grammars T] for [A Parsing to Logical Form A] . We consider the problem of learning to [A parse sentences A] to [A lambda-calculus representations A] of their underlying semantics and present an algorithm that learns a [T weighted combinatory categorial grammar ( CCG ) T] . A key idea is to introduce [T non-standard CCG combinators T] that relax certain parts of the grammar -- for example allowing flexible word order , or insertion of lexical items -- with learned costs . We also present a new , [T online algorithm for inducing a weighted CCG T] . Results for the approach on ATIS data show 86 % F-measure in recovering fully correct semantic analyses and 95.9 % F-measure by a partial-match criterion , a more than 5 % improvement over the 90.3 % partial-match figure reported by He and Young ( 2006 ) . </abstract>
<abstract> A Large-Scale Exploration Of Effective [T Global Features T] For A  Joint [A  Entity Detection And Tracking  A] Model . [A Entity detection and tracking ( EDT ) A] is the task of identifying textual mentions of real-world entities in documents , extending the [A named entity detection A] and [A coreference resolution A] task by considering mentions other than names ( pronouns , de nite descriptions , etc. ) . Like NE tagging and coreference resolution , most solutions to the [A EDT A] task separate out the mention detection aspect from the coreference aspect . By doing so , these solutions are limited to using only local features for learning . In contrast , by modeling both aspects of the EDT task simultaneously , we are able to learn using highly [T complex , non-local features T] . We develop a new  joint [A EDT A]  model and explore the utility of many features , demonstrating their effectiveness on this task . </abstract>
<abstract> [A German Particle Verbs And Pleonastic Prepositions A] . This paper discusses the behavior of [A  German particle verbs A] formed by two-way prepositions in combination with [A pleonastic PPs A] including the verb particle as a preposition . These [A particle verbs A] have a characteristic feature : some of them license directional prepositional phrases in the accusative , some only allow for locative PPs in the dative , and some particle verbs can occur with PPs in the accusative and in the dative . Directional particle verbs together with directional PPs present an additional problem : the particle and the preposition in the PP seem to provide redundant information . The paper gives an overview of the [A semantic verb classes A] in uencing this phenomenon , based on corpus data , and explains the underlying reasons for the behavior of the particle verbs . We also show how the restrictions on [A particle verbs and pleonastic PPs A] can be expressed in a grammar theory like [T Lexical Functional Grammar ( LFG ) T] . </abstract>
<abstract> On The Equivalence Of [A Weighted Finite-State Transducers A] . Although they can be topologically different , two distinct transducers may actually recognize the same rational relation . Being able to test the equivalence of transducers allows to implement such operations as incremental minimization and iterative composition . This paper presents an algorithm for [A testing the equivalence of deterministic weighted finite-state transducers A] , and outlines an implementation of its applications in a prototype weighted finite-state [T calculus tool T] . </abstract>
<abstract> [T Phrase-Based Backoff  T] Models For [A Machine Translation Of Highly Inflected Languages A] . We propose a [T backoff  T] model for  [T phrasebased T] [A  machine translation A] that translates unseen word forms in foreign-language text by [T hierarchical morphological abstractions T] at the word and the phrase level . The model is evaluated on the Europarl corpus for German-English and FinnishEnglish translation and shows improvements over state-of-the-art phrase-based models . </abstract>
<abstract> Growing [A Semantic Grammars A] . A critical path in the development of [A natural language understanding ( NLU ) A] modules lies in the difficulty of defining a mapping from words to semantics : Usually it takes in the order of years of highly-skilled labor to develop a [A semantic mapping A] , e.g. , in the form of a [A semantic grammar A] , that is comprehensive enough for a given domain . Yet , due to the very nature of human language , such mappings invariably fail to achieve full coverage on unseen data . Acknowledging the impossibility of stating a priori all the surface forms by which a concept can be expressed , we present GsG : an empathic computer system for the rapid deployment of [A NLU front-ends A] and their dynamic customization by non-expert end-users . Given a new domain for which an [A NLU front-end A] is to be developed , two stages are involved . In the authoring stage , GSQ aids the developer in the construction of a simple [T domain model T] and a [A kernel analysis grammar A] . Then , in the run-time stage , GSG provides the enduser with an interactive environment in which the kernel grammar is dynamically extended . Three [T learning  T] methods are employed in the acquisition of [A semantic mappings A] from unseen data : ( i ) [T parser predictions T] , ( ii ) [T hidden understanding model T] , and ( iii ) [T end-user paraphrases T] . A baseline version of GsG has been implemented and prellminary experiments show promising results . </abstract>
<abstract> A [A Non-Projective Dependency Parser A] . We describe a practical [A parser for unrestricted dependencies A] . The parser creates links between words and names the links according to their [T syntactic functions T] . We first describe the older [T Constraint Grammar T] [A parser A] where many of the ideas come from . Then we proceed to describe the central ideas of our new parser . Finally , the parser is evaluated . </abstract>
<abstract> Efficient [A Generation A] In Primitive Optimality Theory . This paper introduces primitive Optimality Theory ( OTP ) , a linguistically motivated formalization of OT . OTP specifies the class of [A autosegmental representations A] , the [A universal generator Gen A] , and the two simple families of permissible constraints . In contrast to less restricted theories using Generalized Alignment , OTP 's optimal surface forms can be generated with [T finite-state methods T] adapted from ( Ellison , 1994 ) . Unfortunately these methods take time exponential on the size of the grammar . Indeed the [A generation A] problem is shown NP-complete in this sense . However , techniques are discussed for making Ellison 's approach fast in the typical case , including a simple trick that alone provides a 100-fold speedup on a grammar fragment of moderate size . One avenue for future improvements is a new [T finite-state T] notion , `` [T factored automata T] , '' where regular languages are represented compactly via formal intersections N ~ = IAi of FSAs . </abstract>
<abstract> [T Distributed Word Clustering T] for Large Scale Class-Based Language Modeling in [A Machine Translation A] . In statistical language modeling , one technique to reduce the problematic effects of data sparsity is to partition the vocabulary into equivalence classes . In this paper we investigate the effects of applying such a technique to higherorder [T  n-gram  T]  models trained on large corpora . We introduce a modification of the [T exchange clustering  T] algorithm with improved efficiency for certain [T partially class-based T] models and a [T distributed T] version of this algorithm to efficiently obtain automatic [T  word classifications T] for large vocabularies ( ) 1 million words ) using such large training corpora ( ) 30 billion tokens ) . The resulting clusterings are then used in [T  training partially class-based language  T] models . We show that combining them with [T wordbased n-gram  T] models in the [T log-linear  T] model of a state-of-the-art [A statistical machine translation A] system leads to improvements in translation quality as indicated by the BLEU score . </abstract>
<abstract> Using The [T  Web T] As A [A Bilingual Dictionary A] . We present a system for extracting an English [A translation A] of a given Japanese technical term by collecting and scoring [A translation A] candidates from the [T  web T] . We first show that there are a lot of partially bilingual documents in the web that could be useful for [A term translation A] , discovered by using a commercial technical term [T dictionary T] and an [T Internet search engine T] . We then present an algorithm for obtaining [A  translation  A] candidates based on the distance of Japanese and English terms in [T web documents T] , and report the results of a preliminary experiment . </abstract>
<abstract> [A Semantic Extraction A] With [T Wide-Coverage Lexical Resources T] . We report on results of [T combining graphical modeling techniques T] with Information Extraction resources ( [T Pattern Dictionary and Lexicon T] ) for both [A frame and semantic role assignment A] . Our approach demonstrates the use of two human built knowledge bases ( [T WordNet and FrameNet T] ) for the task of [A semantic extraction A] . </abstract>
<abstract> [A Model Adaptation A] via [T Model Interpolation T] and [T Boosting T] for [A Web Search Ranking A] . This paper explores two classes of [A model adaptation A] methods for [A Web search ranking A] : [T Model Interpolation and error-driven learning  T] approaches based on a [T boosting T] algorithm . The results show that [T model interpolation T] , though simple , achieves the best results on all the open test sets where the test data is very different from the training data . The [T tree-based boosting  T] algorithm achieves the best performance on most of the closed test sets where the test data and the training data are similar , but its performance drops significantly on the open test sets due to the instability of trees . Several methods are explored to improve the robustness of the algorithm , with limited success . </abstract>
<abstract> [A Weighted Deductive Parsing A] And [T Knuth 's Algorithm T] . We discuss [A weighted deductive parsing A] and consider the problem of finding the derivation with the lowest weight . We show that [T Knuth 's generalization of Dijkstra 's  T] algorithm for the shortestpath problem offers a general method to solve this problem . Our approach is modular in the sense that [T Knuth 's algorithm T] is formulated independently from the [A weighted deduction A] system . </abstract>
<abstract> [T Corrective  T] Models For [A Speech Recognition Of Inflected Languages A] . This paper presents a corrective model for [A speech recognition of inflected languages A] . The model , based on a [T discriminative T]  framework , incorporates [T word ngrams T] features as well as [T factored morphological T] features , providing error reduction over the model based solely on word n-gram features . Experiments on a large vocabulary task , namely the Czech portion of the MALACH corpus , demonstrate performance gain of about 1.1 -- 1.5 % absolute in word error rate , wherein morphological features contribute about a third of the improvement . A simple feature selection mechanism based on [T χ2 statistics T] is shown to be effective in reducing the number of features by about 70 % without any loss in performance , making it feasible to explore yet larger feature spaces . </abstract>
<abstract> Automated [A Scoring A] Using A [T Hybrid Feature Identification T] Technique . This study exploits [T statistical redundancy T] inherent in natural language to automatically predict [A scores for essays A] . We use a [T hybrid feature identification T] method , including [T syntactic structure analysis T] , [T rhetorical structure analysis T] , and [T topical analysis T] , to [A score essay  A] responses from test-takers of the Graduate Management Admissions Test ( GMAT ) and the Test of Written English ( TWE ) . For each essay question , a [T stepwise linear regression analysis T] is run on a training set ( sample of human scored essay responses ) to extract a weighted set of predictive features for each test question . Score prediction for cross-validation sets is calculated from the set of predictive features . Exact or adjacent agreement between the Electronic Essay Rater ( e-rater ) score predictions and human rater scores ranged from 87 % to 94 % across the 15 test questions . </abstract>
<abstract> An [T Unsupervised  T]  Model for [A Text Message Normalization A] . Cell phone text messaging users express themselves briefly and colloquially using a variety of creative forms . We analyze a sample of creative , non-standard text message word forms to determine frequent word formation processes in texting language . Drawing on these observations , we construct an [T unsupervised noisy-channel  T] model for [A text message normalization A] . On a test set of 303 text message forms that differ from their standard form , our model achieves 59 % accuracy , which is on par with the best supervised results reported on this dataset . </abstract>
<abstract> Automatic [A  sense prediction for implicit discourse relations  A] in text . We present a series of experiments on automatically identifying the [A  sense of implicit discourse relations A] , i.e. relations that are not marked with a discourse connective such as `` but '' or `` because '' . We work with a corpus of implicit relations present in newspaper text and report results on a test set that is representative of the naturally occurring distribution of senses . We use several [T linguistically informed features T] , including  [T  polarity tags , Levin verb classes , length of verb phrases , modality , context , and lexical features T] . In addition , we revisit past approaches using lexical pairs from unannotated text as features , explain some of their shortcomings and propose modifications . Our best combination of features outperforms the baseline from data intensive approaches by 4 % for comparison and 16 % for contingency . </abstract>
<abstract> A [T Dynamic Temporal Logic Of Events , Intervals And States T] For [A Nominalization A] In Natural Language . The interpretation of [A nominalized expressions A] in English poses several problems . First , it must be explained how their meanings are derived from the meanings of the underlying verbs . Second , different forms of [A nominalizations A] differ in their semantic behavior . Finally , aspectual restrictions which exist for ingofnominals must be explained . The solution to be proposed is based on the assumption that [T non-stative verbs T] denote changes . Changes can be conceived of in two different ways , either as objects which bring about a particular result or as relations between states . A [T dynamic structure of events , intervals and states T] is defined in which both perspectives can be expressed by means of sorting the universe D. The basic idea is to augment a transition system for [T Dynamic Logic T] ( [T DL T] ) by a further domain of events such that programs from DL can be described either as objects or relations between states . The interpretation of verbs is based on the second perspective : they denote ( generalized ) relations between states . The interpretation of [A nominalized expressions A] uses the first perspective : they denote changes as objects . Different forms of [A nominalizations A] denote different sorts of objects which are systematically related to the denotation of the underlying verb . </abstract>
<abstract> Method of [T Selecting Training Data T] to Build a Compact and Efficient [A Translation  A] Model . Target task matched parallel corpora are required for [A statistical translation  A] model training . However , training corpora sometimes include both target task matched and unmatched sentences . In such a case , training set selection can reduce the size of the translation model . In this paper , we propose a [T training set selection T] method for [A translation  A] model training using [T linear T] [A translation A] model interpolation and a [T language model T] technique . According to the experimental results , the proposed method reduces the translation model size by 50 % and improves BLEU score by 1.76 % in comparison with a baseline training corpus usage . </abstract>
<abstract> [T Learning T] [A Greek Verb Complements A] : Addressing The Class Imbalance . Imbalanced training sets , where one class is heavily underrepresented compared to the others , have a bad effect on the classification of rare class instances . We apply [T One-sided Sampling T] for the first time to a [A lexical acquisition task A] ( learning verb complements from Modern Greek corpora ) to remove redundant and misleading training examples of verb nondependents and thereby balance our training set . We experiment with well-known [T learning  T] algorithms to classify new examples . Performance improves up to 22 % in recall and 15 % in precision after balancing the dataset 1 . </abstract>
<abstract> Efficient [A  Collaborative Discourse A] : A Theory And Its Implementation . An architecture for [A voice dialogue machines A] is described with emphasis on the problem solving and high level decision making mechanisms . The architecture provides facilities for [A generating voice interactions A] aimed at cooperative human-machine problem solving . It assumes that the dialogue will consist of a series of local selfconsistent subdialogues each aimed at subgoals related to the overall task . The discourse may consist of a set of such subdiaiogues with jumps from one subdialogue to the other in a search for a successful conclusion . The architecture maintains a [T user model T] to assure that interactions properly account for the level of competence of the user , and it includes an ability for the machine to take the initiative or yield the initiative to the user . It uses [T expectation  T] from the dialogue processor to aid in the correction of errors from the speech recognizer . </abstract>
<abstract> [A Corpus-Based Grammar Specialization A] . Broad-coverage grammars tend to be highly ambiguous . When such grammars are used in a restricted domain , it may be desirable to specialize them , in effect trading some coverage for a reduction in ambiguity . [A Grammar specialization A] is here given a novel formulation as an [T optimization  T] problem , in which the search is guided by a global measure combining coverage , ambiguity and grammar size . The method , applicable to any [A unification grammar A] with a phrasestructure backbone , is shown to be effective in specializing a broad-coverage [A  LFG for French A] . </abstract>
<abstract> [T Discriminative T][A  Word Alignment A] via [T Alignment Matrix  T] Modeling . In this paper a new [T discriminative T] [A   word alignment A]  method is presented . This approach models directly the [T  alignment matrix by a conditional random field ( CRF ) T] and so no restrictions to the alignments have to be made . Furthermore , it is easy to add features and so all available information can be used . Since the structure of the [T CRFs T] can get complex , the inference can only be done approximately and the standard algorithms had to be adapted . In addition , different methods to train the model have been developed . Using this approach the [A alignment  A] quality could be improved by up to 23 percent for 3 different language pairs compared to a combination of both IBM4alignments . Furthermore the [A word alignment A] was used to generate new phrase tables . These could improve the translation quality significantly . </abstract>
<abstract> A Low-Complexity , Broad-Coverage [T Probabilistic T] [A Dependency Parser A] For English . Large-scale [A parsing A] is still a complex and timeconsuming process , often so much that it is infeasible in real-world applications . The [A parsing  A] system described here addresses this problem by combining [T finite-state T] approaches , [T statistical T] [A parsing A] techniques and engineering knowledge , thus keeping parsing complexity as low as possible at the cost of a slight decrease in performance . The parser is robust and fast and at the same time based on strong linguistic foundations . </abstract>
<abstract> [A Semantic Interpretation A] Using [T KL-One T] . This paper presents extensions to the work of Bobrow and Webber ( Bobrow & Webber 80a , Bobrow & Webber 80b ) on [A semantic interpretation A] using [T KL-ONE T] to represent knowledge . The approach is based on an [T extended case frame  T] formalism applicable to all types of phrases , not just clauses . The frames are used to recognize semantically acceptable phrases , identify their structure , and , relate them to their meaning representation through [T translation rules T] . Approaches are presented for generating [T KL-ONE structures T] as the meaning of a sentence , for capturing [A semantic generalizations A] through abstract [T case frames T] , and for handling pronouns and relative clauses . </abstract>
<abstract> A Computational Model Of [T Multi-Modal Grounding T] For [A Human Robot Interaction A] . [A Dialog systems for mobile robots A] operating in the real world should enable mixedinitiative dialog style , handle [T multi-modal T] information involved in the communication and be relatively independent of the domain knowledge . Most dialog systems developed for mobile robots today , however , are often system-oriented and have limited capabilities . We present an [T agentbased  T] [A dialog A] model that are specially designed for [A human-robot interaction A] and provide evidence for its efficiency with our implemented system . </abstract>
<abstract> [A Context Dependent Modeling Of Phones In Continuous Speech A] Using [T Decision Trees T] . In a [A continuous speech recognition A] system it is important to model the context dependent variations in the pronunciations of words . In this paper we present an automatic method for [A modeling phonological variation A] using [T decision trees T] . For each phone we construct a [T decision tree T] that specifies the acoustic realization of the phone as a function of the context in which it appears . Several thousand sentences from a natural language corpus spoken by several talkers are used to construct these [T decision trees T] . Experimental results on a 5000-word vocabulary natural language [A speech recognition A] task are presented . </abstract>
<abstract> [T Focus T] To Emphasize [T Tone Structures T] For [A Prosodic Analysis In Spoken Language Generation A] . We analyze the concept of [T focus T] in speech and the relationship between focus and speech acts for [A prosodic generation A] . We determinehowthespeaker 's utterancesare influenced by speaker 's intention . The relationship between speech acts and focus information is used to define which parts of the sentence serve as the focus parts . We propose the Focus to Emphasize [T Tones T] ( FET ) structure to analyze the focus components . We also design the [T FET grammar T] to analyze the intonation patterns and produce tone marks as a result of our analysis . We present a proof-of-the-concept working example to validate our proposal . More comprehensive evaluations are part of our current work . </abstract>
<abstract> Finding [A Translation A] Equivalents : An Application Of [T Grammatical Metaphor T] . In this paper I describe how a significant class of cases that would involve ( possibly complex ) structural transfer in nmchine [A translation A] can be handled avoiding transfer . This is achieved by applying a [T semantic organization T] developed for monolingual text generation that is sufficiently abstract to remain invariant , within theoretically specifiable limits , across different languages . The further application of a mechanism motivated from within monolingual text generation , ` [T grammatical metaphor T] ' , then allows candidate appropriate translations to be isolated . The incorporation of these essentially monolingual mechanisms within the [A machine translation  A] process promises to significantly improve [A translational  A] capabilities ; examples of this are presented for English and German . </abstract>
<abstract> SEMHE A Generalised Two-Level System . This paper presents a generalised  twolevel implementation which can handle [A linear and non-linear morphological operations A] . An algorithm for the interpretation of multi-tape two-level rules is described . In addition , a number of issues which arise when developing non-linear grammars are discussed with examples from Syriac . </abstract>
<abstract> [A Multilingual Coreference Resolution A] . In this paper we present a new , multilingual [T data-driven T] method for [A coreference resolution A] as implemented in the SWIZZLE system . The results obtained after training this system on a bilingual corpus of English and Romanian tagged texts , outperformed [A coreference resolution A] in each of the individual languages . </abstract>
<abstract>  [T Unification-Based T] [A  Multimodal Integration A] . Recent empirical research has shown conclusive advantages of [A multimodal interaction A] over speech-only interaction for mapbased tasks . This paper describes a [A multimodal language processing A] architecture which supports interfaces allowing simultaneous input from [A  speech and gesture recognition A] . Integration of spoken and gestural input is driven by [T unification of typed feature structures  T] representing the semantic contributions of the different modes . This integration method allows the component modalities to mutually compensate for each others ' errors . It is implemented in QuickSet , a multimodal ( pen\/voice ) system that enables users to set up and control distributed interactive simulations . </abstract>
<abstract> A Study On [T Richer Syntactic Dependencies T] For [A Structured Language Modeling A] . We study the impact of [T richer syntactic dependencies T] on the performance of the [A structured language model  A] ( [A SLM A] ) along three dimensions : [A parsing A] accuracy ( LP\/LR ) , perplexity ( PPL ) and worderror-rate ( WER , N-best re-scoring ) . We show that our models achieve an improvement in LP\/LR , PPL and\/or WER over the reported baseline results using the SLM on the UPenn Treebank and Wall Street Journal ( WSJ ) corpora , respectively . Analysis of [A parsing A] performance shows correlation between the quality of the parser ( as measured by precision\/recall ) and the [A language model A] performance ( PPL and WER ) . A remarkable fact is that the enriched [A SLM A] outperforms the baseline 3-gram model in terms of WER by 10 % when used in isolation as a second pass ( N-best re-scoring ) language model . </abstract>
<abstract> [A Classifying Amharic News Text A] Using [T Self-Organizing Maps T] . The paper addresses using [T artificial neural networks T] for [A classification of Amharic news  A] items . Amharic is the language for countrywide communication in Ethiopia and has its own writing system containing extensive systematic redundancy . It is quite dialectally diversified and probably representative of the languages of a continent that so far has received little attention within the language processing field . The experiments investigated [A document clustering A] around user queries using [T SelfOrganizing Maps , an unsupervised learning neural network strategy T] . The best ANN model showed a precision of 60.0 % when trying to cluster unseen data , and a 69.5 % precision when trying to classify it . </abstract>
<abstract> Advanced [A Human-Computer Interface And Voice Processing Applications In Space A] . Much interest already exists in the electronics research community for developing and integrating speech technology to a variety of applications , ranging from voice-activated systems to automatic telephone transactions . This interest is particularly true in the field of aerospace where the training and operational demands on the crew have significantly increased with the proliferation of technology . Indeed , with advances in vehicule and robot automation , the role of the human operator has evolved from that of pilot\/driver and manual controller to supervisor and decision maker . Lately , some effort has been expended to implement alternative modes of system control , but automatic speech recognition ( ASR ) and human-computer interaction ( HCI ) research have only recently extended to civilian aviation and space applications . The purpose of this paper is to present the particularities of [A operator-computer interaction  A] in the unique conditions found in space . The potential for [A voice control applications inside spacecraft A] is outlined and methods of integrating spoken-language interfaces onto operational space systems are suggested . </abstract>
<abstract> [T Abductive Reasoning T] For [A Syntactic Realization A] . [T Abductive reasoning T] is • used in a bidirectional framework for [A syntactic realization and semantic interpretation A] . The use of the framework is illustrated in a case study of [A sentence generation A] , where different syntactic forms are generated depending on the status of discourse information . Examples are given involving three differen t syntactic constructions in German root clauses . </abstract>
<abstract> A [T Memory-Based T] Approach to Learning [A  Shallow Natural Language Patterns A] . Recognizing [A shallow linguistic patterns A] , such as basic syntactic relationships between words , is a common task in applied natural language and text processing . The common practice for approaching this task is by tedious manual definition of possible pattern structures , often in the form of regular expressions or finite automata . This paper presents a novel [T memory-based learning T] method that recognizes [A  shallow patterns A] in new text based on a [T bracketed training corpus T] . The training data are stored as-is , in efficient suffix-tree data structures . Generalization is performed on-line at recognition time by comparing subsequences of the new text to positive and negative evidence in the corpus . This way , no information in the training is lost , as can happen in other learning systems that construct a single generalized model at the time of training . The paper presents experimental results for [A recognizing noun phrase , subject-verb and verb-object patterns in English A] . Since the learning approach enables easy porting to new domains , we plan to apply it to syntactic patterns in other languages and to sub-language patterns for [A information extraction A] . </abstract>
<abstract> [T Unsupervised T] [A  Named Entity Transliteration A] Using [T Temporal And Phonetic Correlation T] . In this paper we investigate [T unsupervised T] [A  name transliteration A] using comparable corpora , corpora where texts in the two languages deal in some of the same topics -- and therefore share references to named entities -- but are not translations of each other . We present two distinct methods for [A transliteration A] , one approach using an [T unsupervised phonetic  T] [A transliteration A] method , and the other using the [T temporal distribution T] of candidate pairs . Each of these approaches works quite well , but by combining the approaches one can achieve even better results . We believe that the novelty of our approach lies in the [T phonetic-based scoring  T] method , which is based on a combination of carefully crafted phonetic features , and empirical results from the pronunciation errors of second-language learners of English . Unlike previous approaches to transliteration , this method can in principle work with any pair of languages in the absence of a training dictionary , provided one has an estimate of the pronunciation of words in text . </abstract>
<abstract> [A Generating Natural Language Summaries A] From [T Multiple On-Line Sources T] . We present a methodology for [A summarization of news  A] about current events in the form of briefings that include appropriate background ( historical ) information . The system that we developed , SUMMONS , uses the output of systems developed for the DARPA Message Understanding Conferences to [A generate summaries  A] of multiple documents  on the same or related events , presenting similarities and differences , contradictions , and generalizations among sources of information . We describe the various components of the system , showing how information from multiple articles is combined , organized into a paragraph , and finally , realized as English sentences . A feature of our work is the [T extraction of descriptions of entities  T] such as people and places for reuse to enhance a briefing . </abstract>
<abstract> A [T Lexico-Semantic  T] Approach To The [A Structuring Of Terminology A] . This paper discusses a number of implications of using either a [T conceptual T] approach or a [T lexico-semantic T] approach to [A terminology structuring A] , especially for interpreting data supplied by corpora for the purpose of building  [A specialized dictionaries A] . A simple example , i.e. , program , will serve as a basis for showing how relationships between terms are captured in both approaches . My aim is to demonstrate that truly conceptual approaches do not allow a flexible integration of terms and relationships between terms and that [T lexico-semantic approaches T] are more compatible with data gathered from corpora . I will also discuss some of the implications these approaches have for [A computational terminology A] and other corpus-based terminological endeavors . </abstract>
<abstract> UTD-SRL : A Pipeline Architecture for Extracting [A  Frame Semantic Structures A] . This paper describes our system for the task of extracting [A  frame semantic structures  A]  in SemEval 2007 . The system architecture uses two types of [T learning  T] models in each part of the task : [T Support Vector Machines ( SVM ) and Maximum Entropy ( ME ) T] . Designed as a pipeline of classi ers , the [A semantic parsing A] system obtained competitive precision scores on the test data . </abstract>
<abstract> Compound Nouns In A [T Unification-Based T] [A  MT System A] . This paper describes an approach to the treatment of nominal compounds in a [A machine translation A] project employing a modern [T unification-based T] system  . General problems connected with the analysis of compounds are briefly reviewed , and the project , for the automatic [A translation A] of Swiss avalanche bulletins , is introduced . Avalanche bulletins deal with a limited semantic domain and employ a sublanguage in which nominal compounds occur frequently . These and other properties of the texts affect the treatment of compounds , permitting certain simplifications , while leaving a number of possible alternative analyses . We discuss the different problems involving the [A translation A] of compounds between German and French , and show how the computational environment in use permits two different approaches to the problem : an [T interlingua-based T] approach and a [T transfer-based T] approach . Finally , wc evaluate these approaches with respect to linguistic and computational considerations applicable in a [A MT-system A] dealing with a limited semantic domain and describe the solution that has actually been implemented . </abstract>
<abstract> Coping With Ambiguity And Unknown Words Through [T Probabilistic  T] Models . From spring 1990 through fall 1991 , we performed a battery of small experiments to test the effectiveness of supplementing knowledge-based techniques with [T probabilistic T] models . This paper reports our experiments in predicting [A parts of speech  A] of highly ambiguous words , predicting the intended interpretation of an utterance when more than one interpretation satisfies all known syntactic and semantic constraints , and learning [A case frame information for verbs A] from example uses . From these experiments , we are convinced that [T probabilistic  T] models based on annotated corpora can effectively reduce the ambiguity in processing text and can be used to acquire lexical information from a corpu> , by supplementing knowledge-based techniques . Based on the results of those experiments , we have constructed a new natural language system ( PLUM ) for extracting data from text , e.g. , newswire text . </abstract>
<abstract> [A Answering Definition Questions A] With Multiple Knowledge Sources . Definition questions represent a largely unexplored area of [A question answering A] -- they are different from factoid questions in that the goal is to return as many relevant `` nuggets '' of information about a concept as possible . We describe a [T multi-strategy  T] approach to [A answering such questions A] using a database constructed offline with [T surface patterns T] , a Webbased [T dictionary  T], and an off-the-shelf [T document retriever T] . Results are presented from component-level evaluation and from an endto-end evaluation of our implemented system at the TREC 2003 [A Question Answering A] Track . </abstract>
<abstract> Using the [T Web T] for  Language Independent [A Spellchecking and Autocorrection A] . We have designed , implemented and evaluated an end-to-end system [A  spellchecking and autocorrection A] system that does not require any manually annotated training data . The [T World Wide Web T] is used as a large noisy corpus from which we infer knowledge about misspellings and word usage . This is used to build an [T error model T] and an [T n-gram language model T] . A small secondary set of news texts with artificially inserted misspellings are used to tune confidence classifiers . Because no manual annotation is required , our system can easily be instantiated for new languages . When evaluated on human typed data with real misspellings in English and German , our web-based systems outperform baselines which use candidate corrections based on hand-curated dictionaries . Our system achieves 3.8 % total error rate in English . We show similar improvements in preliminary results on artificial data for Russian and Arabic . </abstract>
<abstract> [A Word-Sense Disambiguation A] Using [T Statistical Models Of Roget 's Categories  T] Trained On Large Corpora . This paper describes a program that [A disambignates English word senses A] in unrestricted text using [T statistical T] models of the major [T Roget 's Thesaurus T] categories . Roget 's categories serve as approximations of conceptual classes . The categories listed for a word in Roger 's index tend to correspond to sense distinctions ; thus selecting the most likely category provides a useful level of sense disambiguatiou . The selection of categories is accomplished by identifying and weighting words that are indicative of each category when seen in context , using a [T Bayesian theoretical  T] framework . Other statistical approaches have required special corpora or hand-labeled training examples for much of the lexicon . Our use of class models overcomes this knowledge acquisition bottleneck , enabling training on unresUicted monolingual text without human intervention . Applied to the 10 million word Grolier 's Encyclopedia , the system correctly disambiguated 92 % of the instances of 12 polysemous words that have been previously studied in the literature . </abstract>
<abstract> A [T Semantic Kernel T] For [A Predicate Argument Classification A] . Automatically deriving semantic structures from text is a challenging task for machine learning . The flat feature representations , usually used in learning models , can only partially describe structured data . This makes difficult the processing of the semantic information that is embedded into parse-trees . In this paper a new [T  kernel T]  for automatic [A classification of predicate arguments A]has been designed and experimented . It is based on subparse-trees annotated with predicate argument information from PropBank corpus . This [T kernel T] , exploiting the convolution properties of the [T parse-tree kernel T] , enables us to learn which syntactic structures can be associated with the arguments defined in PropBank . [T Support Vector Machines T] ( [T SVMs T] ) using such a [T kernel T] [A classify arguments A] with a better accuracy than SVMs based on linear kernel . </abstract>
<abstract> The Alligator theorem prover for [T dependent type systems T] : Description and proof samples . This paper introduces the Alligator theorem prover for [T Dependent Type Systems T] ( [T dts T] ) . We start with highlighting a number of properties of [T dts T] that make them specifically suited for computational semantics . We then briefly introduce [T dts T] and our implementation . The paper concludes with an example of a dts proof that illustrates the suitability of dts for modelling [A  anaphora resolution A] . </abstract>
<abstract> The Hyperonym Problem Revisited : [T Conceptual And Lexical Hierarchies T] In [A Language Generation A] . When a lexical item is selected in the [A language production A] process , it needs to be explained why none of its superordinates gets selected instead , since their applicability conditions are fulfilled all the same . This question has received much attention in cognitive modelling and not as much in other branches of NLG . This paper describes the various approaches taken , discusses the reasons why they are so different , and argues that [A production A] models using  [T symbolic representations T] should make a distinction between [T conceptual and lexical hierarchies T] , which can be organized along fixed levels as studied in ( some branches of ) lexical semantics . </abstract>
<abstract> Learning [A Subjective Language A] . [A Subjectivity A] in natural language refers to aspects of language used to express opinions , evaluations , and speculations . There are numerous natural language processing applications for which [A subjectivity A] analysis is relevant , including information extraction and text categorization . The goal of this work is learning [A subjective language A] from corpora . Clues of subjectivity are generated and tested , including low-frequency words , [T collocations T] , and adjectives and verbs identified using  [T  distributional similarity T] . The features are also examined working together in concert . The features , generated from different data sets using different procedures , exhibit consistency in performance in that they all do better and worse on the same data sets . In addition , this article shows that the density of subjectivity clues in the surrounding context strongly affects how likely it is that a word is subjective , and it provides the results of an annotation study assessing the [A subjectivity A] of sentences with [T high-density features T] . Finally , the clues are used to perform [A opinion piece recognition  A] ( a type of [A text categorization A] and [A genre detection A] ) to demonstrate the utility of the knowledge acquired in this article . </abstract>
<abstract> French [A Order A] Without Order . To account for the [A semi-free word order A] of French , [T Unification Categorial Grammar T] is extended in two ways . First , verbal valencies are contained in a set rather than in a list . Second , type-raised NP 's are described as two-sided functors . The new framework does not overgenerate i.e. , it accepts all and only the sentences which are grammatical . This follows partly from the elimination of false lexical ambiguities - i.e. , ambiguities introduced in order to account for all the possible positions a word can be in within a sentence - and partly from a system of features constraining the possible combinations . </abstract>
<abstract> [A LFG Semantics A] Via [T Constraints T] . Semantic theories of natural language associate meanings with utterances by providing meanings for lexical items and rules for determining the meaning of larger units given the meanings of their parts . Traditionally , meanings are combined via function composition , which works well when constituent structure trees are used to guide semantic composition . More recently , the functional structure of [A LFG A] has been used to provide the syntactic information necessary for constraining derivations of meaning in a cross-linguistically uniform format . It has been difficult , however , to reconcile this approach with the combination of meanings by function composition . In contrast to compositional approaches , we present a [T deductive  T] approach to [A assembling meanings A] , based on reasoning with [T constraints T] , which meshes well with the unordered nature of information in the functional structure . Our use of [T linear logic T] as a ` glue ' for [A assembling meanings A] also allows for a coherent treatment of modification as well as of the LFG requirements of completeness and coherence . </abstract>
<abstract> [A Interaction Grammars A] . [A Interaction Grammars A] ( [A IG A] ) are a new linguistic formalism which is based on descriptions of under ~ specified trees in the fl ` amework of [T intuitionistic linear logic T] ( [T ILL T] ) . Syntactic composition , which is expressed by deduction in [T  linear logic T] , is controlled by a system of polarized features . In this way , [A parsing A] amounts to generating models of tree descriptions and it is implemented as a [T constraint satisfaction problem T] . </abstract>
<abstract> [A Query Expansion A] Using Domain Information in [T Compounds T] . This paper describes a [A query expansion A] strategy for [A domain specific information retrieval A] . Components of [T compounds T] are used selectively . Only parts belonging to the same domain as the [T compound T] itself will be used in expanded queries . </abstract>
<abstract> SenseLearner : [A Word Sense Disambiguation A] For All Words In Unrestricted Text . This paper describes SENSELEARNER -- a [T minimally supervised T] [A word sense disambiguation A] system that attempts to disambiguate all content words in a text using [T WordNet senses T] . We evaluate the accuracy of SENSELEARNER on several standard sense-annotated data sets , and show that it compares favorably with the best results reported during the recent SENSEVAL evaluations . </abstract>
<abstract> [T Rhetoric T] As Knowledge . A proper assessment of the relation between discourse structure and speaker 's communicative intentions requires a better understanding of communicative intentions . This contribution proposes that there is a crucial difference between intending the hearer to entertain a certain belief ( or desire , or intention ) , and intending to affect the strength with which the hearer entertains the belief ( or desire , or intention ) . [T Rhetoric T] , if defined as a body of knowledge about how discourse structure affects the strength with which a discourse participant entertains beliefs , desires , and intentions , can be seen to play a precise and crucial role in the planning of  [A  discourse A] . </abstract>
<abstract> Interleaving  Syntax And Semantics In An Efficient [T Bottom-Up Parser T] . We describe an efficient [T bottom-up T] [A parser A] that interleaves syntactic and semantic structure building . Two techniques are presented for reducing search by reducing local ambiguity : Limited [T leftcontext constraints T] are used to reduce local syntactic ambiguity , and [T deferred sortal-constraint T] application is used to reduce local semantic ambiguity . We experimentally evaluate these techniques , and show dramatic reductions in both number of chart edges and total [A parsing A] time . The robust processing capabilities of the [A parser A] are demonstrated in its use in improving the accuracy of a [A speech recognizer A] . </abstract>
<abstract> [T Class-Based Collocations T] For [A Word Sense Disambiguation A] . This paper describes the NMSU-Pitt-UNCA [A word-sense disambiguation A] system participating in the Senseval-3 English lexical sample  task . The focus of the work is on using [T semantic class-based collocations T] to augment traditional word-based collocations . Three separate sources of word relatedness are used for these collocations : 1 ) [T WordNet hypernym relations T] ; 2 ) [T cluster-based word similarity classes T] ; and 3 ) [T dictionary definition analysis T] . </abstract>
<abstract> The [T Character-based CRF T] [A Segmenter A] of MSRA&NEU for the 4th Bakeoff . This paper describes the [A Chinese Word Segmenter A] for the fourth International Chinese Language Processing Bakeoff . Base on [T Conditional Random Field  T] ( [T CRF T] ) model , a basic segmenter is designed as a problem of [A character-based tagging A] . To further improve the performance of our segmenter , we employ a [T word-based T] approach to increase the in-vocabulary ( IV ) word recall and a post-processing to increase the out-of-vocabulary ( OOV ) word recall . We participate in the [A word segmentation A] closed test on all five corpora and our system achieved four second best and one the fifth in all the five corpora . </abstract>
<abstract> [T Jointly T] Identifying [A Predicates , Arguments and Senses A] using [T Markov Logic T] . In this paper we present a [T Markov Logic Network T] for [A Semantic Role Labelling A] that jointly performs [A predicate identification , frame disambiguation , argument identification and argument classification A] for all predicates in a sentence . Empirically we find that our approach is competitive : our best model would appear on par with the best entry in the CoNLL 2008 shared task open track , and at the 4th place of the closed track -- right behind the systems that use significantly better parsers to generate their input features . Moreover , we observe that by fully capturing the complete [A SRL A] pipeline in a single [T  probabilistic  T] model we can achieve significant improvements over more isolated systems , in particular for out-of-domain data . Finally , we show that despite the joint approach , our system is still efficient . </abstract>
<abstract> Improving [A Web Search Relevance A] with [T Semantic Features T] . Most existing [A information retrieval  A] ( [A IR A] ) systems do not take much advantage of natural language processing ( NLP ) techniques due to the complexity and limited observed effectiveness of applying NLP to IR . In this paper , we demonstrate that substantial gains can be obtained over a strong baseline using [T NLP T]  techniques , if properly handled . We propose a framework for deriving [T semantic text matching T] features from [T named entities T] identified in Web queries ; we then utilize these features in a [T supervised machine-learned T] [A ranking A]  approach , applying a set of emerging [T machine learning T] techniques . Our approach is especially useful for queries that contain multiple types of concepts . Comparing to a major commercial Web search engine , we observe a substantial 4 % DCG5 gain over the affected queries . </abstract>
<abstract> A [T Web-Based T]   Interactive Computer Aided [A Translation  A] Tool . We developed caitra , a novel tool that aids human translators by ( a ) making suggestions for sentence completion in an interactive [A  machine translation A] setting , ( b ) providing alternative word and phrase translations , and ( c ) allowing them to postedit machine translation output . The tool uses the [T Moses decoder T] , is implemented in Ruby on Rails and C + + and delivered over the web . </abstract>
<abstract> Outline Of The International Standard Linguistic Annotation  Framework . This paper describes the outline of a linguistic annotation  framework under development by ISO TC37 SC WG1-1 . This international standard provides an architecture for the  creation , annotation , and manipulation of linguistic resources and processing software . The goal is to provide maximum flexibility for encoders and annotators , while at the same time enabling interchange and re-use of annotated linguistic resources . We describe here the outline of the standard for the purposes of enabling annotators to begin to explore how their schemes may map into the framework . </abstract>
<abstract> Estimating the Reliability of  [T MDP Policies T] : a [T Confidence Interval T] Approach . Past approaches for using [T reinforcement learning T] to derive [A dialog A] control policies have assumed that there was enough collected data to derive a reliable policy . In this paper we present a methodology for numerically constructing con dence intervals for the expected cumulative reward for a learned policy . These intervals are used to ( 1 )  better assess the reliability of the expected cumulative reward , and ( 2 ) perform a re ned comparison between policies derived from different [T Markov Decision Processes  T] ( [T MDP T] ) models . We applied this methodology to a prior experiment where the goal was to select the best features to include in the MDP statespace . Our results show that while some of the policies developed in the prior work exhibited very large con dence intervals , the policy developed from the best feature set had a much smaller con dence interval and thus showed very high reliability . </abstract>
<abstract> A Plethora Of Methods For Learning  English Countability . This paper compares a range of methods for [A classifying words A] based on [T linguistic diagnostics T] , focusing on the task of [T learning countabilities T] for English nouns . We propose two basic approaches to feature representation : [T distribution-based representation T] , which simply looks at the distribution of features in the corpus data , and [T agreement-based representation T] which analyses the level of tokenwise agreement between multiple preprocessor systems . We additionally compare a single multiclass classifier architecture with a suite of binary classifiers , and combine analyses from multiple preprocessors . Finally , we present and evaluate a [T feature selection T] method . </abstract>
<abstract> Measuring The [A Relative Compositionality Of Verb-Noun ( V-N ) A] [T Collocations T] By Integrating Features . Measuring the [A relative compositionality of Multi-word Expressions ( MWEs ) A] is crucial to Natural Language Processing . Various [T collocation T] based measures have been proposed to compute the [A relative compositionality of MWEs A] . In this paper , we define novel measures ( both  [T collocation T] based and [T context T] based measures ) to measure the [A relative compositionality of MWEs of V-N type A] . We show that the correlation of these features with the human ranking is much superior to the correlation of the traditional features with the human ranking . We then integrate the proposed features and the traditional features using a [T SVM T] based ranking function to rank the [T collocations T] of [A V-N type A] based on their [A relative compositionality A] . We then show that the correlation between the ranks computed by the [T SVM T] based ranking function and human ranking is significantly better than the correlation between ranking of individual features and human ranking . </abstract>
<abstract> Improving [A Parsing A] Accuracy By  Combining Diverse [A Dependency Parsers A] . This paper explores the possibilities of improving [A parsing A] results by combining outputs of several [A parsers A] . To some extent , we are porting the ideas of Henderson and Brill ( 1999 ) to the world of dependency structures . We differ from them in  exploring [T context features T]  more deeply . All our experiments were conducted on Czech but the method is language-independent . We were able to significantly improve over the best parsing result for the given setting , known so far . Moreover , our experiments show that even parsers far below the state of the art can contribute to the total improvement . </abstract>
<abstract> Detecting Problematic Turns In [A  Human-Machine Interactions A] : [T Rule-Induction T] Versus [T Memory-Based Learning T] Approaches . We address the issue of on-line detection of communication problems in  [A  spoken dialogue A] systems . The usefulness is investigated of the sequence of system question types and the [T word graphs T] corresponding to the respective user utterances . By applying both [T ruleinduction T] and [T memory-based learning T] techniques to data obtained with a Dutch train time-table information system , the current paper demonstrates that the aforementioned features indeed lead to a method for problem detection that performs significantly above baseline . The results are interesting from a [A dialogue A] perspective since they employ features that are present in the majority of [A spoken dialogue A] systems and can be obtained with little or no computational overhead . The results are interesting from a [T machine learning T] perspective , since they show that the [T rule-based T] method performs significantly better than the [T memory-based T] method , because the former is better capable of representing interactions between features . </abstract>
<abstract> REFTEX - A [T Context-Based T] [A Translation Aid A] . The system presented in this paper produces [A bilingual passages of text A] from an original ( source ) text and one ( or more ) of its translated versions . The source text passage includes words or word compounds which a translator wants to retrieve for the current translating of another text . The target text passage is the equivalent version of the source text passage . On the basis of a comparison of the [T contexts T] of these words in the concorded passage and his own text , the translator has to decide on the utility of the [A translation A] proposed in the target text passage . The program might become a component of translator 's work bench . </abstract>
<abstract> [T Timestamped Graphs : Evolutionary T] Models of Text for [A Multi-Document Summarization A] . Current graph-based approaches to automatic [A  text summarization A] , such as LexRank and TextRank , assume a static graph which does not model how the input texts emerge . A suitable [T evolutionary text graph T] model may impart a better understanding of the texts and improve the summarization process . We propose a [T timestamped graph ( TSG )  T] model that is motivated by human writing and reading processes , and show how text units in this model emerge over time . In our model , the graphs used by LexRank and TextRank are specific instances of our [T timestamped graph T] with particular parameter settings . We apply [T timestamped graphs T] on the standard [A DUC multi-document text summarization A] task and achieve comparable results to the state of the art . </abstract>
<abstract> [A Cross-Document Coreference A] On A Large Scale Corpus . In this paper , we will compare and evaluate the effectiveness of different [T statistical T] methods in the task of [A cross-document coreference resolution A] . We created [T entity T] models for different test sets and compare the following [T disambiguation and clustering T] techniques to [T cluster T] the [T entity T] models in order to create  [A coreference chains A] : [T Incremental Vector Space KL-Divergence Agglomerative Vector Space  T] </abstract>
<abstract> [T Learning T] to [A Merge Word Senses A] . It has been widely observed that different NLP applications require different sense granularities in order to best exploit [A word sense A] distinctions , and that for many applications WordNet senses are too fine-grained . In contrast to previously proposed automatic methods for [A sense clustering A] , we formulate [A sense merging A] as a [T supervised learning T] problem , exploiting human-labeled [A sense clusterings A] as training data . We train a [T discriminative classifier T] over a wide variety of features derived from [T WordNet structure T] , corpus-based evidence , and evidence from other lexical resources . Our learned similarity measure outperforms previously proposed automatic methods for [A sense clustering A] on the task of predicting human sense merging judgments , yielding an absolute F-score improvement of 4.1 % on nouns , 13.6 % on verbs , and 4.0 % on adjectives . Finally , we propose a model for [A clustering sense taxonomies A] using the outputs of our classifier , and we make available several automatically sense-clustered WordNets of various sense granularities . </abstract>
<abstract> The NomBank Project : An Interim Report . This paper describes NomBank  , a project that will provide [A argument structure A] for instances of common nouns in the Penn Treebank II corpus . NomBank is part of a larger effort to add additional layers of annotation to the Penn Treebank II corpus . The University of Pennsylvania 's PropBank , NomBank and other annotation projects taken together should lead to the creation of better tools for the automatic analysis of text . This paper describes the NomBank project in detail including its speci cations and the process involved in creating the resource . </abstract>
<abstract> [T Supervised T] [A Grammar Induction A] Using Training Data With Limited Constituent Information . Corpus-based [A grammar induction A] generally relies on hand-parsed training data to learn the structure of the language . Unfortunately , the cost of building large annotated corpora is prohibitively expensive . This work aims to improve the [A induction A] strategy when there are few labels in the training data . We show that the most informative linguistic constituents are the higher nodes in the parse trees , typically denoting complex noun phrases and sentential clauses . They account for only 20 % of all constituents . For [A inducing grammars A] from sparsely labeled training data ( e.g. , only higher-level constituent labels ) , we propose an [T adaptation T] strategy , which produces grammars that parse almost as well as grammars induced from fully labeled corpora . Our results suggest that for a partial parser to replace human annotators , it must be able to automatically extract higher-level constituents rather than base noun phrases . </abstract>
<abstract> Learning [A Word Meanings A] And [A Descriptive Parameter Spaces A] From [T Music T] . The audio bitstream in music encodes a high amount of statistical , acoustic , emotional and cultural information . But music also has an important linguistic accessory ; most musical artists are described in great detail in record reviews , fan sites and news items . We highlight current and ongoing research into extracting relevant [T features from audio T] and simultaneously learning [T language features linked to the music T] . We show results in a [A `` query-bydescription '' task A] in which we learn the perceptual meaning of automatically-discovered single-term descriptive components , as well as a method of automatically uncovering ` [T semantically attached T] ' terms ( terms that have [T perceptual grounding T] . ) We then show recent work in ` [A semantic basis functions A] ' -- parameter spaces of description ( such as fast ... slow or male ... female ) that encode the highest descriptive variance in a semantic space . </abstract>
<abstract> The Benefit Of [T Stochastic T] PP Attachment To A [T Rule-Based T] [A Parser A] . To study PP attachment disambiguation as a benchmark for empirical methods in natural language processing it has often been reduced to a binary decision problem ( between verb or noun attachment ) in a particular syntactic configuration . A parser , however , must solve the more general task of deciding between more than two alternatives in many different contexts . We combine the attachment predictions made by a simple model of [T lexical attraction T] with a full-fledged [A parser A] of German to determine the actual benefit of the subtask to [A parsing A] . We show that the combination of [T data-driven T] and [T rule-based T] components can reduce the number of all [A parsing A] errors by 14 % and raise the attachment accuracy for [A dependency parsing A] of German to an unprecedented 92 % . </abstract>
<abstract> The Derivation Of A [A Grammatically Indexed Lexicon A] From The [T Longman Dictionary Of Contemporary English T] . We describe a methodology and associated software system for the construction of a large [A lexicon A] from an existing machine-readable ( published ) [T dictionary T] . The [A lexicon A] serves as a component of an English morphological and syntactic analyesr and contains entries with grammatical definitions compatible with the word and sentence grammar employed by the analyser . We describe a software system with two integrated components . One of these is capable of extracting syntactically rich , theory-neutral lexical templates from a suitable machine-readabh source . The second supports interactive and semi-automatic generation and testing of target lexical entries in order to derive a sizeable , accurate and consistent [A lexicon A] from the source [T dictionary T] which contains partial ( and occasionally inaccurate ) information . Finally , we evaluate the utility of the [T Longman Dictionary of Contemporary EnglgsA T] as a suitable source dictionary for the target lexicon . </abstract>
<abstract> Formal Description Of [A Multi-Word Lexemes A] With The [T Finite-State Formalism IDAREX T] . Most [A multi-word lexemes A] ( [A MWLs A] ) allow certain types of variation . This has to be taken into account for their description and their recognition in texts . We suggest to describe their syntactic restrictions and their idiosyncratic peculiarities with local grammar rules , which at the same time allow to express in a general way regularities valid for a whole class of [A MWLs A] . The local grammars can be written in a very convenient and compact way as regular expressions in the formalism [T IDAREX T] which uses a two-level [T morphology T] . [T IDAREX T] allows to define various types of variables , and to mix canonical and inflected word forms in the regular expressions . </abstract>
<abstract> [A Modeling Chinese Documents A] with [T Topical Word-Character Models T] . As Chinese text is written without word boundaries , effectively recognizing Chinese words is like recognizing collocations in English , substituting characters for words and words for collocations . However , existing topical models that involve collocations have a common limitation . Instead of directly assigning a topic to a collocation , they take the topic of a word within the collocation as the topic of the whole collocation . This is unsatisfactory for topical [A modeling of Chinese documents A] . Thus , we propose a [T topical word-character  T] model ( [T TWC T] ) , which allows two distinct types of topics : [T word topic T] and [T character topic T] . We evaluated [T TWC T] both qualitatively and quantitatively to show that it is a powerful and a promising [T topic model T] . </abstract>
<abstract> Parsing Word-Aligned Parallel Corpora In A [A Grammar Induction A] Context . We present an [T Earley-style dynamic programming  T] algorithm for [A parsing sentence pairs  A] from a parallel corpus simultaneously , building up two phrase structure trees and a correspondence mapping between the nodes . The intended use of the algorithm is in [A bootstrapping grammars A] for less studied languages by using implicit [T grammatical information  T] in parallel corpora . Therefore , we presuppose a given [T ( statistical ) word alignment T] underlying in the [A synchronous parsing  A] task ; this leads to a significant reduction of the [A parsing A] complexity . The theoretical complexity results are corroborated by a quantitative evaluation in which we ran an implementation of the algorithm on a suite of test sentences from the Europarl parallel corpus . </abstract>
<abstract> On Identifying  [A Sets A] . A range of research has explored the problem of generating referring expressions that uniquely identify a single entity from the shared context . But what about expressions that identify sets of entities ? In this paper , I adapt recent semantic research on plural descriptions -- using [T covers T] to abstract collective and distributive readings and using sets of assignments to represent [T dependencies T] among references -- to describe a search problem for [A set-identifying expressions A] that largely mirrors the search problem for singular referring expressions . By structuring the search space only in terms of the words that can be added to the description , the proposal defuses potential combinatorial explosions that might otherwise arise with reference to sets . </abstract>
<abstract> NUS-ML : Improving [A Word Sense Disambiguation A] Using [T Topic Features T] . We participated in SemEval-1 English [A coarse-grained all-words A] task ( task 7 ) , English [A fine-grained all-words A] task ( task 17 , subtask 3 ) and English [A coarse-grained lexical sample A] task ( task 17 , subtask 1 ) . The same method with different labeled data is used for the tasks ; SemCor is the labeled corpus used to train our system for the allwords tasks while the labeled corpus that is provided is used for the [A lexical sample A] task . The knowledge sources include [T part-of-speech T] of neighboring words , single words in the surrounding [T context , local collocations , and syntactic patterns T] . In addition , we constructed a [T topic feature T] , targeted to capture the global [T context T] information , using the [T latent dirichlet allocation ( LDA ) algorithm T] with unlabeled corpus . A modified [T na ¨ ıve Bayes classifier T] is constructed to incorporate all the features . We achieved 81.6 % , 57.6 % , 88.7 % for [A coarse-grained allwords A] task , [A fine-grained all-words A] task and [A coarse-grained lexical sample  A] task respectively . </abstract>
<abstract> COOPML : Towards Annotating [A Cooperative Discourse A] . In this paper , we present a preliminary version of COOPML , a language designed for annotating [A cooperative discourse A] . We investigate the different [T linguistic marks T] that identify and characterize the different forms of [A cooperativity A] found in written texts from FAQs , Forums and emails  . </abstract>
<abstract> Automatic Detection Of Omissions In [A Translations A] . ADOMIT is an algorithln for Automatic Detection of OMissions in [A Translations A] . The algorithm relies solely on [T geometric analysis of bitext maps T] and uses no linguistic information . This property allows it to deal equally well with omissions that do not correspond to linguistic units , such as might result ti'om word-processing mishaps . ADOMIT has proven itself by discovering many errors in a handconstructed gold standard for evaluating bitext mapping algorithms . Quantitative evaluation on simulated omissions showed that , even with today 's poor bitext mapping technology , ADOMIT is a valuable quality control tool for translators and translation bureaus . </abstract>
<abstract> On using [T Articulatory Features T] for [T Discriminative T] [A Speaker Adaptation A] . This paper presents a way to perform [A speaker adaptation for automatic speech recognition A] using the [T stream weights in a multi-streamsetup T] , whichincludedacoustic models for [T `` Articulatory Features '' T] such as ROUNDED or VOICED . We present [T supervised T] [A speaker adaptation A] experiments on a spontaneous [A speech A] task and compare the above [T stream-based T] approach to conventional approaches , in which the models , and not stream combination weights , are being adapted . In the approach we present , [T stream weights T] model the importance of features such as [T VOICED T] for word discrimination , which offers a descriptive interpretation of the adaptation parameters . </abstract>
<abstract> Automatically Extracting [A Nominal Mentions Of Events A] With A [T Bootstrapped Probabilistic Classifier T] . Most approaches to [A event extraction A] focus on mentions anchored in verbs . However , many mentions of events surface as noun phrases . Detecting them can increase the recall of [A event extraction A] and provide the foundation for detecting relations between events . This paper describes a [T weaklysupervised  T] method for detecting [A  nominal event mentions A] that combines techniques from [T word sense disambiguation ( WSD ) andlexicalacquisitiontocreateaclassifier T] thatlabelsnounphrasesasdenotingevents or non-events . The classifier uses [T bootstrapped probabilistic generative T] models of the contexts of events and non-events . Thecontextsarethelexically-anchoredsemantic dependency relations that the NPs appear in . Our method dramatically improves with [T bootstrapping T] , and comfortably outperforms lexical lookup methods whicharebasedonverymuchlargerhandcrafted resources . </abstract>
<abstract> [A Machine-Readable Dictionaries A] . The papers in this panel consider [A machine-readable dictionaries A] from several perspectives : research in computational linguistics and computational lexicology , the development of tools for improving accessibility , the design of lexical reference systems for educational purposes , and applications of [A machine-readable dictionaries A] in information science contexts . As background and by way of introduction , a description is provided of a workshop on [A machine-readable dictionaries A] that was held at SRI International in April 1983 . </abstract>
<abstract> [A Language Identification A] With [T Confidence Limits T] . A [T statistical classification  T] algorithm and its application to [A language identification  A] from noisy input are described . The main innovation is to compute [T confidence limits on the classification T] , so that the algorithm terminates when enough evidence to make a clear decision has been made , and so avoiding problems with categories that have similar characteristics . A second application , to [A genre identification A] , is briefly examined . The results show that some of the problems of other [A language identification A] techniques can be avoided , and illustrate a more important point : that a [T statistical  T] language process can be used to provide feedback about its own success rate . </abstract>
<abstract> Learning [A  Subjective Nouns A] Using [T Extraction Pattern Bootstrapping T] . We explore the idea of creating a [A subjectivity A] classifier that uses lists of subjective nouns learned by [T bootstrapping T] algorithms . The goal of our research is to develop a system that can distinguish [A subjective A] sentences from objective sentences . First , we use two [T bootstrapping  T] algorithms that exploit [T extraction patterns T] to learn sets of [A subjective nouns A] . Then we train a [T Naive Bayes  T] classifier using the subjective nouns , [T discourse features , and subjectivity clues T] identified in prior research . The [T bootstrapping  T] algorithms learned over 1000 [A subjective nouns A] , and the subjectivity classifier performed well , achieving 77 % recall with 81 % precision . </abstract>
<abstract> Choosing Sense Distinctions for [A WSD A] : Psycholinguistic Evidence . Supervised [A word sense disambiguation A] requires training corpora that have been tagged with word senses , which begs the question of which word senses to tag with . The default choice has been WordNet , with its broad coverage and easy accessibility . However , concerns have been raised about the appropriateness of its fine-grained word senses for WSD . [A WSD A] systems have been far more successful in distinguishing coarsegrained senses than fine-grained ones ( Navigli , 2006 ) , but does that approach neglect necessary meaning differences ? Recent psycholinguistic evidence seems to indicate that closely related word senses may be represented in the mental lexicon much like a single sense , whereas distantly related senses may be represented more like discrete entities . These results suggest that , for the purposes of [A WSD A] , closely related word senses can be [T clustered T] together into a more general sense with little meaning loss . The current paper will describe this psycholinguistic research and its implications for automatic [A  word sense disambiguation A] . </abstract>
<abstract> A [T Block-Based T] Robust [A Dependency Parser A] For Unrestricted Chinese Text . Although substantial efforts have been made to [A parse A] Chinese , very few have been practically used due to incapability of handling unrestricted texts . This paper realizes a practical system for Chinese [A parsing A] by using a [T hybrid T] model of [A phrase structure partial parsing A] and [A dependency parsing A] . This system showed good performance and high robustness in [A parsing A] unrestricted texts and has been applied in a successful [A machine translation A] product . </abstract>
<abstract> [T Ends-Based T] [A Dialogue Processing A] . We describe a reusable and scalable [A dialogue A] toolbox and its application in multiple systems . Our main claim is that [T ends-based T] representation and processing throughout the complete dialogue backbone it essential to our approach . </abstract>
<abstract> Relative Compositionality of [A Multi-word Expressions A] : A Study of [A Verb-Noun ( V-N ) Collocations A] . Recognition of [A Multi-word Expressions ( MWEs ) A] and their relative compositionality are crucial to Natural Language Processing . Various statistical techniques have been proposed to recognize [A MWEs A] . In this paper , we integrate all the existing [T  statistical features T] and investigate a range of [T classifiers T] for their suitability for recognizing the [A non-compositional Verb-Noun ( V-N ) collocations A] . In the task of [A ranking the V-N collocations A] based on their relative compositionality , we show that the correlation between the ranks computed by the classifier and human ranking is significantly better than the correlation between ranking of individual features and human ranking . We also show that the properties [T ` Distributed frequency of object ' T] ( as defined in ( 27 ) ) and ` Nearest [T Mutual Information T] ' ( as adapted from ( 18 ) ) contribute greatly to the recognition of the [A non-compositional MWEs of the V-N type A] and to the [A ranking of the V-N collocations A] based on their relative compositionality . </abstract>
<abstract> [A Machine Translation A] As [T Lexicalized Parsing With Hooks T] . We adapt the [T `` hook '' trick T] for speeding up bilexical [T parsing T] to the decoding problem for  [A machine translation A] models that are based on combining a [T synchronous context free grammar T] as the translation model with an [T n-gram  T] language model . This[T  dynamic programming T] technique yields lower complexity algorithms than have previously been described for an important class of [A translation A] models . </abstract>
<abstract> [T Path-Based T] And [T Node-Based T] Inference In [A Semantic Networks A] . Two styles of performing inference in [A semantic networks A] are presented and compared . [T Path-based T] inference allows an arc or a path of arcs between two given nodes to be inferred from the existence of another specified path between the same two nodes . [T Path-based T] inference rules may be written using a [T binary relational calculus notation T] . [T Node-based T] inference allows a structure of nodes to be inferred from the existence of an instance of a pattern of node structures . [T Node-based T] inference rules can be constructed in a [A semantic network A] using a variant of a [T predicate calculus notation T] . [T Path-based T] inference is more efficient , while [T node-based T] inference is more general . A method is described of combining the two styles in a single system in order to take advantage of the strengths of each . Applications of [T path-based T] inference rules to the representation of the extensional equivalence of intensional concepts , and to the explication of inheritance in hierarchies are sketched . </abstract>
<abstract> Automatic [T  Set Expansion T] for [A List Question Answering A] . This paper explores the use of [T set expansion ( SE ) T] to improve [A question answering A] ( [A QA A] ) when the expected answer is a list of entities belonging to a certain class . Given a small set of seeds , [T SE T] algorithms mine textual resources to produce an extended list including additional members of the class represented by the seeds . We explore the hypothesis that a [T noise-resistant SE  T] algorithm can be used to extend candidate answers produced by a [A QA A]  system and generate a new list of answers that is better than the original list produced by the [A QA A] system . We further introduce a [T hybrid T] approach which combines the original answers from the [A QA A] system with the output from the [T SE  T] algorithm . Experimental results for several state-of-the-art [A QA A] systems show that the [T hybrid T] system performs better than the [A QA A] systems alone when tested on list question data from past TREC evaluations . </abstract>
<abstract> Reuse Of A [T Proper Noun Recognition  T] System In Commercial And Operational NLP Applications . SRA 's proprietary product , NameTag TM , which provides fast and accurate [T name recognition T] , has been reused in many applications in recent and ongoing efforts , including multilingual information retrieval and browsing , text clustering , and assistance to manual text indexing . This paper reports on SRA 's experience in embedding [T name recognition T] in these three specific applications , and the mutual impacts that occur , both on the algorithmic level and in the role that [T name recognition T] plays in user interaction with a system . In the course of this , we touch upon various interactions between [T proper name recognition T] and [A machine translation ( MT )  A], as well as the role of accurate name recognition in improving the performance of [A word segmentation algorithms A] needed for languages whose writing systems do not segment words . </abstract>
<abstract> [A Referring Expression Generation A] Using [T Speaker-based Attribute Selection T] and Trainable [T Realization ( ATTR ) T] . In the first REG competition , researchers proposed several general-purpose algorithms for attribute selection for [A referring expression generation A] . However , most of this work did not take into account : a ) stylistic differences between speakers ; or b ) trainable [T surface realization T] approaches that combine [T semantic T] and [T word order T] information . In this paper we describe and evaluate several end-to-end [A referring expression generation A]  algorithms that take into consideration [T speaker style T] and use [T data-driven surface realization T] techniques . </abstract>
<abstract> Integrating [T  Syntactic Priming  T]  Into An Incremental [T Probabilistic Parser T], With An Application To [A Psycholinguistic Modeling A] . The psycholinguistic literature provides evidence for syntactic priming , i.e. , the tendency to repeat structures . This paper describes a method for [T incorporating priming into an incremental probabilistic parser T] . Three models are compared , which involve [T priming of rules between sentences , within sentences , and within coordinate structures T] . These models simulate the reading time advantage for parallel structures found in human data , and also yield a small increase in overall [A parsing A] accuracy . </abstract>
<abstract> [T Biology Based T] Alignments of [A Paraphrases for Sentence Compression A] . 1 In this paper , we present a study for extracting and aligning [A paraphrases A] in the context of [A Sentence Compression A] . First , we justify the application of a new measure for the automatic extraction of [A  paraphrase  A] corpora . Second , we discuss the work done by ( Barzilay & Lee , 2003 ) who use clustering of paraphrases to induce rewriting rules . We will see , through classical visualization methodologies ( Kruskal & Wish , 1977 ) and exhaustive experiments , that clustering may not be the best approach for automatic pattern identification . Finally , we will provide some results of different [T biology based  T] methodologies for pairwise  [A paraphrase A] alignment . 1 Introduction Sentence Compression can be seen as the removal of redundant words or phrases from an input sentence by creating a new sentence in which the gist of the original meaning of the sentence remains unchanged . Sentence Compression takes an important place for Natural Language Processing ( NLP ) tasks where specific constraints must be satisfied , such as length in summarization ( Barzilay & Lee , 2002 ; Knight & Marcu , 2002 ; Shinyama et al. , 2002 ; Barzilay & Lee , 2003 ; Le Nguyen & Ho , 2004 ; Unno et al. , 2006 ) , style in text simplification ( Marsi & Krahmer , 2005 ) or sentence simplification for subtitling ( Daelemans et al. , 2004 ) . </abstract>
<abstract> Semi-Automatic Practical [A  Ontology Construction A] By Using A [T Thesaurus , Computational Dictionaries , And Large Corpora T] . This paper presents the [T semi-automatic T] construction  method of a practical [A  ontology A] by using various resources . In order to acquire a reasonably practical ontology in a limited time and with less manpower , we extend the [T  Kadokawa thesaurus T] by inserting additional [T semantic relations T] into its hierarchy , which are classified as case relations and other semantic relations . The former can be obtained by converting valency information and case frames from previously-built computational dictionaries used in machine translation . The latter can be acquired from concept co-occurrence information , which is extracted automatically from large corpora . The ontology stores rich semantic constraints among 1,110 concepts , and enables a natural language processing system to resolve semantic ambiguities by making inferences with the concept network of the ontology . In our practical [A machine translation  A] system , our [A ontology-based A] [A word sense disambiguation A] method achieved an 8.7 % improvement over methods which do not use an ontology for Korean translation . </abstract>
<abstract> The RWTH [T System Combination T] System for [A WMT A] 2009 . RWTH participated in the [T System Combination T] task of the Fourth Workshop on [T Statistical T] [A Machine Translation A] ( WMT 2009 ) . Hypotheses from 9 German → English MT systems were combined into a consensus [A translation A] . This consensus [A translation A] scored 2.1 % better in BLEU and 2.3 % better in TER ( abs . ) than the best single system . In addition , cross-lingual output from 10 French , German , and Spanish → English systems was combined into a consensus [A translation A] , which gave an improvement of 2.0 % in BLEU\/3 .5 % in TER ( abs . ) over the best single system . </abstract>
<abstract> [A Event Matching A] Using the [T Transitive Closure of Dependency Relations T] . This paper describes a novel [A event-matching  A] strategy using features obtained from the [T transitive closure of dependency relations T] . The method yields a model capable of [A matching events A] with an F-measure of 66.5 % . </abstract>
<abstract> [T Boosting T] Applied To [A Tagging A] And [A PP Attachment A] . [T Boosting T] is a machine learning algorithm that is not well known in computational linguistics . We apply it to [A part-of-speech tagging A] and [A prepositional phrase attachment A] . Performance is very encouraging . We also show how to improve data quality by using [T boosting T] to identify [A annotation errors A] . </abstract>
<abstract> [A Named Entity Recognition A] for South Asian Languages . Much work has already been done on building [A named entity recognition A] systems . However most of this work has been concentrated on English and other European languages . Hence , building a [A named entity recognition A] ( [A NER A] ) system for South Asian Languages ( SAL ) is still an open problem because they exhibit characteristics different from English . This paper builds a [A named entity recognizer A] which also identifies [A nested name entities A] for the Hindi language using [T machine learning  T] algorithm , trained on an annotated corpus . However , the algorithm is designed in such a manner that it can easily be ported to other South Asian Languages provided the necessary NLP tools like [T POS tagger T] and [T chunker T] are available for that language . I compare results of Hindi data with English data of CONLL shared task of 2003 . </abstract>
<abstract> Large [A Lexicons A] For Natural Language Processing : Utilising The [T Grammar Coding T] System Of LDOCE . This article focusses on the derivation of large [A lexicons A] for natural language processing . We describe thedevelopment of a  [A dictionary A] support environment  linking a restructured version of the Longman Dictionary of Contemporary English to natural language processing systems . The process of [T restructuring T] the information in the [A machine readable version of the dictionary A] is discussed . The [T Longman grammar code T] system is used to construct [A ` theory neutral ' lexical entries A] . We demonstrate how such lexical entries can be put to practical use by linking up the system described here with the experimental PATR-II grammar development environment . Finally , we offer an evaluation of the utility of the [T grammar coding T] system for use by automatic natural language  [A parsing A] systems . </abstract>
<abstract> Towards Using [T Prosody T] In [A Speech Recognition\/Understanding  A] Systems : Differences Between Read And Spontaneous Speech . A persistent problem for keyword-driven [A speech recognition A] systems is that users often embed the to-be-recognized words or phrases in longer utterances . The recognizer needs to locate the relevant sections of the speech signal and ignore extraneous words . Prosody might provide an extra source of information to help locate target words embedded in other speech . In this paper we examine some [T prosodic  T] characteristics of 160 such utterances and compare matched read and spontaneous versions . Half of the utterances are from a corpus of spontaneous answers to requests for the name of a city , recorded from calls to Directory Assistance Operators . The other half are the same word strings read by volunteers attempting to model the real dialogue . Results show a consistent pattern across both sets of data : embedded city names almost always bear nuclear pitch accents and are in their own intonational phrases . However the distributions of tonal make-up of these [T prosodic features T] differ markedly in read versus spontaneous speech , implying that if algorithms that exploit these prosodic regularities are trained on read speech , then the probabilities are likely to be incorrect models of real user speech . </abstract>
<abstract> [T Unsupervised T] [A Semantic Role Labeling A] . We present an [T unsupervised  T] method for  labeling the arguments of verbs with their [A  semantic roles A] . Our [T bootstrapping T] algorithm makes initial unambiguous role assignments , and then iteratively updates the [T probability T] model on which future assignments are based . A novel aspect of our approach is the use of [T verb , slot , and noun class  T] information as the basis for [T backing off T] in our [T probability T] model . We achieve 50 -- 65 % reduction in the error rate over an informed baseline , indicating the potential of our approach for a task that has heretofore relied on large amounts of manually generated training data . </abstract>
<abstract> Preferential Presentation Of Japanese[A  Near-Synonyms A] Using [T Definition T] Statements . This paper proposes a new method of [A ranking near-synonyms A] ordered by their suitability of nuances in a particular context . Our method distincts [A near-synonyms A] by [T semantic features T] extracted from their [T definition T] statements in an ordinary [T dictionary T] , and ranks them by the types of features and a particular context . Our method is an initial step to achieve a [A semantic paraphrase  A] system for authoring support . </abstract>
<abstract> Structural Properties Of [A  Lexical Systems A] : Monolingual And Multilingual Perspectives . We introduce a new type of [A lexical structure A] called [A lexical system A] , an interoperable model that can feed both monolingual and multilingual language resources . We begin with a formal characterization of [A lexical systems A] as `` pure '' [T directed graphs T] , solely made up of nodes corresponding to lexical entities and links . To illustrate our approach , we present data borrowed from a [A lexical system A] that has been generated from the French DiCo database . We later explain how the compilation of the original dictionary-like database into a net-like one has been made possible . Finally , we discuss the potential of the proposed [A lexical structure A] for designing [A multilingual lexical resources A] . </abstract>
<abstract> A [T Hybrid T] Approach to [A Word Segmentation A] and [A POS Tagging A] . In this paper , we present a [T hybrid  T] method for [A word segmentation A] and [A POS tagging A] . The target languages are those in which word boundaries are ambiguous , such as Chinese and Japanese . In the method , [T word-based T] and [T character-based processing T] is combined , and [A word segmentation A] and [A POS tagging A] are conducted simultaneously . Experimental results on multiple corpora show that the integrated method has high accuracy . </abstract>
<abstract> [A Natural Language Planning Dialogue A] For Interactive . </abstract>
<abstract> The Difficulties Of [A Taxonomic Name A] Extraction And A Solution . In modern biology , digitization of biosystematics publications is an important task . Extraction of [A taxonomic names A] from such documents is one of its major issues . This is because these names identify the various genera and species . This article reports on our experiences with [T learning T] techniques for this particular task . We say why established [A Named-Entity Recognition A] techniques are somewhat difficult to use in our context . One reason is that we have only very little training data available . Our experiments show that a combining approach that relies on [T regular expressions T] , [T heuristics T] , and [T word-level language recognition T] achieves very high precision and recall and allows to cope with those difficulties . </abstract>
<abstract> Recognising the [A Predicate-argument Structure A] of Tagalog . This paper describes research on [A parsing A] Tagalog text for [A predicate -- argument structure ( PAS ) A] . We first outline the linguistic phenomenon and corpus annotation process , then detail a series of [A PAS parsing A] experiments . </abstract>
<abstract> A [T POS-Based  T] Model for Long-Range Reorderings in [A SMT A] . In this paper we describe a new approach to model long-range word reorderings in    [T statistical T] [A machine translation ( SMT ) A] . Until now , most SMT approaches are only able to model local reorderings . But even the word order of related languages like German and English can be very different . In recent years approaches that reorder the source sentence in a preprocessing step to better match target sentences according to [T POS ( Part-of-Speech ) - based rules T] have been applied successfully . We enhance this approach to model  long-range reorderings by introducing [T  discontinuous rules T] . We tested this new approach on a GermanEnglish [A translation A] task and could significantly improve the [A translation A] quality , by up to 0.8 BLEU points , compared to a system which already uses continuous POSbased rules to model short-range reorderings . </abstract>
<abstract> Using [T Citations T] to [A Generate surveys of Scientific Paradigms A] . The number of research publications in various disciplines is growing exponentially . Researchers and scientists are increasingly finding themselves in the position of having to quickly understand large amounts of technical material . In this paper we present the first steps in producing an automatically generated , readily consumable , [A technical survey A] . Specifically we explore the combination of [T citation information T] and [T summarization  T] techniques . Even though prior work ( Teufel et al. , 2006 ) argues that citation text is unsuitable for summarization , we show that in the framework of [A multi-document survey creation A] , citation texts can play a crucial role . </abstract>
<abstract> [A Word Sense Disambiguation A] Using [T Sense Examples T] Automatically Acquired From A Second Language . We present a novel [T almost-unsupervised T] approach to the task of [A Word Sense Disambiguation ( WSD ) A] . We build [T sense examples T] automatically , using large quantities of Chinese text , and English-Chinese and Chinese-English bilingual dictionaries , taking advantage of the observation that mappings between words and meanings are often different in typologically distant languages . We train a [T classifier T] on the [T sense examples T] and test it on a gold standard English WSD dataset . The evaluation gives results that exceed previous state-of-the-art results for comparable systems . We also demonstrate that a little manual effort can improve the quality of sense examples , as measured by [A WSD A] accuracy . The performance of the [T classifier T] on [A WSD A] also improves as the number of training sense examples increases . </abstract>
<abstract> A Class-Based Probabilistic Approach To [A Structural Disambiguation A] . Knowledge of which words are able to fill p ~ rticular argum . ent slots of a predicate can be used tbr [A structural disambiguation A] . This paper describes a proposal : for acquiring such knowledge , and in line with much of the recent work in this area , a [T probabilistic T] approach is taken . We develop a novel way of using a [T semantic hierarchy T] to estimate the [T probabilities T] , and demonstrate the general approach using a [T prepositional phrase atta T] . chment experiment . </abstract>
<abstract> [A Coordination Disambiguation A] without Any Similarities . The use of similarities has been one of the main approaches to resolve the ambiguities of coordinate structures . In this paper , we present an alternative method for [A coordination disambiguation A] , which does not use similarities . Our hypothesis is that coordinate structures are supported by surrounding [T dependency relations T] , and that such [T dependency relations T] rather yield similarity between conjuncts , which humans feel . Based on this hypothesis , we built a Japanese [T fully-lexicalized generative T] [A parser A] that includes [A coordination disambiguation A] . Experimental results on web sentencesindicatedtheeffectivenessofor approach , and endorsed our hypothesis . </abstract>
<abstract> Evaluating the [T Syntactic Transformations T] in Gold Standard Corpora for Statistical [A Sentence Compression A] . We present a [T policy-based eror analysis T] aproach that demonstrates a limitation to the curent comonly adopted paradigm for [A sentence compresion A] . We demonstrate that these limitations arise from the strong asumption of locality of the decision making proces in the search for an aceptable derivation in this paradigm . </abstract>
<abstract> The Use of [T Spatial Relations T] in [A Referring Expression Generation A] . There is a prevailing assumption in the literature on [A referring expression generation A] that relations are used in descriptions only ` as a last resort ' , typically on the basis that including the second entity in the relation introduces an additional cognitive load for either speaker or hearer . In this paper , we describe an experiemt that attempts to test this assumption ; we determine that , even in simple scenes where the use of relations is not strictly required in order to identify an entity , relations are in fact often used . We draw some conclusions as to what this means for the development of algorithms for the [A generation of referring expressions A] . </abstract>
<abstract> Robust [A Dialog Management A] with [T N-Best Hypotheses T] Using [A Dialog A] [T Examples and Agenda T] . This work presents an [T agenda-based T] approach to improve the robustness of the [A dialog manager A] by using [A dialog A] [T examples and n-best recognition hypotheses T] . This approach supports [T n-best hypotheses T] in the [A dialog manager A] and keeps track of the dialog state using a discourse interpretation algorithm with the [T agenda graph and focus stack T] . Given the [T agenda graph T] and [T n-best hypotheses T] , the system can predict the next system actions to maximize [T multi-level score functions T] . To evaluate the proposed method , a [A spoken dialog A] system for a building guidance robot was developed . Preliminary evaluation shows this approach would be effective to improve the robustness of [T example-based T] [A dialog A] modeling . </abstract>
<abstract> Resolving [A Translation A] Mismatches With [T Information Flow T] . Languages differ in the concepts and real-world entities for which they have words and grammatical constructs . Therefore [A translation A] must sometimes be a matter of approximating the meaning of a source language text rather than finding an exact counterpart in the target language . We propose a [A translation A] framework based on [T Situation Theory T] . The basic ingredients are an [T information lattice T] , a representation scheme for  [T utterances embedded in contexts T] , and a mismatch resolution scheme defined in terms of  [T information flow T] . We motivate our approach with examples of [A translation A] between English and Japanese . </abstract>
<abstract> [A Summarizing Encyclopedic Term Descriptions A] On The Web . We are developing an automatic method to compile an [A encyclopedic corpus A] from the Web . In our previous work , paragraph-style descriptions for a term are extracted from Web pages and organized based on domains . However , these descriptions are independent and do not comprise a condensed text as in hand-crafted encyclopedias . To resolve this problem , we propose a [A summarization A] method , which produces a single text from multiple descriptions . The resultant summary concisely describes a term from different viewpoints . We also show the eﬀectiveness of our method by means of experiments . </abstract>
<abstract> A Polynomial-Time Algorithm For [T Statistical  T][A Machine Translation A] . We introduce a polynomial-time algorithm for [T statistical  T][A machine translation A] . This algorithm can be used in place of the expensive , slow best-first search strategies in current [T statistical T] [A translation A] architectures . The approach employs the [T stochastic bracketing transduction grammar ( SBTG )  T] model we recently introduced to replace earlier word alignment channel models , while retaining a [T bigram language model T] . The new algorithm in our experience yields major speed improvement with no significant loss of accuracy . </abstract>
<abstract> [A Semantic Pattern Learning A] Through [T Maximum Entropy-Based WSD T] Technique . This paper describes a Natural Language Learning method that extracts knowledge in the form of [A semantic patterns A] with ontology elements associated to syntactic components in the text . The method combines the use of [T EuroWordNet 's ontological concepts T] and the correct sense of each word assigned by a [T Word Sense Disambiguation ( WSD ) T] module to extract three sets of patterns : subject-verb , verb-direct object and verb-indirect object . These sets define the semantic behavior of the main textual elements based on their syntactic role . On the one hand , it is shown that [T Maximum Entropy T] models applied to [T WSD T] tasks provide good results . The evaluation of the [T WSD  T] module has revealed a accuracy rate of 64 % in a preliminary test . On the other hand , we explain how an adequate set of [A semantic or ontological patterns A] can improve the success rate of NLP tasks such us pronoun resolution . We have implemented both modules in C + + and although the evaluation has been performed for English , their general features allow the treatment of other languages like Spanish . a1This paper has been partially supported by the Spanish Government ( CICYT ) project number TIC2000-0664-C0202 . </abstract>
<abstract> A [T Feature-Based T] Model For [A Lexical Databases A] . To date , no fully suitable data model for [A lexical databases A] has been proposed . As [A lexical databases A] have prolifcrated in multiple formats , there has been growing concern over the reusability of lexical resources . In this paper , we propose a model based on [T feature structures T] which overcomes most of the problems inherent in classical database models , anti in particular enables accessing , manipulating or merging information structured in multiple ways . Because of their widespread use in file representation of linguistic information , the applicability of [T feature structures T] to [A lexical databases A] seems natural , although to our knowledge this has not yet been implemented . The nse of [T feature structures T] in [A lexical databases A] also opens up the possibility of compatibility with computational lexicons . </abstract>
<abstract> [T Partially Supervised Coreference Resolution T] For [A Opinion Summarization A] Through [T Structured Rule Learning T] . Combining fine-grained opinion information to produce [A opinion summaries A] is important for [A sentiment analysis A] applications . Toward that end , we tackle the problem of source [A coreference resolution A] -- linking together source mentions that refer to the same entity . The [T partially supervised T] nature of the problem leads us to define and approach it as the novel problem of [T partially supervised clustering T] . We propose and evaluate a new algorithm for the task of [T source coreference resolution T] that outperforms competitive baselines . </abstract>
<abstract> Building [A Domain-Specific Taggers A] without Annotated ( Domain ) Data . [A Part of spech taging A] is a fundamental component in many NLP systems . When tagers developed in one domain are used in another domain , the perforance can degrade considerably . We present a method for developing tagers for new doains without requiring POS anotated text in the ne domain . Our method involves using [T raw doain text T] and identifying [T related ords T] to form a domain specific [T lexicon T] . This [T lexicon T] provides the initial lexical probabilities for [T EM trainig T] of an [T HM model T] . We evaluate the method by aplying it in the [A Biolgy doain A] and show that we achieve results that are comparable ith some tagers developed for this domain . </abstract>
<abstract> [T Grammatical T] Framework [A Web Service A] . We present a [A web service for natural language parsing , prediction , generation , and translation A] using [T grammars in Portable Grammar Format T] ( [T PGF T] ) , the target format of the [T Grammatical Framework ( GF ) grammar compiler T] . The [A web service A] implementation is open source , works with any [T PGF grammar T] , and with any web server that supports FastCGI . The service exposes a simple interface which makes it possible to use it for interactive natural language [A web applications A] . We describe the functionality and interface of the [A web service A] , and demonstrate several applications built on top of it . </abstract>
<abstract> Shared Preferences . This paper attempts to develop a theory of heuristics or preferences that can be shared between [A understanding and generation systems A] . We first develop a formal analysis of preferences and consider the relation between their uses in [A generation and understanding A] . We then present a [T bidirectional T] algorithm for applying them and examine typical heuristics for lexical choice , scope and anaphora in : , more detail . </abstract>
<abstract> [T Functional Constraints T] In [A Knowledge-Based Natural Language Understanding A] . Many knowledge-based systems of semantic interpretation rely explicitly or implicitly on an assumption of structural isomorphy between syntaotic and semantic objects , handling exceptions by ad hoc measures . In this paper I argue that [T constraint equations T] of the kind used in the LFG ( or PATR - ) formalisms provide a more general , and yet restricted formalism : in which not only isomorphic correspondences are expressible , but also many cases of non-isomorphic correspondences . I illustrate with treatments of [A idioms A] , [A speech act interpretation A] and [A discourse pragmatics A] . </abstract>
<abstract> Modelling Atypical [A  Syntax Processing A] . We evaluate the inferences that can be drawn from [T dissociations T] in [A syntax processing A] identified in developmental disorders and acquired language deficits . We use an [T SRN T] to simulate empirical data from Dick et al. ( 2001 ) on the relative difficulty of comprehending different syntactic constructions under normal conditions and conditions of damage . We conclude that [T task constraints T] and internal [T computational constraints T] interact to predict patterns of difficulty . Difficulty is predicted by frequency of constructions , by the requirement of the task to focus on local vs. global sequence information , and by the ability of the system to maintain sequence information . We generate a testable prediction on the empirical pattern that should be observed under conditions of developmental damage . </abstract>
<abstract> Handling [A Scope Ambiguities A] In English . This paper describes a program for handling `` [A scope ambiguities A] '' in individual English sentences . The program operates on initial [T logical translations T] , generated by a [T parser\/translator T] , in which `` unscoped elements '' such as quantifiers , coordinators and negation are left in place to be extracted and positioned by the [A scoping A] program . The program produces the set of valid scoped readings , omitting logically redundant readings , and places the readings in an approximate order of preference using a set of [T domain-independent heuristics T] . The heuristics are based on information about the [T lexical type T] of each operator and on `` [T structural relations T] '' between pairs of operators . The need for such [T domain-independent heuristics T] is emphasized ; in some cases they can be decisive and in general they will serve as a guide to the use of further heuristics based on domain-specific knowledge and on the context of discourse . The emphasis of this paper is on discussing several of the more problematic aspects of the [A scoping A] protocol which wcre encountered during the design of the [A scoping A] program . </abstract>
<abstract> A [T Task-Based T] Framework To Evaluate [A Evaluative Arguments A] . We present an evaluation framework in which the effectiveness of [A evaluative arguments A] can be measured with real users . The framework is based on the [T task-efficacy T] evaluation method . An [A evaluative argument A] is presented in the context of a [A decision task A] and measures related to its effectiveness are assessed . Within this framework , we are currently running a formal experiment to verify whether argument effectiveness can be increased by tailoring the argument to the user and by varying the degree of argument conciseness . </abstract>
<abstract> A [T memory-based learning T] approach to [A event extraction in biomedical texts A] . In this paper we describe the [T memory-based machine learning T] system that we submitted to the BioNLP Shared Task on [A Event Extraction A] . We modeled the [A event extraction A] task using an approach that has been previously applied to other natural language processing tasks like semantic role labeling or negation scope finding . The results obtained by our system ( 30.58 F-score in Task 1 and 29.27 in Task 2 ) suggest that the approach and the system need further adaptation to the complexity involved in extracting [A biomedical events A] . </abstract>
<abstract> Controlling [A  Gender Equality A] With [T Shallow T] NLP Techniques . This paper introduces the Gendercheck Editor '' , a tool to check German texts for [A gender discriminatory formulations A] . It relays on [T shallow rule-based T] techniques as used in the Controlled Language Authoring Technology ( CLAT ) . The paper outlines major sources of [A gender imbalances A] in German texts . It gives a background on the underlying CLAT technology and describes the marking and annotation strategy to automatically detect and visualize the questionable pieces of text . The paper provides a detailed evaluation of the editor . </abstract>
<abstract> [T Factorization Of Language Constraints T] In [A Speech Recognition A] . Integration of language constraints into a large vocabulary speech recognition system often leads to prohibitive complexity . We propose to [T factor the constraints T] into two components . The first is characterized by a [T covering grammar T] which is small and easily integrated into existing [A speech recognizers A] . The recognized string is then decoded by means of an efficient language post-processor in which the full set of [T constraints T] is imposed to correct possible errors introduced by the [A speech recognizer A] . </abstract>
<abstract> Automatic Detection of [A Opinion Bearing Words and Sentences A] . We describe a sentence-level [A opinion detection A] system . We first define what an opinion means in our research and introduce an effective method for obtaining [A opinion-bearing and nonopinion-bearing words A] . Then we describe recognizing opinion-bearing sentences using these words We test the system on 3 different test sets : MPQA data , an internal corpus , and the TREC2003 Novelty track data . We show that our automatic method for obtaining opinion-bearing words can be used effectively to identify [A opinion-bearing sentences A] . </abstract>
<abstract> Automatic Detection And Correction Of [A Repairs In Human-Computer Dialog A] . We have analyzed 607 sentences of spontaneous humancomputer [A speech data containing repairs A] ( drawn from a corpus of 10,718 ) . We present here criteria and techniques for automatically detecting the presence of a  [A repair  A] , its location , and making the appropriate correction. The criteria involve integration of knowledge from several sources : [T pattern matching T] , [T syntactic and semantic T] analysis , and [T acoustics T] . </abstract>
<abstract> [T Modularity And Information Content Classes T] In [T Principle-Based T] [A Parsing A] . In recent years models of parsing that are isomorphic to a [T principle-based theory of grammar T] ( most notably Government and Binding ( GB ) Theory ) have been proposed ( Berwick et al. 1991 ) . These models are natural and direct implementations of the grammar , but they are not efficient , because GB is not a computationally modular theory . This paper investigates one problem related to the tension between building linguistically based parsers and building efficient ones . In particular , the issue of what is a linguistically motivated way of deriving a [A parser A] from [T principle-based theories of grammar T] is explored . It is argued that an efficient and faithful [A parser A] can be built by taking advantage of the way in which principles are stated . To support this claim , two features of an implemented [A parser A] are discussed . First , [T configurations and lexical information T] are precompiled separately into two tables ( an X table and a table of [T lexical co-occurrence T] ) which gives rise to more compact data structures . Secondly , precomputation of [T syntactic features T] ( O-roles , case , etc. ) results in efficient computation of chains , because it reduces several problems of chain formation to a local computation , thus avoiding extensive search of the tree for an antecedent or extensive backtracking . It is also shown that this method of building long-distance dependencies can be computed incrementally . </abstract>
<abstract> Evaluation of a System for [A Noun Concepts Acquisition A] from [T Utterances about Images T] ( SINCA ) Using Daily [T Conversation T] Data . For a robot working in an open environment , a task-oriented language capability will not be sufficient . In order to adapt to the environment , such a robot will have to learn language dynamically . We developed a System for [A Noun Concepts Acquisition A] from utterances about Images , SINCA in short . It is a [A language acquisition A] system without knowledge of grammar and vocabulary , which learns noun concepts from [T user utterances T] . We recorded a video of a child 's daily life to collect [T dialogue T] data that was spoken to and around him . The child is a member of a family consisting of the parents and his sister . We evaluated the performance of SINCA using the collected data . In this paper , we describe the algorithms of [A SINCA A] and an evaluation experiment . We work on Japanese [A language acquisition A] , however our method can easily be adapted to other languages . </abstract>
<abstract> Automatic [A Title Generation For Spoken Broadcast News A] . In this paper , we implemented a set of [A title generation A] methods using [T training T] set of 21190 news stories and evaluated them on an independent test corpus of 1006 broadcast news documents , comparing the results over manual transcription to the results over automatically recognized speech . We use both F1 and the average number of correct title words in the correct order as metric . Overall , the results show that [A title generation for speech recognized news documents A]  is possible at a level approaching the accuracy of titles generated for perfect text transcriptions . Keywords Machine learning , title generation </abstract>
<abstract> [A Sense Tagging A] In Action Combining Different Tests With [T Additive Weighangs T] . This paper describes a working [A sense tagger A] , which attempts to automatically link each word in a text corpus to its corresponding sense in a machinereadable [T dictionary T] . It uses information automatically extracted from the [T MRD T] to find matches between the [T dictionary T] and the Corpus sentences , and combines different types of information by simple [T additive scores T] with manually [T set weightings T] . </abstract>
<abstract> Modeling The User 's [T Plans And Goals T] . This work is an ongoing research effort aimed both at developing techniques for inferring and constructing a user model from an  [A information-seeking dialog A] and at identifying strategies for applying this model to enhance robust [A communication A] . One of the most important components of a user model is a representation of the system 's beliefs about the underlying [T task-related plan T] motivating an information-seeker 's queries . These beliefs can be used to interpret subsequent utterances and produce useful responses . This paper describes the IREPS system , emphasizing its dynamic construction of the [T task-related plan T] motivating the information-seeker 's queries and the application of this component of a user model to handling utterances that violate the [T pragmatic rules T] of the system 's world model . By reasoning on a model of the user 's [T plans and goals T] , the system often can deduce the intended meaning of faulty utterances and allow the [A dialogue A] to continue without interruption . Some limitations of current plan inference systems are discussed . It is suggested that the problem of detecting and recovering from discrepancies between the system 's model of the user 's [T plan T] and the actual plan under construction by the user requires an enriched model that differentiates among its components on the basis of the support the system accords each component as a correct and intended part of the user 's [T plan T] . </abstract>
<abstract> A Tool For The Automatic Creation , Extension And Updating Of  [A Lexical Knowledge Bases A] . A tool is described which helps in the creation , extension and updating of [A lexical knowledge bases A] ( [A LKBs A] ) . Two levels of representation are distinguished : a [T static storage level T] and a [T dynamic knowledge level T] . The latter is an object-oriented environment containing [T linguistic and lexicographic knowledge T] . At the knowledge level , constructors and filters can be defined . Constructors are objects which extend the [A LKB A] both horizontally ( new information ) and vertically ( new entries ) using the [T linguistic knowledge T] . Filters are objects which derive new [A LKBs A] from existing ones thereby optionally changing the storage structure . The latter use [T lexicographic knowledge T] . </abstract>
<abstract> [T Linear T] [A Segmentation A] And [A Segment Significance A] . We present a new method for discovering a [A segmental discourse structure A] of a document while categorizing each segment 's function and importance . Segments are determined by a [T zero-sum weighting T] scheme , used on occurrences of [T noun phrases and pronominal forms T] retrieved from the document . [A Segment roles A] are then calculated from the distribution of the terms in the segment . Finally , we present results of evaluation in terms of precision and recall which surpass earlier approaches ' . </abstract>
<abstract> Choosing The Word Most Typical In Context Using A [T Lexical Co-Occurrence Network T] . This paper presents a partial solution to a component of the problem of [A lexical choice A] : choosing the [A synonym A] most typical , or expected , in context . We apply a new [T statistical T] approach to representing the context of a word through [T lexical co-occurrence networks T] . The implementation was trained and evaluated on a large corpus , and results show that the inclusion of second-order [T co-occurrence relations T] improves the performance of our implemented lexical choice program . </abstract>
<abstract> [A Contextual Preferences A] . The validity of [A semantic inferences A] depends on the contexts in which they are applied . We propose a generic framework for handling contextual considerations within applied inference , termed [A Contextual Preferences A] . This framework defines the various context-aware components needed for inference and their relationships . [A Contextual preferences A] extend and generalize previous notions , such as selectional preferences , while experiments show that the extended framework allows improving inference quality on real application data . </abstract>
<abstract> Combining [T Neural Networks T]  And [T Statistics  T]For Chinese [A Word Sense Disambiguation A] . The input of network is the key problem for Chinese Word sense disambiguation utilizing the Neural Network . This paper presents an input model of [T Neural Network T] that calculates the [T Mutual Information T] between contextual words and ambiguous word by using [T statistical T] method and taking the contextual words to certain number beside the ambiguous word according to ( - M , + N ) . The experiment adopts triple-layer [T BP Neural Network T] model and proves how the size of training set and the value of M and N affect the performance of [T Neural Network T] model . The experimental objects are six pseudowords owning three word-senses constructed according to certain principles . Tested accuracy of our approach on a close-corpus reaches 90.31 % , , and 89.62 % on a open-corpus . The experiment proves that the [T Neural Network T] model has good performance on [A Word sense disambiguation A] . </abstract>
<abstract> An Analysis of [T Active Learning T] Strategies for [A Sequence Labeling A] Tasks . [T Active learning T] is well-suited to many problems in natural language processing , where unlabeled data may be abundant but annotation is slow and expensive . This paper aims to shed light on the best [T active learning T] approaches for [A sequence labeling A] tasks such as [A information extraction A] and [A document segmentation A] . Wesurveypreviouslyusedqueryselection strategies for [A sequence A] models , and propose several novel algorithms to address their shortcomings . We also conduct a large-scale empirical comparison using multiple corpora , which demonstrates that our proposed methods advance the state of the art . </abstract>
<abstract> [T Data-Driven T] Approaches For [A Information Structure A] Identification . This paper investigates automatic identification of [A Information Structure ( IS ) A] in texts . The experiments use the Prague Dependency Treebank which is annotated with IS following the Praguian approach of Topic Focus Articulation . We automatically detect [A t ( opic ) and f ( ocus ) A] , using [T node attributes T] from the treebank as basic features and derived features inspired by the annotation guidelines . We present the performance of [T decision trees ( C4 .5 ) T] , [T maximum entropy T] , and [T rule induction ( RIPPER ) classifiers T] on all tectogrammatical nodes . We compare the results against a baseline system that always assigns f ( ocus ) and against a rule-based system . The best system achieves an accuracy of 90.69 % , which is a 44.73 % improvement over the baseline ( 62.66 % ) . </abstract>
<abstract> [T Cluster Stopping Rules T] For [A Word Sense Discrimination A] . As text data becomes plentiful , [T unsupervised T] methods for [A Word Sense Disambiguation A] ( [A WSD A] ) become more viable . A problem encountered in applying [A WSD A] methods is finding the exact number of senses an ambiguity has in a training corpus collected in an automated manner . That number is not known a priori ; rather it needs to be determined based on the data itself . We address that problem using [T cluster stopping T] methods . Such techniques have not previously applied to WSD . We implement the methods of Calinski and Harabasz ( 1975 ) and Hartigan ( 1975 ) and our adaptation of the [T Gap statistic T] ( Tibshirani , Walter and Hastie , 2001 ) . For evaluation , we use the [A WSD A] Test Set from the National Library of Medicine , whose sense inventory is the Unified Medical Language System . The best accuracy for selecting the correct number of clusters is 0.60 with the C&H method . Our error analysis shows that the cluster stopping methods make finergrained sense distinctions by creating additional clusters . The highest F-scores ( 82.89 ) , indicative of the quality of cluster membership assignment , are comparable to the baseline majority sense ( 82.63 ) and point to a path towards accuracy improvement via additional cluster pruning . The importance and significance of the current work is in applying [T cluster stopping rules T] to [A WSD A] . </abstract>
<abstract> Reversibility In A [T Constraint And Type Based Logic Grammar T] : Application To [A Secondary Predication A] . In this document , we present a formalism for natural language processing which associates [T type construction principles T] to [T constraint logic programming T] . We show that it provides more uniform , expressive and efficient tools for [A parsing A] and [A generating language A] . Next , we present two abstract machines which enable us to design , in a symmetric way , a [A parser A] and a [A generator A] from that formalism . This abstract machinery is then exemplified by a detailed study of [A secondary predication A] within the framework of a principledbased description of language : Government and Binding theory . </abstract>
<abstract> Pedagogically Useful [A Extractive Summaries A] for [A Science Education A] . This paper describes the design and evaluation of an [A extractive summarizer A] for [A educational science A] content called COGENT . COGENT extends MEAD based on strategies elicited from an empirical study with science domain and instructional design experts . COGENT identifies sentences containing pedagogically relevant concepts for a specific science domain . The algorithms pursue a [T hybrid T] approach integrating both domain independent [T bottom-up sentence scoring features T] and [T domain-aware top-down features T] . Evaluation results indicate that COGENT outperforms existing [A summarizers A] and generates [A summaries A] that closely resemble those generated by human experts . COGENT concept inventories appear to also support the computational identification of student misconceptions about earthquakes and plate tectonics . </abstract>
<abstract> [A Language and Translation  A] Model Adaptation using Comparable Corpora . Traditionally , statistical machine translation systems have relied on parallel bi-lingual data to train a translation model . While bi-lingual parallel data are expensive to generate , monolingual data are relatively common . Yet monolingual data have been under-utilized , having been used primarily for training a language model in the target language . This paper describes a novel method for utilizing monolingual target data to improve the performance of a [T statistical T] [A machine translation system A] on news stories . The method exploits the existence of comparable text -- multiple texts in the target language that discuss the same or similar stories as found in the source language document . For every source document that is to be translated , a large monolingual data set in the target language is searched for documents that might be comparable to the source documents . These documents are then used to adapt the [A MT A] system to increase the probability of generating texts that resemble the comparable document . Experimental results obtained by adapting both the [A language and translation A] models show substantial gains over the baseline system . </abstract>
<abstract> [T Unsupervised T] Induction Of Modern Standard Arabic [A Verb Classes A] . We exploit the resources in the Arabic Treebank ( ATB ) for the novel task of automatically creating [A lexical semantic verb classes A] for Modern Standard Arabic ( MSA ) . [A Verbs A] are [T clustered T] into groups that share [T semantic elements of meaning T] as they exhibit similar [T syntactic behavior T] . The results of the [T clustering T] experiments are compared with a gold standard set of classes , which is approximated by using the noisy English translations provided in the ATB to create Levin-like classes for MSA . The quality of the clusters is found to be sensitive to the inclusion of information about [T lexical heads of the constituents in the syntactic frames T] , as well as parameters of the [T clustering T] algorithm . The best set of parameters yields an F β = 1 score of 0.501 , compared to a random baseline with an F β = 1 score of 0.37 . </abstract>
<abstract> [A Textual Entailment A] Using [T Univariate Density T] Model and [T Maximizing Discriminant Function T] . The primary focuses of this entry this year was firstly , to develop a framework to allow multiple researchers from our group to easily contribute metrics measuring [A textual entailment A] , and secondly , to provide a baseline which we could use in our tools to evaluate and compare new metrics . A development environment tool was created to quickly allow for testing of various metrics and to easily randomize the development and test sets . For each test , this RTE tool calculated two sets of results by applying the metrics to both a [T univariate Gaussian density T] and by [T maximizing a linear discriminant function T] . The metrics used for the submission were a [T lexical similarity T] metric and a [T lexical similarity T] metric using [T synonym T] and [T antonym T] replacement . The two submissions for RTE 2007 scored an accuracy of 61.00 % and 62.62 % . </abstract>
<abstract> A [T Speech-First T] Model For [A Repair Detection And Correction A] . Interpreting fttUy natural speech is an important goal for spoken language understanding systems . However , while corpus studies have shown that about 10 % of spontaneous utterances contain [A selfcorrections A] , or [A REPAIRS A] , little is known about the extent to which cues in the speech signal may facilitate repair processing . We identify several cues based on [T acoustic T] and [T prosodic T] analysis of [A repairs A] in the DARPA Air Travel In . formation System database , and propose methods for exploiting these cues to [A detect and correct repairs A] . </abstract>
<abstract> Advanced [T Dynamic Programming T] in [A Semiring and Hypergraph Frameworks A] . [T Dynamic Programming T] ( [T DP T] ) is an important class of algorithms widely used in many areas of speech and language processing . Recently there have been a series of work trying to formalize many instances of [T DP T] algorithms under algebraic and graph-theoretic frameworks . This tutorial surveys two such frameworks , namely [A semirings and directed hypergraphs A] , and draws connections between them . We formalize two particular types of [T DP T] algorithms under each of these frameworks : the [T Viterbi-style topological T] algorithms and the [T Dijkstra-style best-first T] algorithms . Wherever relevant , we also discuss typical applications of these algorithms in Natural Language Processing . </abstract>
<abstract> Training And Evaluating [T Error Minimization Decision Rules T] For [T Statistical T] [A Machine Translation A] . [T Decision rules T] that explicitly account for non-probabilistic evaluation metrics in machine translation typically require special training , often to estimate parameters in exponential models that govern the search space and the selection of candidate translations . While the traditional Maximum A Posteriori ( MAP ) decision rule can be optimized as a piecewise linear function in a greedy search of the parameter space , the Minimum Bayes Risk ( MBR ) decision rule is not well suited to this technique , a condition that makes past results difficult to compare . We present a novel training approach for [T non-tractable decision rules T] , allowing us to compare and evaluate these and other decision rules on a large scale [A translation A] task , taking advantage of the high dimensional parameter space available to the phrase based Pharaoh decoder . This comparison is timely , and important , as decoders evolve to represent more complex search space decisions and are evaluated against innovative evaluation metrics of translation quality . </abstract>
<abstract> Towards Automatic Extraction Of [A Monolingual And Bilingual Terminology A] . In this paper , we make use of [T linguistic T] knowledge to identify certain [A noun phrases A] , both in English and French , which are likely to be [A terms A] . We then test and cmnl ) are ( lifl ` e. rent statistical scores to select the `` good '' ones among tile candidate terms , and finally propose a [T statistical T] method to build correspondences of multi-words units across languages . Acknowledgement Most of this work was carried out under project EUII . OTP ~ A ET-10 \/ 63 , co-sponsored by the European Economic Conmmnity . </abstract>
<abstract> Automatic [A Question Answering A] : Beyond The Factoid . In this paper we describe and evaluate a [A Question Answering A] system that goes beyond answering factoid questions . We focus on [A FAQlike questions and answers A] , and build our system around a [T noisy-channel T] architecture which exploits both a [T language model T] for answers and a [T transformation T] model for [A answer\/question A] terms , trained on a corpus of 1 million question\/answer pairs collected from the Web . </abstract>
<abstract> The `` Casual Cashmere Diaper Bag '' : Constraining [A Speech Recognition A] Using [T Examples T] . We describe a new technology for using small collections of [T example T] sentences to automatically restrict a [A speech recognition grammar A] to allow only the more plausible subset of the sentences it would otherwise admit . This technology is unusual because it bridges the gap between hand-built grammars ( used with no training data ) and statistical approaches ( which require significant data ) . </abstract>
<abstract> [A Dependency Parsing A] Of Japanese Spoken Monologue Based On [T Clause Boundaries T] . Spoken monologues feature greater sentence length and structural complexity than do spoken dialogues . To achieve high [A parsing A] performance for spoken monologues , it could prove effective to simplify the structure by dividing a sentence into suitable language units . This paper proposes a method for [A dependency parsing A] of Japanese monologues based on [T sentence segmentation T] . In this method , the [A dependency parsing A] is executed in two stages : at the clause level and the sentence level . First , the dependencies within a clause are identified by dividing a sentence into clauses and executing [T stochastic T] [A dependency parsing A] for each clause . Next , the dependencies over clause boundaries are identified [T stochastically T] , and the dependency structure of the entire sentence is thus completed . An experiment using a spoken monologue corpus shows this method to be effective for efficient [A dependency parsing A] of Japanese monologue sentences . </abstract>
<abstract> A [T Hierarchical T] Approach to Encoding [A  Medical Concepts for Clinical Notes A] . This paper proposes a [T hierarchical text categorization T] ( [T TC T] ) approach to encoding free-text [A clinical notes A] with ICD-9-CM codes . Preliminary experimental result on the 2007 Computational Medicine Challenge data shows a [T hierarchical TC T] system has achieved a microaveraged F1 value of 86.6 , which is comparable to the performance of state-of-the-art flat classification systems . </abstract>
<abstract> [T Structural Correspondence Learning T] for [A Parse Disambiguation A] . The paper presents an application of [T Structural Correspondence Learning T] ( [T SCL T] ) ( Blitzer et al. , 2006 ) for [A domain adaptation of a stochastic attribute-value grammar A] ( [A SAVG A] ) . So far , [T SCL T] has been applied successfully in NLP for Part-of-Speech tagging and Sentiment Analysis ( Blitzer et al. , 2006 ; Blitzer et al. , 2007 ) . An attempt was made in the CoNLL 2007 shared task to apply [T SCL T] to non-projective dependency parsing ( Shimizu and Nakagawa , 2007 ) , however , without any clear conclusions . We report on our exploration of applying [T SCL T] to adapt a syntactic disambiguation model and show promising initial results on [A Wikipedia A] domains . </abstract>
<abstract> [A Normalizing SMS A] : are Two Metaphors Better than One ? Electronic written texts used in computermediated interactions ( e-mails , blogs , chats , etc ) present major deviations from the norm of the language . This paper presents an comparative study of systems aiming at [A normalizing the orthography of French SMS messages A] : after discussing the linguistic peculiarities of these messages , and possible approaches to their automatic [A normalization A] , we present , evaluate and contrast two systems , one drawing inspiration from the [T Machine Translation T] task ; the other using techniques that are commonly used in automatic [T speech recognition T] devices . Combining both approaches , our best [A normalization A] system achieves about 11 % Word Error Rate on a test set of about 3000 unseen messages . </abstract>
<abstract> [A Word Sense Disambiguation A] Criteria : A Systematic Study . This article describes the results of a systematic indepth study of the criteria used for [A word sense disambiguation A] . Our study is based on 60 target words : 20 nouns , 20 adjectives and 20 verbs . Our results are not always in line with some practices in the field . For example , we show that omitting noncontent words decreases performance and that bigrams yield better results than unigrams . </abstract>
<abstract> [A Named Entity Recognition A] With [T Character-Level T] Models . We discuss two [A named-entity recognition A] models which use [T characters and character a4 - grams T] either exclusively or as an important part of their data representation . The first model is a [T character-level HMM T] with minimal context information , and the second model is a [T maximum-entropy conditional markov T] model with substantially richer context features . Our best model achieves an overall Fa5 of 86.07 % on the English test data ( 92.31 % on the development data ) . This number represents a 25 % error reduction over the same model without word-internal ( substring ) features . </abstract>
<abstract> A [T Data Driven T] Approach To [A Relevancy Recognition A] For [A Contextual Question Answering A] . [A Contextual question answering A] ( [A QA A] ) , in which users ' information needs are satisfied through an interactive [A QA dialogue A] , has recently attracted more research attention . One challenge of engaging dialogue into [A QA A] systems is to determine whether a question is relevant to the previous interaction context . We refer to this task as [A relevancy recognition A] . In this paper we propose a [T data driven T] approach for the task of [A relevancy recognition A] and evaluate it on two data sets : the TREC data and the HandQA data . The results show that we achieve better performance than a previous rule-based algorithm . A detailed evaluation analysis is presented . </abstract>
<abstract> Hypothesis Selection in [A Machine Transliteration A] : A [T Web Mining T] Approach . We propose a new method of selecting hypotheses for  [A machine transliteration A] . We generate a set of Chinese , Japanese , and Korean transliteration hypotheses for a given English word . We then use the set of [A transliteration A] hypotheses as a guide to finding relevant Web pages and [T mining contextual information T] for the [A transliteration A] hypotheses from the Web page . Finally , we use the mined information for [T machine-learning T] algorithms including [T support vector machines T] and [T maximum entropy T] model designed to select the correct [A transliteration A] hypothesis . In our experiments , our proposed method based on [T Web mining T] consistently outperformed systems based on simple Web counts used in previous work , regardless of the language . </abstract>
<abstract> A Robust Cross-Style [A Bilingual Sentences Alignment A] Model . Most current [A sentence alignment A] approaches adopt sentence length and cognate as the alignment features ; and they are mostly trained and tested in the documents with the same style . Since the length distribution , alignment-type distribution ( used by length-based approaches ) and cognate frequency vary significantly across texts with different styles , the length-based approaches fail to achieve similar performance when tested incorpora ofdifferent styles . The experiments show that the performance in F-measure could drop from 98.2 % to 85.6 % when a length-based approach is trained by a technical manual and then tested on a general magazine . Sincealargepercentageofcontentwordsinthesource text would be translated into the corresponding translation duals to preserve the meaning in the target text , [T transfer lexicons T] are usually regarded as more reliable cues for [A aligning sentences A] when the alignment task is performed by human . To enhance the robustness , a robust [T statistical T] model based on both [T transfer lexicons T] and [T sentence lengths T] are proposed in this paper . After integrating the [T transfer lexicons T] into the model , a 60 % F-measure error reduction ( from 14.4 % to 5.8 % ) is observed . </abstract>
<abstract> Common Topics And Coherent Situations : Interpreting [A Ellipsis A] In The Context Of [A Discourse Inference A] . It is claimed that a variety of facts concerning [A ellipsis A] , event reference , and interclausal coherence can be explained by two features of the linguistic form in question : ( 1 ) whether the form leaves behind an empty constituent in the [T syntax T] , and ( 2 ) whether the form is [T anaphoric T] in the [T semantics T] . It is proposed that these features interact with one of two types of [A discourse inference A] , namely Common Topic inference and Coherent Situation inference . The differing ways in which these types of inference utilize [T syntactic and semantic T] representations predicts phenomena for which it is otherwise difficult to account . </abstract>
<abstract> Generating And Selecting [A Grammatical Paraphrases A] . Natural language has a high paraphrastic power yet not all paraphrases are appropriate for all contexts . In this paper , we present a [T TAG T] based [T surface realiser T] which supports both the generation and the selection of [A paraphrases A] . To deal with the combinatorial explosion typical of such an NP-complete task , we introduce a number of new optimisations in a tabular , [T bottom-up surface realisation  T] algorithm . We then show that one of these optimisations supports [A paraphrase selection A] . </abstract>
<abstract> FastSum : Fast and Accurate [A Query-based Multi-document Summarization A] . Wepresentafastquery-basedmulti-document [A summarizer A] called FastSum based solely on [T word-frequency features T] of clusters , documents and topics . Summary sentences are ranked by a [T regression SVM T] . The [A summarizer A] does not use any expensive NLP techniques such as parsing , tagging of names or even part of speech information . Still , the achieved accuracy is comparable to the best systems presented in recent academic competitions ( i.e. , Document Understanding Conference ( DUC ) ) . Because of a detailed feature analysis using [T Least Angle Regression T] ( [T LARS T] ) , FastSum can rely on a minimal set of featuresleading tofastprocessingtimes : 1250 news documents in 60 seconds . </abstract>
<abstract> [T Forest Rescoring T] : Faster [A Decoding A] with [T Integrated Language Models T] . Efficient [A decoding A] has been a fundamental problem in [A machine translation A] , especially with an [T integrated language model T] which is essential for achieving good translation quality . We develop faster approaches for this problem based on [T k-best parsing T] algorithms and demonstrate their effectiveness on both [T phrase-based T] and [T syntax-based T]  [A  MT A] systems . In both cases , our methods achieve significant speed improvements , often by more than a factor of ten , over the conventional beam-search method at the same levels of search error and translation accuracy . </abstract>
<abstract> Building Accurate [A Semantic Taxonomies A] from Monolingual [T MRDs T] . This paper presents a method that conbines a set of [T unsupervised T] algorithms in order to accurately build large [A taxonomies A] from any [T machine-readable dictionary ( MRD ) T] . Our aim is to profit from conventional [T MRDs T] , with no explicit semantic coding . We propose a system that 1 ) performs fully automatic extraction of [A taxonomic links A] from [T MRD T] entries and 2 ) ranks the extracted relations in a way that selective manual refinement is allowed . Tested accuracy can reach around 100 % depending on the degree of coverage selected , showing that taxonomy building is not limited to structured dictionaries such as LDOCE . </abstract>
<abstract> A [A Linguistic Discovery A] Program That Verbalizes Its Discoveries . We describe a [A discovery A] program , called UNIVAUTO ( UNIVersals AUthoringTOol ) , whose domain of application is the study of [A language universals A] , a classic trend in contemporary linguistics . Accepting as input information about languages , presented in terms of [T feature-values T] , the discoveries of another human agent arising from the same data , as well as some additional data , the program discovers the [A universals A] in the data , compares them with the discoveries of the human agent and , if appropriate , generates a report in English on its discoveries . Running UNIVAUTO on the data from the seminal paper of Greenberg ( 1966 ) on [A word order universals A] , the system has produced several linguistically valuable texts , two of which are published in a refereed linguistic journal . </abstract>
<abstract> Towards [A Multilingual Protocol Generation For Spontaneous Speech Dialogues A] . : This paper presents a novel [A multi-lingual progress protocol generation A] module . The module is used within the [A speech-to -- speech translation A] system VERBMOBIL . The task of the protocol is to give the dialogue partners a brief description of the content of their dialogue . We utilize an . abstract representation describing , for instance , [T thematic T]  information and [T dialogue acts T] of the dialogue utterances . From this representation we generate simplified [T paraphrases T] of the individual turns of the dialogue which together make up the protocol . Instead of writing completely new software , the protocol generation component is almost exclusively composed of already existing modules in the system which are extended by planning and formatting routines for protocol formulations . We describe how the abstract information is extracted from user utterances in different languages and how the [T abstract thematic T] representation is used to [A generate a protocol A] in one specific language . Future directions are given . </abstract>
<abstract> [A Relation Extraction A] Using [T Label Propagation Based Semi-Supervised Learning T] . Shortage of manually labeled data is an obstacle to supervised relation extraction methods . In this paper we investigate a [T graph based semi-supervised learning T] algorithm , a [T label propagation ( LP ) T] algorithm , for [A relation extraction A] . It represents labeled and unlabeled examples and their distances as the nodes and the weights of edges of a graph , and tries to obtain a labeling function to satisfy two constraints : 1 ) it should be fixed on the labeled nodes , 2 ) it should be smooth on the whole graph . Experiment results on the ACE corpus showed that this [T LP T] algorithm achieves better performance than SVM when only very few labeled examples are available , and it also performs better than bootstrapping for the [A relation extraction A] task . </abstract>
<abstract> Knowledge-Free [A Induction Of Morphology A] Using [T Latent Semantic Analysis T] . [A Morphology induction A] is a subproblem of important tasks like automatic learning of machine-readable dictionaries and grammar induction . Previous [A morphology induction A] approaches have relied solely on statistics of hypothesized stems and affixes to choose which affixes to consider legitimate . Relying on stemand-affix statistics rather than semantic knowledge leads to a number of problems , such as the inappropriate use of valid affixes ( `` ally '' stemming to `` all '' ) . We introduce a [T semantic-based T] algorithm for learning [A morphology A] which only proposes affixes when the stem and stem-plusaffix are sufficiently similar semantically . We implement our approach using [T Latent Semantic Analysis T] and show that our [T semantics-only T] approach provides [A morphology induction A] results that rival a current state-of-the-art system . </abstract>
<abstract> The Role of [T Implicit Argumentation T] in [A Nominal SRL A] . [A Nominals A] frequently surface without overtly expressed arguments . In order to measure the potential benefit of [A nominal SRL A] for downstream processes , such [A nominals A] must be accounted for . In this paper , we show that a state-of-the-art [A nominal SRL A] system with an overall argument F1 of 0.76 suffers a performance loss of more than 9 % when nominals with implicit arguments are included in the evaluation . We then develop a system that takes [T implicit argumentation T] into account , improving overall performance by nearly 5 % . Our results indicate that the degree of [T implicit argumentation T] varies widely across [A nominals A] , making automated detection of [T implicit argumentation T] an important step for [A nominal SRL A] . </abstract>
<abstract> The Automatic Construction Of A [T Symbolic  T][A Parser A] Via [T Statistical T] Techniques . We report on the development of a robust [A parsing A] device which aims to provide a partial explanation for child language acquisition and help in the construction of better natural language processing systems . The backbone of the new approach is the synthesis of [T statistical T] and [T symbolic T] approaches to natural language . Motivation We report on the progress we have made towards developing a robust ` self-constructing ' [A parsing A] device that uses indirect negative evidence ( Kapur , </abstract>
<abstract> Analysis Of [A Unknown Lexical Items A] Using [T Morphological T] And [T Syntactic T] Information With The TIMIT Corpus . The importance of dealing with [A unknown words A] in Natural Language Processing ( NLP ) is growing as NLP systems are used in more and more applications . One aid in predicting the lexical class of words that do not appear in the lexicon ( referred to as [A unknown words A] ) is the use of [T syntactic T] parsing rules . The distinction between closed-class and open-class words together with [T morphological T] recognition appears to be pivotal in increasing the ability of the system to predict the lexical categories of [A unknown words A] . An experiment is performed to investigate the ability of a parser to parse unknown words using [T morphology T] and [T syntactic T] parsing rules without human intervention . This experiment shows that the performance of the [A parser A] is enhanced greatly when [T morphological T] recognition is used in conjunction with [T syntactic rules T] to [A parse A] sentences containing [A unknown words A] from the TIMIT corpus . </abstract>
<abstract> From Water To Wine : [A Generating Natural Language Text A] From Today 's Applications Programs . In this paper we present a means of compensating for the semantic deficits of linguistically naive underlying application programs without compromising principled grammatical treatments in [A natural language generation A] . We present a method for building an interface from today 's underlying application programs to the linguistic realization component [T Mumble-86 T] . The goal of the paper is not to discuss how Mumble works , but to describe how one exploits its capabilities . We provide examples from current generation projects using [T Mumble T] as their linguistic component . </abstract>
<abstract> [T Bootstrapping T] [A Word Alignment A] via [T Word Packing T] . We introduce a simple method to [T pack words T] for [T statistical T] [A word alignment A] . Our goal is to simplify the task of automatic [A word alignment A] by [T packing T] several consecutive words together when we believe they correspond to a single word in the opposite language . This is done using the word aligner itself , i.e. by [T bootstrapping T] on its output . We evaluate the performance of our approach on a Chinese-to-English [A machine translation A] task , and report a 12.2 % relative increase in BLEU score over a state-of-the art phrasebased SMT system . </abstract>
<abstract> Decoding Complexity In [A Word-Replacement Translation A] Models . [T Statistical T] [A machine translation A] is a relatively new approach to the long-standing problem of translating human languages by computer . Current statistical techniques uncover translation rules from bilingual training texts and use those rules to translate new texts . The general architecture is the [T source-channel T] model : an English string is statistically generated ( source ) , then statistically transformed into French ( channel ) . In order to translate ( or `` decode '' ) a French string , we look for the most likely English source . We show that for the simplest form of [T statistical T] models , this problem is NP-complete , i.e. , probably exponential in the length of the observed sentence . We trace this complexity to factors not present in other decoding problems . </abstract>
<abstract> An Evaluation Of [A Anaphor Generation A] In Chinese . In this paper , we present an evaluation of [A anaphors generated A] by a Chinese [A natural language generation A] system . In the evaluation work , the anaphors in five test texts generated by three test systems employing generation [T rules T] with different complexities ~ vere compared with the ones in the same texts created by twelve native speakers of Chinese . We took the average number of [A anaphors A] matching between the machine and human texts as a measure of the quality of anaphors generated by the test systems . The results suggest that the one we have chosen and which has the most complex [T rule T] is better than the other two . There axe , however , real difficulties in establishing the significance of the results because of the degree of disagreement among the native speakers . </abstract>
<abstract> LHIP : Extended [T DCGs T] For Configurable Robust [A Parsing A] . We present LHIP , a system for [A incremental grammar development A] using an extended [T DCG T] formalism . ` rite system uses a robust [T island-based T] [A parsing A] method controlled by user-defined performance thresholds . Keywords : I ) CG , head , island parsing , robust parsing , Prolog </abstract>
<abstract> Computing [A Optimal Descriptions For Optimality Theory Grammars A] With [T Context-Free Position Structures T] . This paper describes an algorithm for computing [A optimal structural descriptions for Optimality Theory grammars A] with [T context-free position structures T] . This algorithm extends Tesar 's [T dynamic programming T] approach ( Tesar , 1994 ) ( Tesar , 1995 @ to computing [A optimal structural descriptions A] from regular to context-free structures . The generalization to [T contextfree structures T] creates several complications , all of which are overcome without compromising the core [T dynamic programming T] approach . The resulting algorithm has a time complexity cubic in the length of the input , and is applicable to grammars with universal constraints that exhibit context-free locality . </abstract>
<abstract> [T Association-Based T] [A Bilingual Word Alignment A] . [A Bilingual word alignment A] forms the foundation of current work on statistical machine translation . Standard [A wordalignment A] methods involve the use of probabilistic generative models that are complex to implement and slow to train . In this paper we show that it is possible to approach the alignment accuracy of the standard models using algorithms that are much faster , and in some ways simpler , based on basic [T word-association statistics T] . </abstract>
<abstract> [T Regularized Least-Squares Classification T] For [A Word Sense Disambiguation A] . The paper describes RLSC-LIN and RLSCCOMB systems which participated in the Senseval-3 English lexical sample task . These systems are based on [T Regularized Least-Squares Classification T] ( [T RLSC T] ) [T learning T] method . We describe the reasons of choosing this method , how we applied it to [A word sense disambiguation A] , what results we obtained on Senseval1 , Senseval-2 and Senseval-3 data and discuss some possible improvements . </abstract>
<abstract> [A Paraphrase A] Identification as [T Probabilistic Quasi-Synchronous T] Recognition . We present a novel approach to deciding whether two sentences hold a [A paraphrase A] relationship . We employ a [T generative T] model that generates a [A paraphrase A] of a given sentence , and we use [T probabilistic inference T] to reason about whether two sentences share the [A paraphrase A] relationship . The model cleanly incorporates both [T syntax and lexical semantics T] using [T quasi-synchronous dependency grammars T] ( Smith and Eisner , 2006 ) . Furthermore , using a product of experts ( Hinton , 2002 ) , we combine the model with a complementary [T logistic regression T] model based on state-of-the-art [T lexical overlap features T] . We evaluate our models on the task of distinguishing true [A paraphrase A] pairs from false ones on a standard corpus , giving competitive state-of-the-art performance . </abstract>
<abstract> [T Orthogonal Negation In Vector Spaces T] For Modelling [A Word-Meanings And Document Retrieval A] . Standard [A IR A] systems can process queries such as `` web NOT internet '' , enabling users who are interested in arachnids to avoid documents about computing . The documents retrieved for such a query should be irrelevant to the negated query term . Most systems implement this by reprocessing results after retrieval to remove documents containing the unwanted string of letters . This paper describes and evaluates a theoretically motivated method for removing unwanted meanings directly from the original query in [T vector models T] , with the same [T vector negation T] operator as used in quantum logic . Irrelevance in vector spaces is modelled using [T orthogonality T] , so query vectors are made orthogonal to the negated term or terms . As well as removing unwanted terms , this form of [T vector negation T] reduces the occurrence of synonyms and neighbors of the negated terms by as much as 76 % compared with standard Boolean methods . By altering the query vector itself , [T vector negation T] removes not only unwanted strings but unwanted meanings . </abstract>
<abstract> [A Hierarchical Clustering Of Words A] And Application To NLP Tasks . This paper describes a [T data-driven T] method for  [A hierarchical clustering of words A] and [A clustering of multiword compounds A] . A large vocabulary of English words ( 70,000 words ) is [A clustered A] [T bottom-up T] , with respect to corpora ranging in size from 5 million to 50 million words , using [T mutual information T] as an objective function . The resulting [A hierarchical clusters of words A] are then naturally transformed to a bit-string representation of ( i.e. word bits for ) all the words in the vocabulary . Evaluation of the word bits is carried out through the measurement of the error rate of the ATR [T Decision-Tree T] [A Part-Of-Speech Tagger A] . The same [A clustering A] technique is then applied to the [A classification of multiword compounds A] . In order to avoid the explosion of the number of compounds to be handled , compounds in a small subclass are bundled and treated as a single compound . Another merit of this approach is that we can avoid the data sparseness problem which is ubiquitous in corpus statistics . The quality of one of the obtained compound classes is examined and compared to a conventional approach . </abstract>
<abstract> An [T Expert Lexicon T] Approach To Identifying English [A Phrasal Verbs A] . [A Phrasal Verbs A] are an important feature of the English language . Properly identifying them provides the basis for an English parser to decode the related structures . [A Phrasal verbs A] have been a challenge to Natural Language Processing ( NLP ) because they sit at the borderline between lexicon and syntax . Traditional NLP frameworks that separate the lexicon module from the parser make it difficult to handle this problem properly . This paper presents a [T finite state T] approach that integrates a [A phrasal verb A] [T expert lexicon T] between shallow parsing and deep parsing to handle morpho-syntactic interaction . With precision\/recall combined performance benchmarked consistently at 95.8 % -97.5 % , the [A Phrasal Verb A] identification problem has basically been solved with the presented method . </abstract>
<abstract> [T Zero Morphemes T] In [A Unification-Based Combinatory Categorial Grammar A] . In this paper , we report on our use of [T zero morphemes T] in [A Unification-Based Combinatory Categorial Grammar A] . After illustrating the benefits of this approach with several examples , we describe the algorithm for compiling [T zero morphemes T] into [T unary rules T] , which allows us to use [T zero morphemes T] more efficiently in natural language processing . 1 Then , we discuss the question of equivalence of a grammar with these [T unary rules T] to the original grammar . Lastly , we compare our approach to [T zero morphemes T] with possible alternatives . </abstract>
<abstract> The Role of [T Sentence Structure T] in Recognizing [A Textual Entailment A] . Recent research suggests that [T sentence structure T] can improve the accuracy of recognizing [A textual entailments A] and paraphrasing . Although background knowledge such as gazetteers , WordNet and custom built knowledge bases are also likely to improve performance , our goal in this paper is to characterize the [T syntactic features T] alone that aid in accurate [A entailment A] prediction . We describe candidate features , the role of [T machine learning T] , and two final [T decision rules T] . These rules resulted in an accuracy of 60.50 and 65.87 % and average precision of 58.97 and 60.96 % in RTE3 Test and suggest that [T sentence structure T] alone can improve [A entailment A] accuracy by 9.25 to 14.62 % over the baseline majority class . </abstract>
<abstract> Recovery of [A Empty Nodes in Parse Structures A] . In this paper , we describe a new algorithm for recovering [A WH-trace empty nodes A] . Our approach combines a set of [T hand-written patterns T] together with a [T probabilistic T] model . Because the [T patterns T] heavily utilize [T regular expressions T] , the pertinent tree structures are covered using a limited number of [T patterns T] . The [T probabilistic T] model is essentially a [T probabilistic context-free grammar ( PCFG ) T] approach with the [T patterns T] acting as the terminals in production rules . We evaluate the algorithm 's performance on gold trees and parser output using three different metrics . Our method compares favorably with state-of-the-art algorithms that recover [A WH-traces A] . </abstract>
<abstract> Importance Of [T Pronominal Anaphora Resolution T] In [A Question Answering A] Systems . The main aim of this paper is to analyze the e # 0Bects of applying [T pronominal anaphora resolution T] to [A Question Answering A] # 28QA # 29 systems . For this task a complete [A QA A] system has been implemented . System evaluation measures performance improvements obtained when information that is referenced anaphorically in documents is not ignored . </abstract>
<abstract> Learning [A Word Sense A] With [T Feature Selection T] And [T Order Identification T] Capabilities . This paper presents an [T unsupervised T] [A word sense A] learning algorithm , which induces senses of target word by grouping its occurrences into a `` natural '' number of clusters based on the similarity of their contexts . For removing noisy words in feature set , [T feature selection T] is conducted by optimizing a cluster validation criterion subject to some constraint in an [T unsupervised T] manner . [T Gaussian mixture T] model and [T Minimum Description Length T] criterion are used to estimate cluster structure and cluster number . Experimental results show that our algorithm can find important feature subset , estimate model order ( cluster number ) and achieve better performance than another algorithm which requires cluster number to be provided . </abstract>
<abstract> On the Complexity of [T Non-Projective Data-Driven T] [A Dependency Parsing A] . In this paper we investigate several [T nonprojective T]  [A parsing A] algorithms for [A dependency parsing A] , providing novel polynomial time solutions under the assumption that each dependency decision is independent of all the others , called here the [T edge-factored T] model . We also investigate algorithms for [T non-projective T]  [A parsing A] that account for nonlocal information , and present several hardness results . This suggests that it is unlikely that exact [T non-projective T] [A dependency parsing A] is tractable for any model richer than the [T edge-factored T] model . </abstract>
<abstract> Using [T Emoticons T] To Reduce Dependency In [T Machine Learning T] Techniques For [A Sentiment Classification A] . [A Sentiment Classification A] seeks to identify a piece of text according to its author 's general feeling toward their subject , be it positive or negative . Traditional machine learning techniques have been applied to this problem with reasonable success , but they have been shown to work well only when there is a good match between the training and test data with respect to topic . This paper demonstrates that match with respect to domain and time is also important , and presents preliminary experiments with training data labeled with [T emoticons T] , which has the potential of being independent of domain , topic and time . </abstract>
<abstract> Lexicalized [A Phonotactic Word Segmentation A] . This paper presents a new [T unsupervised T] algorithm ( WordEnds ) for inferring [A word boundaries A] from transcribed adult conversations . [T Phone ngrams T] before and after observed pauses are used to [T bootstrap T] a simple [T discriminative T] model of [A boundary marking A] . This fast algorithm delivers high performance even on morphologically complex words in English and Arabic , and promising results on accurate phonetic transcriptions with extensive pronunciation variation . Expanding training data beyond the traditional miniature datasets pushes performance numbers well above those previously reported . This suggests that WordEnds is a viable model of child language acquisition and might be useful in speech understanding . </abstract>
<abstract> Adding More Languages Improves [T Unsupervised T] [A Multilingual Part-of-Speech Tagging A] : a [T Bayesian Non-Parametric T] Approach . We investigate the problem of [T unsupervised T] [A part-of-speech tagging A] when raw parallel data is available in a large number of languages . Patterns of ambiguity vary greatly across languages and therefore even unannotated multilingual data can serve as a learning signal . We propose a [T non-parametric Bayesian T] model that connects related tagging decisions across languages through the use of [T multilingual latent variables T] . Our experiments show that performance improves steadily as the number of languages increases . </abstract>
<abstract> A Re-examination of [T Machine Learning T] Approaches for [A Sentence-Level MT Evaluation A] . Recent studies suggest that [T machine learning T] can be applied to develop good automatic [A evaluation A] metrics for machine translated sentences . This paper further analyzes aspects of learning that impact performance . We argue that previously proposed approaches of training a HumanLikeness classifier is not as well correlated with human judgments of translation quality , but that [T regression-based learning T] produces more reliable metrics . We demonstrate the feasibility of [T regression-based T] metrics through empirical analysis of learning curves and generalization studies and show that they can achieve higher correlations with human judgments than standard automatic metrics . </abstract>
<abstract> A [A Multilingual Dependency A] Analysis System Using [T Online Passive-Aggressive Learning T] . This paper presents an [T online T] algorithm for [A dependency parsing A] problems . We propose an adaptation of the [T passive and aggressive online learning T] algorithm to the [A dependency parsing A] domain . We evaluate the proposed algorithms on the 2007 CONLL Shared Task , and report errors analysis . Experimental results show that the system score is better than the average score among the participating systems . </abstract>
<abstract> Creating a [A Knowledge Base A] from a Collaboratively Generated [T Encyclopedia T] . We present our work on using [T Wikipedia T] as a knowledge source for Natural Language Processing . We first describe our previous work on computing semantic relatedness from [T Wikipedia T] , and its application to a [T machine learning T] based [A coreference resolution A] system . Our results suggest that [T Wikipedia T] represents a semantic resource to be treasured for NLP applications , and accordingly present the work directions to be explored in the future . </abstract>
<abstract> Identifying [A Unknown Proper Names A] In Newswire Text . The identification of [A unknown proper names A] in text is a significant challenge for NLP systems operating on unrestricted text . A system which indexes documents according to name references can be useful for information retrieval or as a preprocessor for more knowledge intensive tasks such as database extraction . This paper describes a system which uses [T text skimming T] techniques for deriving [A proper names A] and their semantic attributes automatically from newswire text , without relying on any listing of name elements . In order to identify new names , the system treats proper names as ( potentially ) context-dependent linguistic expressions . In addition to using information in the [T local context T] , the system exploits a [T computational model of discourse T] which identifies individuals based on the way they are described in the text , instead of relying on their description in a pre-existing knowledge base . </abstract>
<abstract> English-Chinese [A Bi-Directional OOV Translation A] based on [T Web Mining T] and [T Supervised Learning T] . In Cross-Language Information Retrieval ( CLIR ) , [A Out-of-Vocabulary ( OOV ) detection and translation A] pair relevance evaluation still remain as key problems . In this paper , an English-Chinese [A Bi-Directional OOV translation A] model is presented , which utilizes [T Web mining T] as the corpus source to collect translation pairs and combines [T supervised learning T] to evaluate their association degree . The experimental results show that the proposed model can successfully filter the most possible translation candidate with the lower computational cost , and improve the [A OOV translation A] ranking effect , especially for popular new words . </abstract>
<abstract> Rich Source-Side [T Context T] for [T Statistical T] [A Machine Translation A] . We explore the augmentation of [T statistical  T][A machine translation A] models with features of the context of each phrase to be translated . This work extends several existing threads of research in [T statistical T] [A MT A] , including the use of context in example-based machine translation ( Carl and Way , 2003 ) and the incorporation of word sense disambiguation into a translation model ( Chan et al. , 2007 ) . The context features we consider use [T surrounding words T] and [T part-of-speech tags T] , [T local syntactic structure T] , and other properties of the source language sentence to help predict each phrase 's translation . Our approach requires very little computation beyond the standard [T phrase extraction T] algorithm and scales well to large data scenarios . We report significant improvements in automatic evaluation scores for Chineseto-EnglishandEnglish-to-Germantranslation , and also describe our entry in the WMT-08 shared task based on this approach . </abstract>
<abstract> [T Coreference Handling In XMG T] . We claim that existing specification languages for [A tree based grammars A] fail to adequately support identifier managment . We then show that [T XMG ( eXtensible MetaGrammar ) T] provides a sophisticated treatment of identifiers which is effective in supporting a [A linguist-friendly grammar design A] . </abstract>
<abstract> ParaEval : Using [T Paraphrases T] To [A Evaluate Summaries A] Automatically . ParaEval is an automated [A evaluation A] method for comparing reference and peer [A summaries A] . It facilitates a tieredcomparison strategy where recall-oriented global optimal and local greedy searches for [T paraphrase T] matching are enabled in the top tiers . We utilize a domainindependent [T paraphrase T] table extracted from a large bilingual parallel corpus using methods from [T Machine Translation T] ( [T MT T] ) . We show that the quality of ParaEval 's evaluations , measured by correlating with human judgments , closely resembles that of ROUGE 's . </abstract>
<abstract> [T Unsupervised T] [A Concept Discovery A] In Hebrew Using Simple [T Unsupervised Word Prefix Segmentation T] for Hebrew and Arabic . Fully unsupervised pattern-based methods for discovery of word categories have been proven to be useful in several languages . The majority of these methods rely on the existence of function words as separate text units . However , in morphology-rich languages , in particular Semitic languages such as Hebrew and Arabic , the equivalents of such function words are usually written as morphemes attached as prefixes to other words . As a result , they are missed by word-based pattern discovery methods , causing many useful patterns to be undetected and a drastic deterioration in performance . To enable high quality [A lexical category A] acquisition , we propose a simple [T unsupervised word segmentation T] algorithm that separates these morphemes . We study the performance of the algorithm for Hebrew and Arabic , and show that it indeed improves a state-of-art [T  unsupervised T] [A concept acquisition A] algorithm in Hebrew . </abstract>
<abstract> Automatic [A Answer Typing for How-Questions A] . We introduce an [A answer typing strategy specific to quantifiable how questions A] . Using the [T web T] as a data source , we automatically collect answer units appropriate to a given [A how-question A] type . Experimental results show [A answer typing A] with these units outperforms traditional fixedcategory [A answer typing A] and other strategies based on the occurrences of numerical entities in text . </abstract>
<abstract> Using Three Way Data for [A Word Sense Discrimination A] . In this paper , an extension of a [T dimensionality reduction T] algorithm called [T NONNEGATIVE MATRIX FACTORIZATION T] is presented that combines both ` [T bag of words T] ' data and [T syntactic T] data , in order to find semantic dimensions according to which both words and syntactic relations can be classified . The use of three way data allows one to determine which dimension ( s ) are responsible for a certain [A sense of a word A] , and adapt the corresponding feature vector accordingly , ` subtracting ' one sense to discover another one . The intuition in this is that the [T syntactic features of the syntax-based T] approach can be disambiguated by the semantic dimensions found by the [T bag of words T] approach . The novel approach is embedded into [T clustering T] algorithms , to make it fully automatic . The approach is carried out for Dutch , and evaluated against EuroWordNet . </abstract>
<abstract> Language Independent [A Text Correction A] using [T Finite State Automata T] . Many natural language applications , like machine translation and information extraction , are required to operate on text with [A spelling errors A] . Those [A spelling mistakes A] have to be corrected automatically to avoid deteriorating the performance of such applications . In this work , we introduce a novel approach for automatic [A correction of spelling mistakes A] by deploying [T finite state automata T] to propose candidates corrections withinaspecifiededitdistancefromthemisspelled word . After choosing candidate corrections , a [T language model T] is used to assign scores the candidate corrections and choose best correction in the given context . The proposed approach is language independent and requires only a [T dictionary T] and text data for building a [T language model T] . The approach have been tested on both Arabic and English text and achieved accuracy of 89 % . </abstract>
<abstract> On The Complexity Of [T ID\/LP T] [A Parsing A] . Modern linguistic theory attributes surface complexity to interacting subsystems of constraints . For instance , the [T ID\/LP T] [A grammar formalism A] separates constraints on immediate dominance from those on linear order . An [T ID\/LP T] [A parsing A] algorithm by Shieber shows how to use [T ID and LP constraints T] directly in language processing , without expanding them into an intermediate context-free `` object grammar '' . However , Shieber 's purported runtime bound underestimates the difficulty of [T ID\/LP T] [A parsing A] . [T ID\/LP T] [A parsing A] is actually NP-complete , and the worst-case runtime of Shieber 's algorithm is actually exponential in grammar size . The growth of parser data structures causes the difficulty . Some computational and linguistic implications follow ; in particular , it is important to note that , desplte its potential for combinatorial explosion , Shieber 's algorithm remains better than the alternative of parsing an expanded object grammar . </abstract>
<abstract> An Improved Error Model For [T Noisy Channel T] [A Spelling Correction A] . The [T noisy channel T] model has been applied to a wide range of problems , including [A spelling correction A] . These models consist of two components : a source model and a [T channel model T] . Very little research has gone into improving the [T channel model T] for [A spelling correction A] . This paper describes a new [T channel model T] for [A spelling correction A] , based on generic string to [T string edits T] . Using this model gives significant performance improvements compared to previously proposed models . </abstract>
<abstract> Extending [T TimeML T] With Typical Durations Of Events . In this paper , we demonstrate how to extend [T TimeML T] , a rich specification language for [A event and temporal expressions in text A] , with the implicit typical [A durations of events A] , [A temporal information A] in text that has hitherto been largely unexploited . [A Event duration A] information can be very important in applications in which the time course of events is to be extracted from text . For example , whether two events overlap or are in sequence often depends very much on their durations . </abstract>
<abstract> [T Bayesian Nets T] For [A Syntactic Categorization A] Of Novel Words . This paper presents an application of a [T Dynamic Bayesian Network T] ( [T DBN T] ) to the task of assigning [A Part-of-Speech A] ( [A PoS A] ) tags to novel text . This task is particularly challenging for non-standard corpora , such as Internet lingo , where a large proportion of words are unknown . Previous work reveals that [A PoS tags A] depend on a variety of [T morphological T] and [T contextual features T] . Representing these dependencies in a [T DBN T] results into an elegant and effective [A PoS tagger A] . </abstract>
<abstract> An Integrated Model For [A Anaphora Resolution A] . The paper discusses a new [T knowledgebased and sublanguage-oriented T] model for [A anaphora resolution A] , which integrates [T syntactic , semantic , discourse , domain and heuristical T] knowledge for the sublanguage of computer science . Special attention is paid to a new approach for tracking the center throughout a discourse segment , which plays an imtx ~ rtant role in proposing the most likely antecedent to the anaphor in case of ambiguity . </abstract>
<abstract> Alternative Approaches For [A Generating Bodies Of Grammar Rules A] . We compare two approaches for describing and [A generating bodies of rules A] used for natural language [A parsing A] . In today 's parsers rule bodies do not exist a priori but are generated on the fly , usually with methods based on n-grams , which are one particular way of inducing probabilistic regular languages . We compare two approaches for inducing such languages . One is based on [T n-grams T] , the other on minimization of the [T Kullback-Leibler divergence T] . The inferred regular languages are used for [A generating bodies of rules A] inside a [A parsing A] procedure . We compare the two approaches along two dimensions : the quality of the [T probabilistic T] regular language they produce , and the performance of the [A parser A] they were used to build . The second approach outperforms the first one along both dimensions . </abstract>
<abstract> A [T Unified Syntactic T] Model for [A Parsing Fluent and Disfluent Speech A] . This paper describes a [T syntactic representation T] for modeling [A speech repairs A] . This representation makes use of a right corner transform of [T syntax trees T] to produce a tree representation in which [A speech repairs A] require very few special [T syntax rules T] , making better use of training data . [T PCFGs T] trained on [T syntax trees T] using this model achieve high accuracy on the standard Switchboard [A parsing A] task . </abstract>
<abstract> [T Markov Random Field T] Based English [A Part-Of-Speech Tagging A] System . Probabilistic models have been widely used for natural language processing . [A Part-of-speech tagging A] , which assigns the most likely tag to each word in a given sentence , is one . of tire problems which can be solved by statisticM approach . Many researchers haw ~ tried to solve the problem by hidden Marker model ( HMM ) , which is well known as one of the statistical models . But it has many difficulties : integrating heterogeneous information , coping with data sparseness prohlem , and adapting to new environments . In this paper , we propose a [T Markov radom field T] ( [T MRF T] ) model based approach to the [A tagging A] problem . The [T MRF T] provides the base frame to combine various statistical information with [T maximum entropy T] ( [T ME T] ) method . As [T Gibbs distribution T] can be used to describe a posteriori probability of tagging , we use it in ma . ximum a posteriori ( MAP ) estimation of optimizing process . Besides , several [A tagging A] models are developed to show the effect of adding information . Experimental results show that the performance of the [A tagger A] gets improved as we add more [T statistical T] information , and that Mt -LCB- F-based [A tagging A] model is better than ttMM based [A tagging A] model in data sparseness problem . </abstract>
<abstract> [T Dependencies T] Vs. [T Constituents T] For [A Tree-Based Alignment A] . Given a parallel parsed corpus , [T statistical T] [A treeto-tree alignment A] attempts to match nodes in the syntactic trees for a given sentence in two languages . We train a [T probabilistic tree transduction T] model on a large automatically parsed Chinese-English corpus , and evaluate results against human-annotated word level alignments . We find that a [T constituent-based T] model performs better than a similar probability model trained on the same trees converted to a [T dependency T] representation . </abstract>
<abstract> Text [A Genre A] Detection Using [T Common Word Frequencies T] . In this paper we present a method for detecting the text [A genre A] quickly and easily following an approach originally proposed in authorship attribution studies which uses as style markers the frequencies of occurrence of the most frequent words in a training corpus ( Burrows , 1992 ) . In contrast to this approach we use the frequencies of occurrence of the most [T frequent words T] of the entire written language . Using as testing ground a part of the Wall Street Journal corpus , we show that the most [T frequent words T] of the British National Corpus , representing the most [T frequent words T] of the written English language , are more reliable discriminators of text [A genre A] in comparison to the most frequent words of the training corpus . Moreover , the fi'equencies of occurrence of the most common punctuation marks play an important role in terms of accurate text categorization as well as when dealing with training data of limited size . </abstract>
<abstract> A [T Dynamical T] System Approach To [A Continuous Speech Recognition A] . A [T dynamical T] system model is proposed for better representing the [T spectral dynamics T] of [A speech for recognition A] . We assume that the observed feature vectors of a phone segment are the output of a [T stochastic linear dynamical T] system and consider two alternative assumptions regarding the relationship of the segment length and the evolution of the dynamics . Training is equivalent to the identification of a [T stochastic linear T] system , and we follow a nontraditional approach based on the [T Estlmate-Maximize T] algorithm . We evaluate this model on a [A phoneme classification A] task using the TIMIT database . </abstract>
<abstract> Modern [A Natural Language Interfaces To Databases A] : Composing [T Statistical Parsing T] With [T Semantic Tractability T] . [A Natural Language Interfaces to Databases A] ( [A NLIs A] ) can benefit from the advances in [T statistical parsing T] over the last fifteen years or so . However , statistical parsers require training on a massive , labeled corpus , and manually creating such a corpus for each database is prohibitively expensive . To address this quandary , this paper reports on the PRECISE NLI , which uses a [T statistical parser T] as a `` plug in '' . The paper shows how a strong [T semantic T] model coupled with ``[T  light re-training T] '' enables PRECISE to overcome [T parser T] errors , and correctly map from parsed questions to the corresponding SQL queries . We discuss the issues in using [T statistical parsers T] to build [A database-independent NLIs A] , and report on experimental results with the benchmark ATIS data set where PRECISE achieves 94 % accuracy . </abstract>
<abstract> Learning [T Transformation Rules T] To Find [A Grammatical Relations A] . [A Grammatical relationships A] are an important level of natural language processing . We present a trainable approach to find these relationships through [T transformation sequences and-error-driven learning T] . Our approach finds [A grammatical relationships A] between core syntax groups and bypasses much of the parsing phase . On our training and test set , our procedure achieves 63.6 % recall and 77.3 % precision ( f-score = 69.8 ) . </abstract>
<abstract> Identifying [A Chemical Names In Biomedical Text A] : An Investigation Of [T Substring Co-Occurrence T] Based Approaches . We investigate various strategies for finding [A chemicals in biomedical text A] using [T substring co-occurrence T] information . The goal is to build a system from readily available data with minimal human involvement . Our models are trained from a [T dictionary T] of chemical names and general biomedical text . We investigated several strategies including [T Naïve Bayes classifiers T] and several types of [T N-gram models T] . We introduced a new way of interpolating [T N-grams T] that does not require tuning any parameters . We also found the task to be similar to Language Identification . </abstract>
<abstract> [T Finite-State  T] Reduplication In One-Level [A Prosodic Morphology A] . Reduplication , a central instance of [A prosodic morphology A] , is particularly challenging for state-ofthe-art [A computational morphology A] , since it involves copying of some part of a phonological string . In this paper I advocate a [T finite-state T] method that combines enriched [T lexical representations T] via intersection to implement the copying . The proposal includes a resource-conscious variant of [T automata T] and can benefit from the existence of [T lazy T] algorithms . Finally , the implementation of a complex case from Koasati is presented . </abstract>
<abstract> A Procedure for [T Multi-Class Discrimination T] and some Linguistic Applications . The paper describes a novel computational tool for [T multiple concept learning T] . Unlike previous approaches , whose major goal is prediction on unseen instances rather than the legibility of the output , our [T MPD T] ( [T Maximally Parsimonious Discrimination T] ) program emphasizes the conciseness and intelligibility of the resultant class descriptions , using three intuitive simplicity criteria to this end . We illustrate [T MPD T] with applications in [A componential analysis A] ( in [A lexicology and phonology A] ) , [A language typology A] , and [A speech pathology A] . </abstract>
<abstract> An Empirical Study on [A Language Model Adaptation A] Using a [T Metric of Domain Similarity T] . This paper presents an empirical study on four techniques of [A language model adaptation A] , including a [T maximum a posteriori T] ( [T MAP T] ) method and three [T discriminative training T] models , in the application of Japanese Kana-Kanji conversion . We compare the performance of these methods from various angles by [A adapting A] the baseline model to four adaptation domains . In particular , we attempt to interpret the results given in terms of the character error rate ( CER ) by correlating them with the characteristics of the [A adaptation A] domain measured using the [T information-theoretic T] notion of [T cross entropy T] . We show that such a metric correlates well with the CER performance of the [A adaptation A] methods , and also show that the [T discriminative T] methods are not only superior to a [T MAP-based T] method in terms of achieving larger CER reduction , but are also more robust against the similarity of background and adaptation domains . </abstract>
<abstract> Simultaneous Identification of [A Biomedical Named-Entity A] and [A Functional Relation A] Using [T Statistical Parsing T] Techniques . In this paper we propose a [T statistical parsing T] technique that simultaneously identifies [A biomedical named-entities A] ( [T NEs T] ) and extracts subcellular [A localization relations A] for bacterial proteins from the text in MEDLINE articles . We build a [T parser T] that derives both [T syntactic T] and domain-dependent [T semantic T] information and achieves an F-score of 48.4 % for the [A relation extraction A] task . We then propose a [T semi-supervised T] approach that incorporates noisy automatically labeled data to improve the F-score of our parser to 83.2 % . Our key contributions are : learning from noisy data , and building an annotated corpus that can benefit [A relation extraction A] research . 1 Introduction Relation extraction from text is a step beyond Named-Entity Recognition ( NER ) and generally demands adequate domain knowledge to build relations among domain-specific concepts . A Biomedical Functional Relation ( relation for short ) states interactions among biomedical substances . In this paper we focus on one such relation : Bacterial Protein Localization ( BPL ) , and introduce our approach for identifying BPLs from MEDLINE1 articles . BPL is a key functional characteristic of proteins . It is essential to the understanding of the function of different proteins and the discovery of suitable drugs , vaccines and diagnostic targets . We are collaborating with researchers in molecular biology with the goal of automatically extracting BPLs from ∗ This research was partially supported by NSERC , Canada . </abstract>
<abstract> Discovering [A Synonyms A] And Other Related Words . Discovering [A synonyms A] and other related words among the words in a document collection can be seen as a [T clustering T] problem , where we expect the words in a cluster to be closely related to one another . The intuition is that words occurring in similar contexts tend to convey similar meaning . We introduce a way to use [T translation dictionaries T] for several languages to evaluate the rate of [A synonymy A] found in the word clusters . We also apply the [T information radius T] to calculating similarities between words using a full [T dependency syntactic feature space T] , and introduce a method for similarity recalculation during [T clustering T] as a fast approximation of the high-dimensional feature space . Finally , we show that 69-79 % of the words in the clusters we discover are useful for [A thesaurus construction A] . </abstract>
<abstract> A [T Case-Based Reasoning T] Approach for [A Speech Corpus Generation A] . Corpus-based stochastic language models have achieved significant success in speech recognition , but construction of a corpus pertaining to a specific application is a difficult task . This paper introduces a [T Case-Based Reasoning T] system to [A generate natural language A] corpora . In comparison to traditional [A natural language generation A] approaches , this system overcomes the inflexibility of template-based methods while avoiding the linguistic sophistication of rule-based packages . The evaluation of the system indicates our approach is effective in [A generating A] users ' specifications or queries as 98 % of the generated sentences are grammatically correct . The study result also shows that the [T language model T] derived from the generated corpus can significantly outperform a general language model or a dictation grammar . </abstract>
<abstract> A [T Frame-Based Probabilistic  T] Framework for [A Spoken Dialog Management A] Using Dialog  [T Examples T] . This paper proposes a [T probabilistic T] framework for [A spoken dialog management A] using dialog examples . To overcome the complexity problems of the classic partially observable Markov decision processes ( POMDPs ) based dialog manager , we use a [T frame-based belief state representation T] that reduces the complexity of belief update . We also used dialog [T examples T] to maintain a reasonable number of system actions to reduce the complexity of the optimizing policy . We developed [A weather information A] and [A car navigation dialog A] system that employed a [T frame-based probabilistic T] framework . This framework enables people to develop a [A spoken dialog A] system using a [T probabilistic T] approach without complexity problem of POMDP . </abstract>
<abstract> [A Text Generation A] From Keywords . We describe a method for [A generating sentences A] from `` keywords '' or `` headwords '' . This method consists of two main parts , candidate-text construction and evaluation . The construction part [A generates text sentences A] in the form of [T dependency trees T] by using complementary information to replace information that is missing because of a `` knowledge gap '' and other missing function words to [A generate natural text sentences A] based on a particular monolingual corpus . The evaluation part consists of a model for generating an appropriate text when given keywords . This model considers not only [T word n-gram T] information , but also [T dependency T] information between words . Furthermore , it considers both string information and [T morphological T] information . </abstract>
<abstract> [T Multi-Criteria-Based Active Learning T] For [A Named Entity Recognition A] . In this paper , we propose a [T multi-criteria based active learning T] approach and effectively apply it to [A named entity recognition A] . [T Active learning T] targets to minimize the human annotation efforts by selecting examples for labeling . To maximize the contribution of the selected examples , we consider the multiple criteria : informativeness , representativeness and diversity and propose measures to quantify them . More comprehensively , we incorporate all the criteria using two selection strategies , both of which result in less labeling cost than single - criterion-based method . The results of the [A named entity recognition A] in both MUC-6 and GENIA show that the labeling cost can be reduced by at least 80 % without degrading the performance . </abstract>
<abstract> [A Discriminating Image Senses A] By [T Clustering With Multimodal Features T] . We discuss [A Image Sense Discrimination A] ( [A ISD A] ) , and apply a method based on [T spectral clustering T] , using [T multimodal features T] from the image and text of the embedding web page . We evaluate our method on a new data set of annotated web images , retrieved with ambiguous query terms . Experiments investigate different levels of sense granularity , as well as the impact of text and image features , and global versus local text features . </abstract>
<abstract> A [T Layered T] Approach to [T NLP-Based T] [A Information Retrieval A] . A [T layered T] approach to [A information retrieval A] permits the inclusion of multiple search engines as well as multiple databases , with a natural language layer to convert English queries for use by the various search engines . The [T NLP layer T] incorporates [T morphological T] analysis , [T noun phrase syntax T] , and [T semantic T] expansion based on [T WordNet T] . </abstract>
<abstract> Measuring Importance and Query Relevance in [A Topic-focused Multi-document Summarization A] . The increasing complexity of summarization systems makes it difficult to analyze exactly which modules make a difference in performance . We carried out a principled comparison between the two most commonly used schemes for assigning importance to words in the context of [A query focused multi-document summarization A] : [T raw frequency T] ( [T word probability T] ) and [T log-likelihood ratio T] . We demonstrate that the advantages of [T log-likelihood ratio T] come from its known [T distributional T] properties which allow for the identification of a set of words that in its entirety defines the aboutness of the input . We also find that [T LLR T] is more suitable for [A query-focused summarization A] since , unlike raw frequency , it is more sensitive to the integration of the information need defined by the user . </abstract>
<abstract> Message-To-Speech : High Quality [A Speech Generation A] For [A Messaging And Dialogue Systems A] . In this paper , we present a Message-toSpeech ( MTS ) system that offers the linguistic flexibility desired for [A spoken dialogue and message generating A] systems . The use of [T prosody transplantation T] and special purpose [T prosody T] models results in highly natural prosody for the synthesised speech . </abstract>
<abstract> [T FLSA T] : Extending [T Latent Semantic Analysis T] With Features For [A Dialogue Act Classification A] . We discuss [T Feature Latent Semantic Analysis T] ( [T FLSA T] ) , an extension to [T Latent Semantic Analysis T] ( [T LSA T] ) . [T LSA T] is a [T statistical T] method that is ordinarily trained on words only ; [T FLSA T] adds to [T LSA T] the richness of the many other linguistic features that a corpus may be labeled with . We applied [T FLSA T] to [A dialogue act classification A] with excellent results . We report results on three corpora : CallHome Spanish , MapTask , and our own corpus of tutoring dialogues . </abstract>
<abstract> [A Parsing A] The WSJ Using [A CCG A] And [T Log-Linear T] Models . This paper describes and evaluates [T log-linear T] [A parsing A] models for [A Combinatory Categorial Grammar ( CCG ) A] . A parallel implementation of the [T L-BFGS optimisation T] algorithm is described , which runs on a Beowulf cluster allowing the complete Penn Treebank to be used for estimation . We also develop a new efficient [A parsing A] algorithm for [A CCG A] which maximises expected recall of dependencies . We compare models which use all [A CCG A] derivations , including nonstandard derivations , with normal-form models . The performances of the two models are comparable and the results are competitive with existing wide-coverage [A CCG parsers A] . </abstract>
<abstract> Inducing [A Information Extraction A] Systems For New Languages Via [T Cross-Language Projection T] . [A Information extraction A] ( [A IE A] ) systems are costly to build because they require development texts , parsing tools , and specialized dictionaries for each application domain and each natural language that needs to be processed . We present a novel method for rapidly creating [A IE A] systems for new languages by exploiting existing [A IE A] systems via [T crosslanguage projection T] . Given an [A IE A] system for a source language ( e.g. , English ) , we can transfer its annotations to corresponding texts in a target language ( e.g. , French ) and learn [A information extraction A] [T rules T] for the new language automatically . In this paper , we explore several ways of realizing both the transfer and learning processes using off-theshelf [T machine translation T] systems , [T induced word alignment T] , [T attribute projection T] , and [T transformationbased learning T] . We present a variety of experiments that show how an English [A IE A] system for a plane crash domain can be leveraged to automatically create a French [A IE A] system for the same domain . </abstract>
<abstract> [T Unsupervised T] [A Classification A] with [T Dependency Based Word Spaces T] . We present the results of [A clustering A] experiments with a number of different evaluation sets using [T dependency based word spaces T] . Contrary to previous results we found a clear advantage using a [T parsed corpus over word spaces T] constructed with the help of simple [T patterns T] . We achieve considerable gains in performance over these spaces ranging between 9 and 13 % in absolute terms of cluster purity . </abstract>
<abstract> [T Log-Linear T] Models of [A Non-Projective Trees A] , [T $ k $ - best MST T] [A Parsing A] and [A Tree-Ranking A] . We present our system used in the CoNLL 2007 shared task on [A multilingual parsing A] . The system is composed of three components : a [T k-best maximum spanning tree ( MST ) T] [A parser A] , a [T tree labeler T] , and a [T reranker T] that orders the k-best labeled trees . We present two techniques for training the [T MST T] [A parser A] : [T tree-normalized T] and [T graphnormalized conditional training T] . The [T treebased T] [A reranking A] model allows us to explicitly model global syntactic phenomena . We describe the reranker features which include [T non-projective edge attributes T] . We provide an analysis of the errors made by our system and suggest changes to the models and features that might rectify the current system . </abstract>
<abstract> [T Cluster-Based Query Expansion T] for [T Statistical T] [A  Question Answering A] . Document retrieval is a critical component of [A question answering A] ( [A QA A] ) , yet little work has been done towards statistical modeling of queries and towards automatic generation of high quality query content for QA . This paper introduces a new , [T cluster-based query expansion T] method that learns queries known to be successful when applied to similar questions . We show that [T cluster-based expansion T] improves the retrieval performance of a  [T statistical T] [A question answering A] system when used in addition to existing query expansion methods . This paper presents experiments with several [T feature selection T] methods used individually and in combination . We show that documents retrieved using the [T cluster-based T] approach are inherently different than documents retrieved using existing methods and provide a higher data diversity to answers extractors . </abstract>
<abstract> [A Positioning for Conceptual Development A] using [T Latent Semantic Analysis T] . With increasing opportunities to learn online , the problem of positioning learners in an educational network of content offers new possibilities for the utilisation of geometry-based natural language processing techniques . In this article , the adoption of [T latent semantic analysis T] ( [T LSA T] ) for guiding learners in their conceptual development is investigated . We propose five new algorithmic derivations of [T LSA T] and test their validity for positioning in an experiment in order to draw back conclusions on the suitability of machine learning from previously accredited evidence . Special attention is thereby directed towards the role of distractors and the calculation of thresholds when using similarities as a proxy for assessing conceptual closeness . Results indicate that [T learning T] improves [A positioning A] . Distractors are of low value and seem to be replaceable by generic noise to improve threshold calculation . Furthermore , new ways to flexibly calculate thresholds could be identified . </abstract>
<abstract> Exploiting [T Syntactic Patterns T] As Clues In [A Zero-Anaphora Resolution A] . We approach the [A zero-anaphora resolution A] problem by decomposing it into intra-sentential and inter-sentential [A zeroanaphora resolution A] . For the former problem , [T syntactic patterns T] of the appearance of zero-pronouns and their antecedents are useful clues . Taking Japanese as a target language , we empirically demonstrate that incorporating rich [T syntactic pattern T] features in a state-of-the-art [T learning-based T] [A anaphora resolution A] model dramatically improves the accuracy of intra-sentential [A zero-anaphora A] , which consequently improves the overall performance of [A zeroanaphora resolution A] . </abstract>
<abstract> [A Speech to speech machine translation A] : Biblical chatter from Finnish to English . [A Speech-to-speech machine translation A] is in some ways the peak of natural language processing , in that it deals directly with our original , oral mode of communication ( as opposed to derived written language ) . As such , it presents challenges that are not to be taken lightly . Although existing technology covers each of the steps in the process , from speech recognition to synthesis , deriving a model of translation that is effective in the domain of spoken language is an interesting and challenging task . If we could teach our algorithms to learn as children acquire language , the result would be useful both for language technology and cognitive science . We propose several potential approaches , an implementation of a [T multi-path T] model that [A translates A] recognized morphemes alongside words,andaweb-interfacetotestourspeech translation tool as trained for Finnish to English . We also discuss current approaches to [A machine translation A] and the problems they face in adapting simultaneously to morphologically rich languages and to the spoken modality . </abstract>
<abstract> Compiling And Using [T Finite-State T][A  Syntactic Rules A] . A language-independent framework for [T syntactic finlte-state T] [A parsing A] is discussed . The article presents a framework , a formalism , a compiler and a parser for grammars written in this forrealism . As a substantial example , fragments from a nontrivial [T finite-state T] grammar of English are discussed . The linguistic framework of the present approach is based on a [T surface syntactic tagging T] scheme by F. Karlsson . This representation is slightly less powerful than phrase structure tree notation , letUng some ambiguous constructions be described more concisely . The [T finite-state T] rule compiler implements what was briefly sketched by Koskenniemi ( 1990 ) . It is based on the [T calculus of finite-state machines T] . The compiler transforms rules into [T rule-automata T] . The run-time parser exploits one of certain alternative strategies in performing the effective intersection of the [T rule automata T] and the [T sentence automaton T] . Fragments of a fairly comprehensive [T finite-state T] granmmr of English axe presented here , including samples from non-finite constructions as a demonstration of the capacity of the present formalism , which goes far beyond plain disamblguation or part of speech tagging . The grammar itself is directly related to a parser and tagging system for English created as a part of project SIMPR I using Karlsson 's [T CG T] ( [T Constraint Grammar T] ) formalism . </abstract>
<abstract> Adapting a [T Probabilistic Disambiguation T] Model of an [T HPSG T] [A Parser A] to a New Domain . This paper describes a method of adapting a domain-independent [T HPSG T] [A parser A] to a [A biomedical A] domain . Without modifying the grammar and the [T probabilistic T] model of the original  [T HPSG T] [A parser A] , we develop a [T log-linear T] model with additional features on a treebank of the [A biomedical A] domain . Since the treebank of the target domain is limited , we need to exploit an original disambiguation model that was trained on a larger treebank . Our model incorporates the original model as a reference [T probabilistic T] distribution . The experimental results for our model trained with a small amount of a treebank demonstrated an improvement in [A parsing A] accuracy . </abstract>
<abstract> Learning [A Word Clusters A] From [T Data Types T] . The paper illustrates a [A linguistic knowledge acquisition A] model making use of [T data types T] , [T infinite nlenlory T] , and an [T inferential mechanism T] tbr inducing new intbrmation Dora known data . The mode ) is colnpared with standard stochastic lnethods applied to data tokens , and tested on a task of [A lexico semantic classification A] . </abstract>
<abstract> Automatic Derivation Of [T Surface Text Patterns T] For A [T Maximum Entropy T] Based [A Question Answering A] System . In this paper we investigate the use of [T surface text patterns T] for a [T Maximum Entropy T] based [A Question Answering A] ( [A QA A] ) system . These [T text patterns T] are collected automatically in an [T unsupervised T] fashion using a collection of trivia question and answer pairs as seeds . These [T patterns T] are used to generate features for a [T statistical T] [A question answering A] system . We report our results on the TREC-10 question set . </abstract>
<abstract> [T Learning T] To [A Tag Multilingual Texts A] Through [T Observation T] . This paper describes RoboTag , an advanced prototype for a [T machine learningbased T] [A multilingual information extraction A] system . First , we describe a general client\/server architecture used in [T learning from observation T] . Then we give a detailed description of our novel [T decision-tree T] [A tagging A] approach . RoboTag performance for the [A proper noun tagging A] task in English and Japanese is compared against humantagged keys and to the best hand-coded pattern performance ( as reported in the MUC and MET evaluation results ) . Related work and future directions are presented . </abstract>
<abstract> [T Lattice-Based Search T] For [A Spoken Utterance Retrieval A] . Recent work on [A spoken document retrieval A] has suggested that it is adequate to take the singlebest output of ASR , and perform text retrieval on this output . This is reasonable enough for the task of retrieving broadcast news stories , where word error rates are relatively low , and the stories are long enough to contain much redundancy . But it is patently not reasonable if one 's task is to retrieve a short snippet of speech in a domain where WER 's can be as high as 50 % ; such would be the situation with teleconference speech , where one 's task is to find if and when a participant uttered a certain phrase . In this paper we propose an [T indexing T] procedure for [A spoken utterance retrieval A] that works on [T lattices T] rather than just single-best text . We demonstrate that this procedure can improve F scores by over five points compared to singlebest retrieval on tasks with poor WER and low redundancy . The representation is flexible so that we can represent both [T word lattices T] , as well as [T phone lattices T] , the latter being important for improving performance when searching for phrases containing OOV words . </abstract>
<abstract> [A Bio-Medical Entity Extraction A] Using [T Support Vector Machines T] . [T Support Vector Machines T] have achieved state of the art performance in several classification tasks . In this article we apply them to the identification and semantic annotation of scientific and technical terminology in the domain of molecular biology . This illustrates the extensibility of the traditional [A named entity A] task to special domains with extensive terminologies such as those in medicine and related disciplines . We illustrate [T SVM T] 's capabilities using a sample of 100 journal abstracts texts taken from the fhuman , blood cell , transcription factorg domain of MEDLINE . Approximately 3400 terms are annotated and the model performs at about 74 % F-score on cross-validation tests . A detailed analysis based on empirical evidence shows the contribution of various feature sets to performance . </abstract>
<abstract> [T Substring-Based T] [A Transliteration A] . [A Transliteration A] is the task of converting a word from one alphabetic script to another . We present a novel , [T substring-based T] approach to [A transliteration A] , inspired by phrasebased models of machine translation . We investigate two implementations of [T substringbased T] [A transliteration A] : a [T dynamic programming T] algorithm , and a [T finite-state transducer T] . We show that our [T substring-based transducer T] not only outperforms a state-of-the-art letterbased approach by a significant margin , but is also orders of magnitude faster . </abstract>
<abstract> Inferring [A Tutorial Dialogue Structure A] with [T Hidden Markov T] Modeling . The field of intelligent [A tutoring A] systems has seen many successes in recent years . A significant remaining challenge is the automatic creation of corpus-based [A tutorial dialogue management A] models . This paper reports on early work toward this goal . We identify [A tutorial dialogue A] modes in an [T unsupervised T] fashion using [T hidden Markov T] models ( [T HMMs T] ) trained on input sequences of manually-labeled dialogue acts and adjacency pairs . The two best-fit [T HMMs T] are presented and compared with respect to the [A dialogue structure A] they suggest ; we also discuss potential uses of the methodology for future work . </abstract>
<abstract> Efficient Linearization of [T Tree Kernel Functions T] . The combination of Support Vector Machines with very high dimensional [T kernels T] , such as string or tree kernels , suffers from two major drawbacks : first , the implicit representation of feature spaces does not allow us to understand which features actually triggered the generalization ; second , the resulting computational burden may in some cases render unfeasible to use large data sets for training . We propose an approach based on [T feature space T] reverse engineering to tackle both problems . Our experiments with [T Tree Kernels T] on a [A Semantic Role Labeling A] data set show that the proposed approach can drastically reduce the computational footprint while yielding almost unaffected accuracy . </abstract>
<abstract> [T Unsupervised Learning T] of [A Narrative Event Chains A] . Hand-coded scripts were used in the 1970-80s as knowledge backbones that enabled inference and other NLP tasks requiring deep semantic knowledge . We propose [T unsupervised T] induction of similar schemata called  [A narrative event chains A] from raw newswire text . A [A narrative event chain A] is a partially ordered set of events related by a common protagonist . We describe a three step process to learning [A narrative event chains A] . The first uses[T  unsupervised distributional T] methods to learn [A narrative relations A] between events sharing coreferring arguments . The second applies a [T temporal classifier T] to partially order the connected events . Finally , the third prunes and [T clusters T] self-contained chains from the space of events . We introduce two evaluations : the narrative cloze to evaluate event relatedness , and an order coherence task to evaluate narrative order . We show a 36 % improvement over baseline for narrative prediction and 25 % for temporal coherence . </abstract>
<abstract> [T Translating Lexical Semantic Relations T] : The First Step Towards [A Multilingual Wordnets A] . Establishing correspondences between [A wordnets A] of different languages is essential to both multilingual knowledge processing and for bootstrapping wordnets of low-density languages . We claim that such correspondences must be based on [T lexical semantic relations T] , rather than top ontology or word translations . In particular , we define a translation equivalence relation as a bilingual [T lexical semantic relation T] . Such relations can then be part of a [T logical entailment T] predicting whether source language semantic relations will hold in a target language or not . Our claim is tested with a study of 210 Chinese lexical lemmas and their possible semantic relations links bootstrapped from the Princeton WordNet . The results show that [T lexical semantic relation translations T] are indeed highly precise when they are logically inferable . </abstract>
<abstract> Corpus-Based [A Syntactic Error Detection A] Using [T Syntactic Patterns T] . This paper presents a [T parsing T] system for the [A detection of syntactic errors A] . It combines a robust [T partial parser T] which obtains the main sentence components and a [T finite-state parser T] used for the description of [A syntactic error A][T  patterns T] . The system has been tested on a corpus of real texts , containing both correct and incorrect sentences , with promising results . </abstract>
<abstract> [T Latent Dirichlet Allocation T] with [T Topic-in-Set Knowledge T] . [T Latent Dirichlet Allocation T] is an [T unsupervised graphical T] model which can discover [A latent topics in unlabeled data A] . We propose a mechanism for adding [T partial supervision T] , called [T topic-in-set knowledge T] , to [T latent topic modeling T] . This type of supervision can be used to encourage the recovery of topics which are more relevant to user modeling goals than the topics which would be recovered otherwise . Preliminary experiments on text datasets are presented to demonstrate the potential effectiveness of this method . </abstract>
<abstract> [T Jointly Combining Implicit Constraints T] Improves [A Temporal Ordering A] . Previous work on [A ordering events A] in text has typically focused on local pairwise decisions , ignoring globally inconsistent labels . However , temporal ordering is the type of domain in which global constraints should be relatively easy to represent and reason over . This paper presents a framework that informs local decisions with two types of implicit [T global constraints T] : [T transitivity T] ( A before B and B beforeC implies AbeforeC ) and [T time expression normalization T] ( e.g. last month is before yesterday ) . We show how these constraints can be used to create a more densely-connected network of events , and how global consistency can be enforced by incorporating these constraints into an [T integer linear programming T] framework . We present results on two [A event ordering A] tasks , showing a 3.6 % absolute increase in the accuracy of before\/after classification over a pairwise model . </abstract>
<abstract> [T Tree Kernel T] Engineering In [A Semantic Role Labeling A] Systems . Recent work on the design of automatic systems for [A semantic role labeling A] has shown that feature engineering is a complex task from a modeling and implementation point of view . [T Tree kernels T] alleviate suchcomplexityaskernelfunctionsgenerate features automatically and require less software development for data extraction . In this paper , we study several [T tree kernel T] approaches for both [A boundary detection A] and [A argument classification A] . The comparative experiments on [T Support Vector Machines T] with such [T kernels T] on the CoNLL 2005 dataset show that very simple tree manipulations trigger automatic feature engineering that highly improves accuracy and efficiency in both phases . Moreover , the use of different classifiers for internal andpre-terminalnodesmaintainsthesame accuracy and highly improves efficiency . </abstract>
<abstract> A [T Semi-Supervised T] Approach To Build Annotated Corpus For Chinese  [A Named Entity Recognition A] . 1 This paper presents a [T semi-supervised T] approach to reduce human effort in building an annotated Chinese corpus . One of the disadvantages of many statistical Chinese [A named entity recognition A] systems is that training data may be in short supply , and manually building annotated corpus is expensive . In the proposed approach , we construct an 80M handannotated corpus in three steps : ( 1 ) Automatically annotate training corpus ; ( 2 ) Manually refine small subsets of the automatically annotated corpus ; ( 3 ) Combine small subsets and whole corpus in a [T bootstrapping T] process . Our approach is tested on a state-ofthe-art Chinese [A word segmentation A] system ( Gao et al. , 2003 , 2004 ) . Experiments show that only a small subset of hand-annotated corpus is sufficient to achieve a satisfying performance of the named entity component in this system . </abstract>
<abstract> [A Named Entity Recognition A] for Indian Languages  . Abstract Stub This paper talks about a new approach to recognize [A named entities A] for Indian languages . [T Phonetic matching T] technique is used to match the strings of different languages on the basis of their similar sounding property . We have tested our system with a comparable corpus of English and Hindi language data . This approach is language independent and requires only a set of [T rules T] appropriate for a language . </abstract>
<abstract> [A Parsing Noisy Sentences A] . This paper describes a method to [A parse and understand a `` noisy '' sentence A] that possibly includes errors caused by a speech recognition device . Our parser is connected to a speech recognition device which takes a continuously spoken sentence in Japanese and produces a sequence of phonemes . The output sequence of phonemes can quite possibly include errors : altered phonemes , extra phonemes and missing phonemes . The task is to [A parse the noisy phoneme sequence A] and understand the meaning of the original input sentence , given an augmented context-free grammar whose terminal symbols are phonemes . A very efficient [A parsing A] method is required , as the task 's search space is much larger than that of parsing un-noisy sentences . We adopt the [T generalized LR T][A  parsing A] algorithm , and a certain scoring scheme to select the most likely sentence o ~ t of multiple sentence candidates . The use of a [T confusion matrix T] , which is created in advance by analyzing a large set of input\/output pairs , is discussed to improve the scoring accuracy . The system has been integrated into CMU 's knowledge-based [A machine translation A] system . </abstract>
<abstract> A Framework for Identifying [A Textual Redundancy A] . The task of identifying [A redundant A] information in documents that are generated from multiple sources provides a significant challenge for summarization and QA systems . Traditional clustering techniques detect redundancy at the sentential level and do not guarantee the preservation of all information within the document . We discuss an algorithm that generates a novel [T graph-based T] representation for a document and then utilizes a [T set cover approximation T] algorithm to remove [A redundant A] text from it . Our experiments show that this approach offers a significant performance advantage over clustering when evaluated over an annotated dataset . </abstract>
<abstract> Tools And Methods For [A Computational Lexicology A] . This paper presents a set of tools and methods for acquiring , manipulating , and analyzing [A machinereadable dictionaries A] . We give several detailed examples of the use of these tools and methods for particular analyses . A novel aspect of our work is that it allows the combined processing of multiple [A machine-readable dictionaries A] . Our examples describe analyses of data from Webster 's Seventh Collegiate Dictionary , the Longman Dictionary of Contemporary English , the Collins bilingual dictionaries , the Collins Thesaurus , and the Zingarelli Italian dictionary . We describe existing facilities and results they have produced as well as planned enhancements to those facilities , particularly in the area of managing associations involving the [T senses of polysemous words T] . We show how these enhancements expand the ways in which we can exploit [A machine-readable dictionaries A] in the construction of large [A lexicons A] for natural language processing systems . </abstract>
<abstract> Using A [T Hybrid T] System Of [T Corpus - And Knowledge-Based T] Techniques To Automate The Induction Of A [A Lexical Sublanguage Grammar A] . Porting a Natural Language Processing ( NLP ) system to a new donmin renmins one of the bottlenecks in syntactic parsing , because of the amount of effort required to fix gaps in the lexicon , and to attune the existing grammar to the idiosyncracics of the new sublanguage . This paper shows how thc process of fitting a [A lexicalizcd grammar A] to a domain can be automated to a great extent by using a [T hybrid T] system that combines traditimml knowledgebased techniques with a corpus-based approach . </abstract>
<abstract> [T Clustering T] Technique in [A Multi-Document Personal Name Disambiguation A] . Focusing on [A multi-document personal name disambiguation A] , this paper develops an [T agglomerative clustering T] approach to resolving this problem . We start from an analysis of [T pointwise mutual information T] between feature and the ambiguous name , which brings about a novel weight computing method for feature in [T clustering T] . Then a trade-off measure between within-cluster compactness and among-cluster separation is proposed for stopping [T clustering T] . After that , we apply a [T labeling T] method to find representative feature for each cluster . Finally , experiments are conducted on word-based clustering in Chinese dataset and the result shows a good effect . </abstract>
<abstract> Inducing [A Search Keys for Name Filtering A] . This paper describes [T ETK T] ( [T Ensemble of Transformation-based Keys T] ) a new algorithm for inducing [A search keys for name filtering A] . [T ETK T] has the low computational cost and ability to [A filter A] by [T phonetic similarity T] characteristic of phonetic keys such as Soundex , but is adaptable to alternative similarity models . The accuracy of [T ETK T] in a preliminary empirical evaluation suggests that it is well-suited for phonetic [A filtering A] applications such as recognizing alternative [A cross-lingual transliterations A] . </abstract>
<abstract> A Punjabi [A Grammar Checker A] . This article provides description about the [A grammar checking A] software developed for detecting the  [A grammatical errors A] in Punjabi texts and providing suggestions wherever appropriate to rectify those errors . This system utilizes a full-form  [T lexicon T] for [T morphology T] analysis and [T rule-based T] systems for [T part of speech tagging T] and [T phrase chunking T] . The system supported by a set of carefully devised [A error detection A] [T rules T] can detect and suggest rectifications for a number of [A grammatical errors A] , resulting from lack of agreement , order of words in various phrases etc. , in literary style Punjabi texts . </abstract>
<abstract> Using [T Word Support T] Model To Improve Chinese [A Input A] System . This paper presents a [T word support T] model ( [T WSM T] ) . The [T WSM T] can effectively perform [T homophone T] selection and [T syllable-word segmentation T] to improve Chinese [A input A] systems . The experimental results show that : ( 1 ) the [T WSM T] is able to achieve tonal ( [A syllables input A] with four tones ) and toneless ( [A syllables input A] without four tones ) syllable-to-word ( STW ) accuracies of 99 % and 92 % , respectively , among the converted words ; and ( 2 ) while applying the [T WSM T] as an adaptation processing , together with the Microsoft Input Method Editor 2003 ( MSIME ) and an optimized [T bigram T] model , the average tonal and toneless STW improvements are 37 % and 35 % , respectively . </abstract>
<abstract> Coupling [T Semi-Supervised Learning T] of [A Categories and Relations A] . We consider [T semi-supervised learning T] of [A information extraction A] methods , especially for extracting instances of noun categories ( e.g. , ` athlete , ' ` team ' ) and relations ( e.g. , ` playsForTeam ( athlete , team ) ' ) . [T Semisupervised T] approaches using a small number of labeled examples together with many unlabeled examples are often unreliable as they frequently produce an internally consistent , but nevertheless incorrect set of extractions . We propose that this problem can be overcome by [T simultaneously learning classifiers T] for many different [A categories and relations A] in the presence of an [T ontology T] defining [T constraints T] that couple the training of these [T classifiers T] . Experimental results show that [T simultaneously learning T] a coupled collection of [T classifiers T] for 30 [A categories and relations A] results in much more accurate extractions than training classifiers individually . </abstract>
<abstract> Acquiring [A Predicate-Argument Mapping A] Information From Multilingual Texts . This paper discusses automatic acquisition of [A predicate-argument mapping A] information from multilingual texts . The [T lexicon T] of our NLP system abstracts the language-dependent portion of [A predicate-argument mapping A] information from the core meaning of verb senses ( i.e. semantic concepts as defined in the knowledge base ) . We represent this mapping information in terms of cross-linguistically generalized mapping types called [T situation types T] and [T word sense-specific idiosyncrasies T] . This representation has enabled us to automatically acquire [A predicate-argument mapping A] information , specifically [T situation types T] and [T idiosyncrasies T] , for verbs in English , Spanish , and Japanese texts . </abstract>
<abstract> Robustness Issues In A [T Data-Driven T] [A Spoken Language Understanding A] System . Robustness is a key requirement in [A spoken language understanding A] ( [A SLU A] ) systems . Human speech is often ungrammatical and ill-formed , and there will frequently be a mismatch between training and test data . This paper discusses robustness and adaptation issues in a [T statistically-based T] [A SLU A] system which is entirely [T data-driven T] . To test robustness , the system has been tested on data from the Air Travel Information Service ( ATIS ) domain which has been artificially corrupted with varying levels of additive noise . Although the speech recognition performance degraded steadily , the system did not fail catastrophically . Indeed , the rate at which the end-to-end performance of the complete system degraded was significantly slower than that of the actual recognition component . In a second set of experiments , the ability to rapidly adapt the core understanding component of the system to a different application within the same broad domain has been tested . Using only a small amount of training data , experiments have shown that a semantic parser based on the [T Hidden Vector State T] ( [T HVS T] ) model originally trained on the ATIS corpus can be straightforwardly adapted to the somewhat different DARPA Communicator task using standard [T adaptation T] algorithms . The paper concludes by suggesting that the results presented provide initial support to the claim that an [A SLU  A]system which is [T statistically-based T] and trained entirely from data is intrinsically robust and can be readily adapted to new applications . </abstract>
<abstract> Automatic [A Partial Parsing Rule A] Acquisition Using [T Decision Tree T] Induction . [A Partial parsing A] techniques try to recover syntactic information eﬃciently and reliably by sacrificing completeness and depth of analysis . One of the diﬃculties of [A partial parsing A] is finding a means to extract the grammar involved automatically . In this paper , we present a method for automatically extracting [A partial parsing rules A] from a tree-annotated corpus using [T decision tree T] induction . We define the [A partial parsing rules A] as those that can decide the structure of a substring in an input sentence deterministically . This decision can be considered as a [T classification T] ; as such , for a substring in an input sentence , a proper structure is chosen among the structures occurred in the corpus . For the classification , we use [T decision tree T] induction , and induce [A partial parsing rules A] from the [T decision tree T] . The acquired grammar is similar to a phrase structure grammar , with contextual and lexical information , but it allows building structures of depth one or more . Our experiments showed that the proposed [A partial parser A] using the automatically extracted rules is not only accurate and eﬃcient , but also achieves reasonable coverage for Korean . </abstract>
<abstract> An Experiment In [A Semantic Tagging A] Using [T Hidden Markov Model T] [A Tagging A] . The same word can have many different meanings depending on the context in which it is used . Discovering the meaning of a word , given the text around it , has been an interesting problem for both the psychology and the artificial intelligence research communities . In this article , we present a series of experiments , using methods which have proven to be useful for eliminating part-of-speech ambiguity , to see if such simple methods can be used to resolve [A semantic ambiguities A] . Using a publicly available semantic [T lexicon T] , we find the [T Hidden Markov T] Models work surprising well at choosing the right [A semantic categories A] , once the sentence has been stripped of purely functional words . </abstract>
<abstract> Using [T Encyclopedic T] Knowledge For [A Named Entity A] Disambiguation . We present a new method for detecting and disambiguating  [A named entities A] in open domain text . A disambiguation [T SVM kernel T] is trained to exploit the high coverage and rich structure of the knowledge encoded in an online [T encyclopedia T] . The resulting model significantly outperforms a less informed baseline . </abstract>
<abstract> Open Text [A Semantic Parsing A] Using [T FrameNet T] And [T WordNet T] . This paper describes a [T rule-based T] [A semantic parser A] that relies on a [T frame T] dataset ( [T FrameNet T] ) , and a [T semantic network T] ( [T WordNet T] ) , to identify semantic relations between words in open text , as well as shallow semantic features associated with concepts in the text . [A Parsing semantic structures A] allows semantic units and constituents to be accessed and processed in a more meaningful way than syntactic parsing , moving the automation of understanding natural language text to a higher level . </abstract>
<abstract> [T Statistical Phrase-Based T] Models For Interactive [A Computer-Assisted Translation A] . Obtaining high-quality machine translations is still a long way off . A postediting phase is required to improve the output of a machine translation system . An alternative is the so called [A computerassisted translation A] . In this framework , a human translator interacts with the system in order to obtain high-quality translations . A [T statistical phrase-based T] approach to [A computer-assisted translation A] is described in this article . A new decoder algorithm for interactive search is also presented , that combines monotone and nonmonotone search . The system has been assessed in the TransType-2 project for the translation of several printer manuals , from ( to ) English to ( from ) Spanish , German and French . </abstract>
<abstract> Japanese [A Morphological Analyzer A] using [T Word Co-occurrence T] - JTAG . We developed a Japanese [A morphological analyzer A] that uses the [T co-occurrence of words T] to select the correct sequence of words in an unsegmented Japanese sentence . The [T co-occurrence T] information can be obtained from cases where the system incorrectly analyzes sentences . As the amount of information increases , the accuracy of the system increases with a small risk of degradation . Experimental results show that the proposed system assigns the correct phonological representations to unsegmented Japanese sentences more precisely than do other popular systems . </abstract>
<abstract> [T Active Learning T] for [A Word Sense Disambiguation A] with Methods for Addressing the Class Imbalance Problem . In this paper , we analyze the effect of [T resampling T] techniques , including undersampling and over-sampling used in [T  active learning T] for [A word sense disambiguation A] ( [A WSD A] ) . Experimental results show that under-sampling causes negative effects on active learning , but over-sampling is a relatively good choice . To alleviate the withinclass imbalance problem of over-sampling , we propose a [T bootstrap-based oversampling T] ( [T BootOS T] ) method that works better than ordinary over-sampling in [T active learning T] for [A WSD A] . Finally , we investigate when to stop [T active learning T] , and adopt two strategies , max-confidence and min-error , as stopping conditions for [T active learning T] . According to experimental results , we suggest a prediction solution by considering max-confidence as the upper bound and min-error as the lower bound for stopping conditions . </abstract>
<abstract> Combining [T Trigram T] and [T Winnow T] in Thai [A OCR Error Correction A] . For languages that have no explicit word boundary such as Thai , Chinese and Japanese , correcting words in text is harder than in English because of additional ambiguities in locating error words . The traditional method handles this by hypothesizing that every substrings in the input sentence could be error words and trying to correct all of them . In this paper , we propose the idea of reducing the scope of [A spelling correction A] by focusing only on dubious areas in the input sentence . Boundaries of these dubious areas could be obtained approximately by applying [T word segmentation T] algorithm and finding word sequences with low probability . To generate the candidate correction words , we used a modified [T edit distance T] which reflects the characteristic of Thai [A OCR errors A] . Finally , a [T part-ofspeech trigram T] model and [T Winnow T] algorithm are combined to determine the most probable correction . </abstract>
<abstract> A [T Machine Learning T] Approach To Extract [A  Temporal Information A] From Texts In Swedish And Generate Animated 3D Scenes . Carsim is a program that automatically converts narratives into 3D scenes . Carsim considers authentic texts describing road accidents , generally collected from web sitesofSwedishnewspapers ortranscribed from hand-written accounts by victims of accidents . One of the program 's key featuresisthatitanimatesthegenerated scene to visualize events . To create a consistent animation , Carsim extracts the participants mentioned in a text and identifies what they do . In this paper , we focus on the extraction of [A temporal relations A] between actions . We first describe how we detect [A time expressions and events A] . We then present a [T machine learning T] technique to [A order the sequence of events A] identified in the narratives . We finally report the results we obtained . </abstract>
<abstract> [T Task-Based T]  [A Dialog Management A] Using An [T Agenda T] . Dialog man tigement addresses two specific problems : ( 1 ) providing a coherent overall structure to interaction that extends beyond the single turn , ( 2 ) correctly managing mixedinitiative interaction . We propose a [A dialog management A] architecture based on the following elements : handlers that manage interaction focussed on tightly coupled sets of information , a product that reflects mutually agreed-upon information and an [T agenda T] that orders the topics relevant to task completion . </abstract>
<abstract> Sharing Problems And Solutions For [A Machine Translation A] Of Spoken And Written Interaction . Examples from chat interaction are presented to demonstrate that [A machine translation A] of written interaction shares many problems with translation of spoken interaction . The potential for common solutions to the problems is illustrated by describing operations that [T normalize T] and [T tag T] input before [A translation A] . [T Segmenting utterances T] into small translation units and processing short turns separately are also motivated using data from chat . </abstract>
<abstract> Constructing [A Verb Semantic Classes A] For French : Methods And Evaluation . In this paper , we study a reformulation , which is better adapted to NLP , of the alternation system developed for English by B. Levin . We have studied a set of 1700 verbs from which we explain how [A verb semantic classes A] can be built in a systematic way . The quality of the results w.r. t , semantic chLssifications such as WordNet is then evaluated . </abstract>
<abstract> [A Named Entity Recognition A] Using An [T HMM-Based Chunk Tagger T] . This paper proposes a [T Hidden Markov Model T] ( [T HMM T] ) and an [T HMM-based chunk tagger T] , from which a [A named entity ( NE ) recognition ( NER )  A] system is built to recognize and classify names , times and numerical quantities . Through the [T HMM T] , our system is able to apply and integrate four types of internal and external evidences : 1 ) simple deterministic internal feature of the words , such as [T capitalization T] and [T digitalization T] ; 2 ) internal [T semantic T] feature of important triggers ; 3 ) internal [T gazetteer T] feature ; 4 ) external [T macro context T] feature . In this way , the [A NER A] problem can be resolved effectively . Evaluation of our system on MUC-6 and MUC-7 English [A NE A] tasks achieves F-measures of 96.6 % and 94.1 % respectively . It shows that the performance is significantly better than reported by any other machine-learning system . Moreover , the performance is even consistently better than those based on handcrafted rules . </abstract>
<abstract> [T Clause Aggregation T] Using [T Linguistic Knowledge T] . By combining multiple clauses into one single sentence , a [A text generation A] system can express the same amount of information in fewer words and at the same time , produce a great variety of complex constructions . In this paper , we describe [T hypotactic and paratactic operators T] for [A generating complex sentences A] from clause-sized semantic representations . These two types of operators are portable and reusable because they are based on general resources such as the [T lexicon T] and the [T grammar T] . </abstract>
<abstract> [T Prior Derivation T] Models For Formally [T Syntax-Based T] [A  Translation A] Using [T Linguistically Syntactic Parsing T] and [T Tree Kernels T] . This paper presents an improved formally [T syntax-based T] [A SMT A] model , which is enriched by linguistically syntactic knowledge obtained from [T statistical constituent parsers T] . We propose a linguistically-motivated [T prior derivation T] model to score hypothesis derivations on top of the baseline model during the translation decoding . Moreover , we devise a fast [T training T] algorithm to achieve such improved models based on [T tree kernel T] methods . Experiments on an English-to-Chinese task demonstrate that our proposed models outperformed the baseline formally syntaxbased models , while both of them achieved signi cant improvements over a state-of-theart phrase-based SMT system . </abstract>
<abstract> An Efficient [T Clustering T] Algorithm For [T  Class-Based T] [A Language Models A] . This paper defines a general form for  [T classbased probabilistic T] [A language models A] and proposes an efficient algorithm for [T clustering T] based on this . Our evaluation experiments revealed that our method decreased computation time drastically , while retaining accuracy . </abstract>
<abstract> Transforming [T Examples Into Patterns T] For [A Information Extraction A] . [A Information Extraction A] ( [A IE A] ) systems today are commonly based on pattern matching . The patterns are regular expressions stored in a customizable knowledge base . Adapting an IE system to a new subject domain entails the construction of a new pattern base - a time-consuming and expensive task . We describe a strategy for building [T patterns from examples T] . To adapt the IE system to a new domain quickly , the user chooses a set of examples in a training text , and for each example gives the logical form entries which the example induces . The system transforms these examples into patterns and then applies [T meta-rules T] to generalize these patterns . </abstract>
<abstract> [A Word Sense Disambiguation A] for All Words using [T Tree-Structured Conditional Random Fields T] . We propose a [T supervised T] [A word sense disambiguation A] ( [A WSD A] ) method using [T tree-structured conditional random fields T] ( [T TCRFs T] ) . By applying [T TCRFs T] to a sentence described as a dependency tree structure , we conduct [A WSD A] as a labeling problem on tree structures . To incorporate dependencies between word senses , we introduce a set of features on tree edges , in combination with coarse-grained tagsets , and show that these contribute to an improvement in WSD accuracy . We also show that the [T tree-structured T] model outperforms the linear-chain model . Experiments on the SENSEVAL-3 data set show that our TCRF model performs comparably with state-of-the-art WSD systems . </abstract>
<abstract> A [T Relational T] Model of [A Semantic Similarity A] between Words using Automatically Extracted [T Lexical Pattern Clusters from the Web T] . [A Semantic similarity A] is a central concept that extends across numerous fields such as artificial intelligence , natural language processing , cognitive science and psychology . Accurate measurement of [A semantic similarity A] between words is essential for various tasks such as , document clustering , information retrieval , and synonym extraction . We propose a novel model of [A semantic similarity A] using the [T semantic relations T] that exist among words . Given two words , first , we represent the [T semantic relations T] that hold between those words using automatically extracted [T lexical pattern clusters T] . Next , the semantic similarity between the two words is computed using a [T Mahalanobis distance measure T] . We compare the proposed similarity measure against previously proposed [A semantic similarity A] measures on Miller-Charles benchmark dataset and WordSimilarity353 collection . The proposed method outperforms all existing web-based [A semantic similarity A] measures , achieving a Pearson correlation coefficient of 0.867 on the Millet-Charles dataset . </abstract>
<abstract> Extraction of [A Entailed Semantic Relations A] Through [T Syntax-Based Comma Resolution T] . This paper studies [A textual inference A] by investigating [T comma structures T] , which are highly frequent elements whose major role in the extraction of [A semantic relations A] has not been hitherto recognized . We introduce the problem of [T comma resolution T] , defined as understanding the role of commas and extracting the relations they imply . We show the importance of the problem using examples from Textual Entailment tasks , and present A [T Sentence Transformation Rule Learner T] ( [T ASTRL T] ) , a [T machine learning T] algorithm that uses a [T syntactic analysis T] of the sentence to learn [T sentence transformation rules T] that can then be used to extract [A relations A] . We have manually annotated a corpus identifying [T comma structures T] and relations they entail and experimented with both gold standard parses and parses created by a leading statistical parser , obtaining F-scores of 80.2 % and 70.4 % respectively . </abstract>
<abstract> [A Lexical Discovery A] With An Enriched [T Semantic Network T] . The study of [A lexical semantics A] has produced a systematic analysis of binary relationships between content words that has greatly benefited lexical search tools and natural language processing algorithms . We first introduce a database system called [T FreeNet T] that facilitates the description and exploration of finite binary relations . We then describe the design and implementation of Lexical [T FreeNet T] , a [T semantic network T] that mixes [T WordNet-derived T] semantic relations with [T data-derived T] and [T phonetically-derived T] relations . We discuss how [T Lexical FreeNet T] has aided in [A lexical discovery A] , the pursuit of linguistic and factual knowledge by the computer-aided exploration of lexical relations . </abstract>
<abstract> [A Name Origin Recognition A] Using [T Maximum Entropy Model T] and Diverse Features . [A Name origin recognition A] is to identify the source language of a personal or location name . Some early work used either rulebased or statistical methods with single knowledge source . In this paper , we cast the [A name origin recognition A] as a [T multi-class classification T] problem and approach the problem using [T Maximum Entropy T] method . In doing so , we investigate the use of different features , including [T phonetic rules T] , [T ngram statistics T] and [T character position T] information for name origin recognition . Experiments on a publicly available personal name database show that the proposed approach achieves an overall accuracy of 98.44 % for names written in English and 98.10 % for names written in Chinese , which are significantly and consistently better than those in reported work . </abstract>
<abstract> [A Summarizing Natural Language Database Responses A] . In a human dialogue it is usually considered inappropriate if one conversant monopolizes the conversation . Similarly it can be inappropriate for a natural language database interface to respond with a lengthy list of data . A non-enumerative `` summary '' response is less verbose and often avoids misleading the user where an extensional response might . In this paper we investigate the problem of generating such [A discourse-oriented concise responses A] . We present details of the design and implementation of a system that produces [A summary responses A] to queries of a relational data base . The system employs a set of [T heuristics T] that work in conjunction with a [T knowledge base T] to discover underlying regularities that form the basis of [A summary responses A] . The system is largely domain-independent , and hence can be ported relatively easily from one data base to another . It can handle a wide variety of situations requiring a [A summary response A] and can be readily extended . It also has a number of shortcomings which are discussed thoroughly and which form the basis for a number of suggested research directions . </abstract>
<abstract> [A Acceptability Prediction A] By Means Of [T Grammaticality Quantification T] . We propose in this paper a method for [A quantifying sentence grammaticality A] . The approach based on [T Property Grammars T] , a [T constraint-based syntactic T] formalism , makes it possible to evaluate a [A grammaticality index A] for any kind of sentence , including ill-formed ones . We compare on a sample of sentences the [A grammaticality indices A] obtained from [T PG T] formalism and the acceptability judgements measured by means of a psycholinguistic analysis . The results show that the derived [A grammaticality index A] is a fairly good tracer of acceptability scores . </abstract>
<abstract> Robust , Applied [A Morphological Generation A] . In practical natural language generation systems it is often advantageous to have a separate component that deals purely with morphological processing . We present such a component : a fast and robust [A morphological generator A] for English based on [T finite-state T] techniques that generates a word form given a specification of the [T lemma T] , [T part-of-speech T] , and the type of [T inflection T] required . We describe how this [A morphological generator A] is used in a prototype system for automatic simplification of English newspaper text , and discuss practical morphological and orthographic issues we have encountered in generation of unrestricted text within this application . </abstract>
<abstract> [A Multilingual Semantic Parsing A] with a Pipeline of [T Linear Classifiers T] . I describe a fast [A multilingual parser A] for [A semantic dependencies A] . The parser is implemented as a pipeline of [T linear classifiers T] trained with [T support vector machines T] . I use only [T first order features T] , and no pair-wise feature combinations in order to reduce training and prediction times . Hyper-parameters are carefully tuned for each language and sub-problem . The system is evaluated on seven different languages : Catalan , Chinese , Czech , English , German , Japanese and Spanish . An analysis of learning rates and of the reliance on syntactic parsing quality shows that only modest improvements could be expected for most languages given more training data ; Better syntactic parsing quality , on the other hand , could greatly improve the results . Individual tuning of hyper-parameters is crucial for obtaining good [A semantic parsing A] quality . </abstract>
<abstract> On The [A Subjectivity Of Human Authored Summaries A] . We address the issue of [A human subjectivity when authoring summaries A] , aiming at a simple , robust evaluation of machine generated summaries . Applying a [T cross comprehension test T] on human authored short summaries from broadcast news , the level of subjectivity is gauged among four authors . The instruction set is simple , thus there is enough room for subjectivity . However the approach is robust because the test does not use the absolute score , relying instead on relative comparison , effectively alleviating the subjectivity . Finally we illustrate the application of the above scheme when evaluating the informativeness of machine generated summaries . </abstract>
<abstract> Pragmatic [A Information Extraction A] From [T Subject Ellipsis T] In Informal English . [T Subject ellipsis T] is one of the characteristics of informal English . The investigation of [T subject ellipsis T] in corpora thus reveals an abundance of pragmatic and extralinguistic information associated with [T subject ellipsis T] that enhances natural language understanding . In essence , the presence of [T subject elipsis T] conveys an ` informal ' conversation involving 1 ) an informal ` Topic ' as well as familiar\/close ` Participants ' , 2 ) specific ` Conotations ' that are different from the corresponding ful sentences : interruptive ( ending discourse coherence ) , polite , intimate , friendly , and less determinate implicatures . This paper also construes linguistic environments that triger the use of [T subject ellipsis T] and resolve [T subject elipsis T] . </abstract>
<abstract> [T Data-Driven T]Classification Of  [A Linguistic Styles In Spoken Dialogues A] . Language users have individual [A linguistic styles A] . A [A spoken dialogue A] system may benefit from adapting to the linguistic style of a user in input analysis and output generation . To investigate the possibility to automatically classify speakers according to their [A linguistic style A] three corpora of [A spoken dialogues A] were analyzed . Several numerical parameters were computed for every speaker . These parameters were reduced to linguistically interpretable components by means of a [T principal component analysis T] . Classes were established from these components by [T cluster analysis T] . Unseen input was classified by trained [T neural networks T] with varying error rates depending on corpus type . A first investigation in using special [T language models T] for speaker classes was carried out . </abstract>
<abstract> A [T Lexicon-Constrained Character T] Model for Chinese  [A Morphological Analysis A] . This paper proposes a [T lexicon-constrained character T] model that combines both [T word and character features T] to solve complicated issues in Chinese  [A morphological analysis A] . A Chinese  [T character-based T] model constrained by a [T lexicon T] is built to acquire word building rules . Each character in a Chinese sentence is assigned a tag by the proposed model . The [A word segmentation A] and [A partof-speech tagging A] results are then generated based on the character tags . The proposed method solves such problems as unknown word identification , data sparseness , and estimation bias in an integrated , unified framework . Preliminary experiments indicate that the proposed method outperforms the best SIGHAN word segmentation systems in the open track on 3 out of the 4 test corpora . Additionally , our method can be conveniently integrated with any other Chinese morphological systems as a post-processing module leading to significant improvement in performance . </abstract>
<abstract> Extracting [A Comparative Sentences A] from Korean Text Documents Using Comparative [T Lexical Patterns T] and [T Machine Learning T] Techniques . This paper proposes how to automatically identify Korean [A comparative sentences A] from text documents . This paper first investigates many comparative sentences referring to previous studies and then defines a set of comparative keywords from them . A sentence which contains one or more elements of the keyword set is called a comparative-sentence candidate . Finally , we use [T machine learning T] techniques to [A eliminate non-comparative sentences from the candidates A] . As a result , we achieved significant performance , an F1-score of 88.54 % , in our experiments using various web documents . </abstract>
<abstract> The Good , the Bad , and the Unknown : Morphosyllabic  [A Sentiment Tagging of Unseen Words A] . The omnipresence of unknown words is a problem that any NLP component needs to address in some form . While there exist many established techniques for dealing with unknown words in the realm of POS-tagging , for example , guessing unknown words ' semantic properties is a less-explored area with greater challenges . In this paper , we study the [A semantic field of sentiment A] and propose five methods for assigning prior [A sentiment polarities to unknown words A] based on known [T sentiment carriers T] . Tested on 2000 cases , the methods mirror human judgements closely in threeand twoway polarity classification tasks , and reach accuracies above 63 % and 81 % , respectively . </abstract>
<abstract> [T Segment Choice T] Models : [T Feature-Rich T] Models For Global Distortion In [T Statistical T] [A  Machine Translation A] . This paper presents a new approach to [T distortion T] ( [T phrase reordering T] ) in [T phrasebased T] [A machine translation ( MT ) A] . Distortion is modeled as a sequence of choices during translation . The approach yields trainable , [T probabilistic distortion T] models that are global : they assign a probability to each possible [T phrase reordering T] . These [T `` segment choice '' T] models ( [T SCMs T] ) can be trained on `` segment-aligned '' sentence pairs ; they can be applied during decoding or rescoring . The approach yields a metric called `` [T distortion perplexity T] '' ( `` disperp '' ) for comparing SCMs offline on test data , analogous to perplexity for language models . A [T decision-tree-based SCM T] is tested on Chinese-to-English [A translation A] , and outperforms a baseline distortion penalty approach at the 99 % confidence level . </abstract>
<abstract> [T Minimum Cut T] Model For [A Spoken Lecture Segmentation A] . We consider the task of [T unsupervised T] [A lecture segmentation A] . We formalize [A segmentation A] as a [T graph-partitioning T] task that optimizes the [T normalized cut T] criterion . Our approach moves beyond localized comparisons and takes into account [T longrange cohesion dependencies T] . Our results demonstrate that global analysis improves the [A segmentation A] accuracy and is robust in the presence of speech recognition errors . </abstract>
<abstract> A [T Computational T] Approach To [A Deciphering Unknown Scripts A] . We propose and evaluate [T computational  T]techniques for [A deciphering unknown scripts A] . We focus on the case in which an unfamiliar script encodes a known language . The decipherment of a brief document or inscription is driven by data about the spoken language . We consider which scripts are easy or hard to decipher , how much data is required , and whether the techniques are robust against language change over time . </abstract>
<abstract> A Small-Vocabulary Shared Task for [A Medical Speech Translation A] . We outline a possible small-vocabulary shared task for the emerging [A medical speech translation A] community . Data would consist of about 2000 recorded and transcribed utterances collected during an evaluation of an English ↔ Spanish version of the Open Source MedSLT system ; the vocabulary covered consisted of about 450 words in English , and 250 in Spanish . The key problem in defining the task is to agree on a scoring system which is acceptable both to medical professionals and to the speech and language community . We suggest a framework for defining and administering a scoring system of this kind . </abstract>
<abstract> Using [T Bilingual Knowledge T] and [T Ensemble T] Techniques for [T Unsupervised T] Chinese[A  Sentiment A] Analysis . It is a challenging task to identify [A sentiment polarity A] of Chinese reviews because the resources for Chinese [A sentiment A] analysis are limited . Instead of leveraging only monolingual Chinese knowledge , this study proposes a novel approach to leverage reliable English resources to improve Chinese  [A sentiment A] analysis . Rather than simply projecting English resources onto Chinese resources , our approach first [T translates T] Chinese reviews into English reviews by [T machine translation T] services , and then identifies the [A sentiment polarity A] of English reviews by directly leveraging English resources . Furthermore , our approach performs [A sentiment A] analysis for both Chinese reviews and English reviews , and then uses [T ensemble T] methods to combine the individual analysis results . Experimental results on a dataset of 886 Chinese product reviews demonstrate the effectiveness of the proposed approach . The individual analysis of the translated English reviews outperforms the individual analysis of the original Chinese reviews , and the combination of the individual analysis results further improves the performance . </abstract>
<abstract> [T Coreference-inspired T] [A Coherence Modeling A] . Research on [T coreference resolution T] and summarization has modeled the way entities are realized as concrete phrases in discourse . In particular there exist models of the noun phrase syntax used for discourse-new versus discourse-old referents , and models describing the likely distance between a pronoun and its antecedent . However , models of [A discourse coherence A] , as applied to [A information ordering A] tasks , have ignored these kinds of information . We apply a [T discourse-new classifier T] and [T pronoun coreference T] algorithm to the [A information ordering A] task , and show significant improvements in performance over the entity grid , a popular model of local coherence . </abstract>
<abstract> [A Spoken Dialogue Interpretation A] with the [T DOP T] Model . We show how the [T DOP T] model can be used for fast and robust processing of [A spoken input A] in a practical [A spoken dialogue A] system called OVIS . OVIS , Openbaar Vervoer Informatie Systeem ( `` Public Transport Information System '' ) , is a Dutch [A spoken language information A] system which operates over ordinary telephone lines . The prototype system is the immediate goal of the NWO 1 Priority Programme `` Language and Speech Technology '' . In this paper , we extend the original [T DOP T] model to [T context-sensitive T]interpretation of  [A spoken input A] . The system we describe uses the OVIS corpus ( 10,000 trees enriched with compositional semantics ) to compute from an input word-graph the best utterance together with its meaning . Dialogue [T context T] is taken into account by dividing up the OVIS corpus into context-dependent subcorpora . Each system question triggers a subcorpus by which the user answer is analyzed and interpreted . Our experiments indicate that the [T context-sensitive DOP T] model obtains better accuracy than the original model , allowing for fast and robust processing of [A spoken input A] . </abstract>
<abstract> [T MMR-Based Active Machine Learning T] For [A Bio Named Entity Recognition A] . This paper presents a new [T active learning T] paradigm which considers not only the uncertainty of the classifier but also the diversity of the corpus . The two measures for uncertainty and diversity were combined using the [T MMR T] ( [T Maximal Marginal Relevance T] ) method to give the sampling scores in our [T active learning T] strategy . We incorporated [T MMR-based active machinelearning T] idea into the [A biomedical namedentity recognition A] system . Our experimental results indicated that our strategies for [T active-learning T] based sample selection could significantly reduce the human effort . </abstract>
<abstract> Accurate Learning for Chinese  [A Function Tags A] from Minimal Features . [T Data-driven T] [A function tag A] assignment has been studied for English using Penn Treebank data . In this paper , we address the question of whether such method can be applied to other languages and Treebank resources . In addition to simply extend previous method from English to Chinese , we also proposed an effective way to recognize [A function tags A] directly from [T lexical information T] , which is easily scalable for languages that lack sufficient parsing resources or have inherent linguistic challenges for parsing . We investigated a [T supervised sequence learning T] method to automatically recognize [A function tags A] , which achieves an F-score of 0.938 on gold-standard POS ( Part-ofSpeech ) tagged Chinese text -- a statistically significant improvement over existing Chinese function label assignment systems . Results show that a small number of linguistically motivated [T lexical features T] are sufficient to achieve comparable performance to systems using sophisticated parse trees . </abstract>
<abstract> A [T DOP T] Model For [A Semantic Interpretation A] . In data-oriented language processing , an annotated language corpus is used as a stochastic grammar . The most probable analysis of a new sentence is constructed by combining fragments from the corpus in the most probable way . This approach has been successfully used for syntactic analysis , using corpora with syntactic annotations such as the Penn Tree-bank . If a corpus with semantically annotated sentences is used , the same approach can also generate the most probable [A semantic interpretation A] of an input sentence . The present paper explains this [A semantic interpretation A] method . A [T data-oriented T] [A semantic interpretation A] algorithm was tested on two semantically annotated corpora : the English ATIS corpus and the Dutch OVIS corpus . Experiments show an increase in semantic accuracy if larger corpus-fragments are taken into consideration . </abstract>
<abstract> Classifying Particle Semantics In [A English Verb-Particle Constructions A] . Previous computational work on learning the semantic properties of [A verb-particle constructions A] ( [A VPCs A] ) has focused on their compositionality , and has left unaddressed the issue of which meaning of the component words is being used in a given [A VPC A] . We develop a [T feature space T] for use in classification of the sense contributed by the particle in a [A VPC A] , and test this on [A VPCs A] using the particle up . The features that capture [T linguistic properties T] of [A VPCs A] that are relevant to the semantics of the particle outperform linguistically uninformed word co-occurrence features in our experiments on unseen test [A VPCs A] . </abstract>
<abstract> A High-Performance [A Coreference Resolution A] System Using A [T Constraint-Based Multi-Agent T] Strategy . This paper presents a [T constraint-based multiagent T] strategy to [A coreference resolution A] of general noun phrases in unrestricted English text . For a given anaphor and all the preceding referring expressions as the antecedent candidates , a common [T constraint agent T] is first presented to filter out invalid antecedent candidates using various kinds of general knowledge . Then , according to the type of the anaphor , a special [T constraint agent T] is proposed to filter out more invalid antecedent candidates using constraints which are derived from various kinds of special knowledge . Finally , a simple [T preference agent T] is used to choose an antecedent for the anaphor form the remaining antecedent candidates , based on the proximity principle . One interesting observation is that the most recent antecedent of an anaphor in the [A coreferential chain A] is sometimes indirectly linked to the anaphor via some other antecedents in the chain . In this case , we find that the most recent antecedent always contains little information to directly determine the [A coreference A] relationship with the anaphor . Therefore , for a given anaphor , the corresponding special constraint agent can always safely filter out these less informative antecedent candidates . In this way , rather than finding the most recent antecedent for an anaphor , our system tries to find the most direct and informative antecedent . Evaluation shows that our system achieves Precision \/ Recall \/ F-measures of 84.7 % \/ 65.8 % \/ 73.9 and 82.8 % \/ 55.7 % \/ 66.5 on MUC6 and MUC-7 English [A coreference A] tasks respectively . This means that our system achieves significantly better precision rates by about 8 percent over the best-reported systems while keeping recall rates . </abstract>
<abstract> Towards An Optimal Lexicalization In A Natural-Sounding Portable [A Natural Language Generator A] For [A Dialog A] Systems . In contrast to the latest progress in speech recognition , the state-of-the-art in [A natural language generation for spoken language dialog A] systems is lagging behind . The core dialog managers are now more sophisticated ; and natural-sounding and flexible output is expected , but not achieved with current simple techniques such as template-based systems . Portability of systems across subject domains and languages is another increasingly important requirement in dialog systems . This paper presents an outline of LEGEND , a system that is both portable and generates natural-sounding output . This goal is achieved through the novel use of existing [T lexical resources T] such as [T FrameNet T] and [T WordNet T] . </abstract>
<abstract> Making Sense of [A Word Sense Variation A] . We present a pilot study of [A word-sense A] annotation using multiple annotators , relatively polysemous words , and a heterogenous corpus . Annotators selected senses for words in context , using an annotation interface that presented [T WordNet T] senses . Interannotator agreement ( IA ) results show that annotators agree well or not , depending primarily on the individual words and their general usage properties . Our focus is on identifying systematic differences across words and annotators that can account for IA variation . We identify three lexical use factors : semantic specificity of the context , sense concreteness , and similarity of senses . We discuss systematic differences in sense selection across annotators , and present the use of [T association rules T] to mine the data for systematic differences across annotators . </abstract>
<abstract> [A Query Segmentation A] Based on [T Eigenspace Similarity T] . [A Query segmentation A] is essential to query processing . It aims to tokenize query words into several semantic segments and help the search engine to improve the precision of retrieval . In this paper , we present a novel [T unsupervised learning T] approach to [A query segmentation A] based on [T principal eigenspace similarity T] of [T queryword-frequency matrix T] derived from [T web statistics T] . Experimental results show that our approach could achieve superior performance of 35.8 % and 17.7 % in Fmeasure over the two baselines respectively , i.e. MI ( Mutual Information ) approach and EM optimization approach . </abstract>
<abstract> Exploiting [T Syntactic Structure T] for [A Language Modeling A] . The paper presents a [A language model A] that develops [T syntactic structure T] and uses it to extract meaningful information from the word history , thus enabling the use of [T long distance dependencies T] . The model assigns probability to every joint sequence of words-binary-parse-structure with headword annotation and operates in a left-to-right manner - therefore usable for [A automatic speech recognition A] . The model , its [T probabilistic T] parameterization , and a set of experiments meant to evaluate its predictive power are presented ; an improvement over standard [A trigram modeling A] is achieved . </abstract>
<abstract> [A Spelling-Checking A] For Highly Inflective Languages . [A Spelling-checkers A] have become an integral part of most text processing software . From different reasons among which the speed of processing prevails they are usually based on dictionaries of word forms instead of words . This approach is sufficient for languages with little inflection such as English , but fails for highly inflective languages such as Czech , Russian , Slovak or other Slavonic languages . We have developed a special method for describing inflection for the purpose of building [A spelling-checkers A] for such languages . The speed of the resulting program lies somewhere in the middle of the scale of existing spelling-checkers for English and the main dictionary fits into the standard 360K floppy , whereas the number of recognized word forms exceeds 6 million ( for Czech ) . Further , a special method has been developed for easy [T word classification T] . </abstract>
<abstract> [T Prosodic T] Aids To [A Syntactic And Semantic A] Analysis Of Spoken English . [T Prosody T] can be useful in resolving certain lexical and structural ambiguities in spoken English . In this paper we present some results of employing two types of [T prosodic T] information , namely [T pitch and pause T] , to assist [A syntactic and semantic A] analysis during [A parsing A] . </abstract>
<abstract> [T Unsupervised Learning T] Of [A Derivational Morphology A] From [T Inflectional Lexicons T] . We present in this paper an [T unsupervised T] method to learn [A suffixes and suffixation operations A] from an [T inflectional lexicon T] of a language . The elements acquired with our method are used to build [A stemming A] procedures and can assist lexicographers in the development of new lexical resources . </abstract>
<abstract> Using The [T Distribution Of Performance T] For [A Studying Statistical NLP Systems A] And Corpora . Statistical NLP systems are frequently evaluated and compared on the basis of their performances on a single split of training and test data . Results obtained using a single split are , however , subject to sampling noise . In this paper we argue in favor of reporting a [T distribution of performance T] gures , obtained by [T resampling T] the training data , rather than a single number . The additional information from distributions can be used to make statistically quanti ed statements about di erences across parameter settings , systems , and corpora . </abstract>
<abstract> [A Story Understanding A] Through [T Multi-Representation  T] Model Construction . We present an implemented model of [A story understanding A] and apply it to the understanding of a children 's story . We argue that understanding a story consists of building [T multirepresentation T] models of the story and that story models are efficiently constructed using a [T satisfiability solver T] . We present a computer program that contains [T multiple representations T] of commonsense knowledge , takes a narrative as input , transforms the narrative and representations of commonsense knowledge into a [T satisfiability T] problem , runs a satisfiability solver , and produces models of the story as output . The narrative , models , and representations are expressed in the language of Shanahan 's event calculus . </abstract>
<abstract> [T Alignment-Based Discriminative T] [A String Similarity A] . A character-based measure of similarity is an important component of many natural language processing systems , including approaches to transliteration , coreference , word alignment , spelling correction , and the identi cation of cognates in related vocabularies . We propose an [T alignment-based discriminative T] framework for [A string similarity A] . We gather features from substring pairs consistent with a [T character-based alignment T] of the two strings . This approach achieves exceptional performance ; on nine separate cognate identi cation experiments using six language pairs , we more than double the precision of traditional orthographic measures like Longest Common Subsequence Ratio and Dice 's Coef cient . We also show strong improvements over other recent discriminative and heuristic similarity functions . </abstract>
<abstract> Learning Where to Look : Modeling [A Eye Movements in Reading A] . We propose a novel [T machine learning T] task that consists in learning to predict which words in a text are  [A fixated by a reader A] . In a first pilot experiment , we show that it is possible to outperform a majority baseline using a [T transitionbased T] model with a [T logistic regression classifier T] and a very limited set of features . We also show that the model is capable of capturing frequency effects on [A eye movements A] observed in human readers . </abstract>
<abstract> Incorporating [T Topic Information T] Into [A Semantic Analysis A] Models . This paper reports experiments in [A classifying texts A] based upon their favorability towards the subject of the text using a feature set enriched with [T topic T] information on a small dataset of music reviews hand-annotated for topic . The results of these experiments suggest ways in which incorporating [T topic T] information into such models may yield improvement over models which do not use topic information . </abstract>
<abstract> Incorporating [T Temporal T] and [T Semantic T] Information with Eye Gaze for Automatic [A Word Acquisition in Multimodal Conversational A] Systems . One major bottleneck in [A conversational A] systems is their incapability in interpreting unexpected user language inputs such as out-ofvocabulary words . To overcome this problem , [A conversational A] systems must be able to learn new words automatically during human machine conversation . Motivated by psycholinguistic findings on eye gaze and human language processing , we are developing techniques to incorporate human eye gaze for automatic [A word acquisition in multimodal conversational A] systems . This paper investigates the use of [T temporal alignment T] between speech and [T eye gaze T] and the use of [T domain knowledge T] in word acquisition . Our experiment results indicate that [T eye gaze T] provides a potential channel for automatically acquiring new words . The use of extra [T temporal and domain knowledge T] can significantly improve acquisition performance . </abstract>
<abstract> [T Bootstrapping T] [A Spoken Dialog A] Systems With Data Reuse . Building natural language spoken dialog systems requires large amounts of human transcribed and labeled speech utterances to reach useful operational service performances . Furthermore , the design of such complex systems consists of several manual steps . The User Experience ( UE ) expert analyzes and de nes by hand the system core functionalities : the system semantic scope ( call-types ) and the dialog manager strategy which will drive the human-machine interaction . This approach is extensive and error prone since it involves several non-trivial design decisions that can only be evaluated after the actual system deployment . Moreover , scalability is compromised by time , costs and the high level of UE know-how needed to reach a consistent design . We propose a novel approach for [T bootstrapping T] [A spoken dialog A] systems based on reuse of existing transcribed and labeled data , common reusable dialog [T templates and patterns T] , generic language and understanding models , and a consistent design process . We demonstrate that our approach reduces design and development time while providing an effective system without any application speci c data . </abstract>
<abstract> A Perspective On [A Word Sense Disambiguation A] Methods And Their Evaluation . In this position paper , we make several observations about the state of the art in automatic [A word sense disambiguation A] . Motivated by these observations , we offer several specific proposals to the community regarding improved evaluation criteria , common training and testing resources , and the definition of sense inventories . </abstract>
<abstract> [T Discriminative T] [A Word Alignment A] With [T Conditional Random Fields T] . In this paper we present a novel approach for inducing [A word alignments A] from sentence aligned data . We use a [T Conditional Random Field T] ( [T CRF T] ) , a [T discriminative T] model , which is estimated on a small supervised training set . The [T CRF T] is conditioned on both the source and target texts , and thus allows for the use of arbitrary and overlapping features over these data . Moreover , the [T CRF T] has efficient training and decoding processes which both find globally optimal solutions . We apply this [A alignment A] model to both French-English and Romanian-English language pairs . We show how a large number of highly predictive features can be easily incorporated into the [T CRF T] , and demonstratethatevenwithonlyafewhundred word-aligned training sentences , our model improves over the current state-ofthe-art with [A alignment A] error rates of 5.29 and 25.8 for the two tasks respectively . </abstract>
<abstract> [T Machine Learning T] Methods For Chinese [A Web Page Categorization A] . This paper reports our evaluation of [T k Nearest Neighbor T] ( [T kNN T] ) , [T Support Vector Machines T] ( [T SVM T] ) , and [T Adaptive Resonance Associative Map T] ( [T ARAM T] ) on Chinese [A web page classification A] . Benchmark experiments based on a Chinese web corpus showed that their predictive performance were roughly comparable although [T ARAM T] and [T kNN T] slightly outperformed [T SVM T] in small categories . In addition , inserting [T rules T] into [T ARAM T] helped to improve performance , especially for small welldefined categories . </abstract>
<abstract> Training a [T Perceptron with Global and Local Features T] for Chinese [A Word Segmentation A] . This paper proposes the use of [T global features T] for Chinese [A word segmentation A] . These [T global features T] are combined with [T local features T] using the [T averaged perceptron T] algorithm over N-best candidate word segmentations . The N-best candidates are produced using a [T conditional random field T] ( [T CRF T] ) [T character-based tagger T] for [A word segmentation A] . Our experiments show that by adding [T global features T] , performance is significantly improved compared to the character-based CRF tagger . Performance is also improved compared to using only [T local features T] . Our system obtains an F-score of 0.9355 on the CityU corpus , 0.9263 on the CKIP corpus , 0.9512 on the SXU corpus , 0.9296 on the NCC corpus and 0.9501 on the CTB corpus . All results are for the closed track in the fourth SIGHAN Chinese [A Word Segmentation A] Bakeoff . </abstract>
<abstract> The [T Hidden Information State T] [A Dialogue Manager A] : A Real-World [T POMDP-Based T] System . The [T Hidden Information State ( HIS ) T] [A Dialogue A] System is the first trainable and scalable implementation of a [A spoken dialog A] system based on the [T PartiallyObservable Markov-Decision-Process T] ( [T POMDP T] ) model of [A dialogue A] . The system responds to n-best output from the speech recogniser , maintains multiple concurrent dialogue state hypotheses , and provides a visual display showing how competing hypotheses are ranked . The demo is a prototype application for the Tourist Information Domain and achieved a task completion rate of over 90 % in a recent user study . </abstract>
<abstract> Learning a [A Compositional Semantic Parser A] using an Existing [T Syntactic Parser T] . We present a new approach to learning a [A semantic parser A] ( a system that maps natural language sentences into logical form ) . Unlikepreviousmethods , itexploitsanexisting [T syntactic parser T] to produce disambiguated [T parse trees T] that drive the [A compositional semantic A] interpretation . The resulting system produces improved results on standard corpora on natural language interfaces for [A database querying A] and simulated [A robot control A] . </abstract>
<abstract> Using a [T Dependency Parser T] to Improve [A SMT A] for Subject-Object-Verb Languages . We introduce a novel [T precedence reordering T] approach based on a [T dependency parser T] to [T statistical T] [A machine translation A] systems . Similar to other preprocessing reordering approaches , our method can efficiently incorporate [T linguistic knowledge T] into [A SMT A] systems without increasing the complexity of decoding . For a set of five subject-object-verb ( SOV ) order languages , we show significant improvements in BLEU scores when [A translating A] from English , compared to other reordering approaches , in state-of-the-art phrase-based SMT systems . </abstract>
<abstract> A [T Pragmatics-Based T] Approach To [A Ellipsis Resolution A] . [A Intersentential elliptical utterances A] occur frequently during [A information-seeking dialogs A] in task domains . This paper presents a [T pragmatics-based T] framework for interpreting such utterances . Discourse expectations and focusing heuristics are used to facilitate recognition of an information-seeker 's intent in uttering an elliptical fragment . The [A ellipsis A] is comprehended by identifying both the aspect of the information-seeker 's task-related plan highlighted by the fragment and the conversational discourse goal fulfilled by the utterance . The contribution of this approach is its consideration of [T pragmatic T] information , including [T discourse content T] and [T conversational goals T] , rather than just the precise representation of the preceding utterance . </abstract>
<abstract> [A Anaphora Resolution A] : A [T Multi-Strategy T] Approach . [A Anaphora resolution A] has proven to be a very difficult problem ; it requires the integrated application of syntactic , semantic , and pragmatic knowledge . This paper examines the hypothesis that instead of attempting to construct a monolithic method for [A resolving anaphora A] , the combination of [T multiple strategies T] , each exploiting a different knowledge source , proves more effective , theoretically and computationally . Cognitive plausibility is established in that human judgements of the optimal anaphoric referent accord with those of the [T strategy-based T] method , and human inability to determine a unique referent corresponds to the cases where different strategies offer conflicting candidates for the anaphoric referent . </abstract>
<abstract> [A Classifying Factored Genres A] with [T Part-of-Speech Histograms T] . This work addresses the problem of [A genre classification A] of text and speech transcripts , with the goal of handling genres not seen in training . Two frameworks employing different statistics on [T word\/POS histograms T] with a [T PCA transform T] are examined : a single model for each genre and a factored representation of genre . The impact of the two frameworks on the classification of training-matched and new [A genres A] is discussed . Results show that the factored models allow for a finer-grained representation of [A genre A] and can more accurately characterize [A genres A] not seen in training . </abstract>
<abstract> Using Natural Language Processing to [A Classify Suicide Notes A] . We hypothesize that [T machine-learning T] algorithms ( [T MLA T] ) can classify completer and simulated [A suicide notes A] as well as mental health professionals ( MHP ) . Five MHPs classified 66 simulated or completer notes ; [T MLAs T] were used for the same task . Results : MHPs were accurate 71 % of the time ; using the [T sequential minimization optimization T] algorithm ( [T SMO T] ) [T MLAs T] were accurate 78 % of the time . There was no significant difference between the MLA and MPH classifiers . This is an important first step in developing an [T evidence T] based [A suicide predictor A] for emergency department use . </abstract>
<abstract> [A Biomedical Text Retrieval A] In Languages With A Complex Morphology . [A Document retrieval A] in languages with a rich and complex morphology -- particularly in terms of derivation and ( single-word ) composition -- suffers from serious performance degradation with the stemming-only query-term-to-text-word matching paradigm . We propose an alternative approach in which [A morphologically A] complex word forms are segmented into relevant subwords ( such as [T stems T] , [T named entities T] , [T acronyms T] ) , and [T subwords T] constitute the basic unit for indexing and [A retrieval A] . We evaluate our approach on a large biomedical document collection . </abstract>
<abstract> Adaptive [T String Similarity T] Metrics For [A Biomedical Reference Resolution A] . In this paper we present the evaluation of a set of [T string similarity T] metrics used to [A resolve the mapping from strings to concepts A] in the UMLS MetaThesaurus . [T String similarity T] is conceived as a single component in a full [A Reference Resolution A] System that would resolve such a mapping . Given this qualification , we obtain positive results achieving 73.6 F-measure ( 76.1 precision and 71.4 recall ) for the task of assigning the correct UMLS concept to a given string . Our results demonstrate that [T adaptive string similarity T] methods based on [T Conditional Random Fields T] outperform standard metrics in this domain . </abstract>
<abstract> [A Part-Of-Speech Tagging A] Using [T Virtual Evidence And Negative Training T] . We present a [A part-of-speech tagger A] which introduces two new concepts : [T virtual evidence T] in the form of an observed child node , and [T negative training T] data to learn the conditional probabilities for the observed child . Associated with each word is a exible feature-set which can include binary ags , neighboring words , etc. . The conditional probability of Tag given Word + Features is implemented using a [T factored language-model with back-off T] to avoid data sparsity problems . This model remains within the framework of [T Dynamic Bayesian Networks T] ( [T DBNs T] ) and is conditionally-structured , but resolves the label bias problem inherent in the conditional Markov  model ( CMM ) . </abstract>
<abstract> [A Context Sensing A] Using [T Speech And Common Sense T] . We present a method of inferring aspects of a person 's [A context A] by capturing [T conversation topics T] and using [T prior knowledge T] of human behavior . This paper claims that topic-spotting performance can be improved by using a large database of [T common sense T] knowledge . We describe two systems we built to infer [A context from noisy transcriptions of spoken conversations A] using [T common sense T] , and detail some preliminary results . The GISTER system uses [T OMCSNet T] , a [T commonsense semantic network T] , to infer the most likely topics under discussion in a conversation stream . The OVERHEAR system is built on top of GISTER , and distinguishes between aspects of the conversation that refer to past , present , and future events by using [T LifeNet T] , a [T probabilistic graphical T] model of human behavior , to help infer the events that occurred in each of those three time periods . We conclude by discussing some of the future directions we may take this work . </abstract>
<abstract> Constructing [A Transliteration Lexicons A] From [T Web T] Corpora . This paper proposes a novel approach to automating the construction of [A transliterated-term lexicons A] . A simple [T syllable alignment T] algorithm is used to construct [T confusion matrices T] for cross-language syllable-phoneme conversion . Each row in the [T confusion matrix T] consists of a set of syllables in the source language that are ( correctly or erroneously ) matched [T phonetically T] and [T statistically T] to a syllable in the target language . Two conversions using [T phoneme-to-phoneme T] and [T text-to-phoneme syllabification T] algorithms are automatically deduced from a training corpus of paired terms and are used to calculate the degree of similarity between phonemes for [A transliterated-term A] extraction . In a large-scale experiment using this automated learning process for conversions , more than 200,000 [A transliterated-term A] pairs were successfully extracted by analyzing query results from Internet search engines . Experimental results indicate the proposed approach shows promise in [A transliterated-term A] extraction . </abstract>
<abstract> [A Reversible Sound-to-Letter\/Letter-to-Sound A] Modeling Based on [T Syllable Structure T] . This paper describes a new [A grapheme-tophoneme A] framework , based on a combination of formal [T linguistic T] and [T statistical T] methods . A [T context-free grammar T] is used to [T parse T] words into their underlying [T syllable structure T] , and a set of subword `` spellneme '' units encoding both phonemic and graphemic information can be automatically derived from the parsed words . A [T statistical a1 - gram T] model can then be trained on a large lexicon of words represented in terms of these linguistically motivated subword units . The framework has potential applications in modeling unknown words and in linking spoken spellings with spoken pronunciations for fully automatic new-word acquisition via dialogue interaction . Results are reported on [A sound-to-letter A] experiments for the nouns in the Phonebook corpus . </abstract>
<abstract> A [T Maximum-Entropy T] [A Partial Parser A] For Unrestricted Text . This paper describes a [A partial parser A] that assigns syntactic structures to sequences of partof-speech tags . The program uses the [T maximum entropy T] parameter estimation method , which Mlows a flexible combination of different knowledge sources : the [T hierarchical structure T] , [T parts of speech T] and [T phrasal categories T] . In effect , the [A parser A] goes beyond simple bracketing and recognizes even fairly complex structures . We give accuracy figures for different applications of the [A parser A] . </abstract>
<abstract> Recognizing [A Unregistered Names A] For Mandarin [A Word Identification A] . [A Word Identification A] has been an important and active issue in Chinese Natural Language Processing . In this paper , a new mechanism , based on the concept of [T sublanguage T] , is proposed for identifying [A unknown words A] , especially [A personal names A] , in Chinese newspapers . The proposed mechanism includes title . [T driven name recognition T] , [T adaptive dynamic word formation T] , identification of Z-character and 3-character Chinese names without title . We will show the e ~ : perimental results for two corpora and compare them with the results by the NTIIU 's statistic-based system , the only system that we know has attacked the same problem . The ezperimental results have shown significant improvements over the WI systems without the name identification capability . </abstract>
<abstract> [T  Pattern-Based Context-Free Grammars T] For [A Machine Translation A] . This paper proposes the use of [T `` patternbased '' context-free grammars T] as a basis for building [A machine translation A] ( [A MT A] ) systems , which are now being adopted as personal tools by a broad range of users in the cyberspace society . We discuss major requirements for such tools , including easy customization for diverse domains , the efficiency of the [A translation A] algorithm , and scalability ( incremental improvement in [A translation A] quality through user interaction ) , and describe how our approach meets these requirements . </abstract>
<abstract> Using [T Language Modeling T] to [A Select Useful Annotation Data A] . An [A annotation A] project typically has an abundant supply of unlabeled data that can be drawn from some corpus , but because the labeling process is expensive , it is helpful to pre-screen the pool of the candidate instances based on some criterion of future usefulness . In many cases , that criterion is to improve the presence of the rare classes in the data to be annotated . We propose a novel method for solving this problem and show that it compares favorably to a random sampling baseline and a clustering algorithm . </abstract>
<abstract> [A Clustering Hungarian Verbs A] on the Basis of [T Complementation Patterns T] . Our paper reports an attempt to apply an [T unsupervised clustering T] algorithm to a Hungarian treebank in order to obtain [A semantic verb classes A] . Starting from the hypothesis that semantic metapredicates underlie verbs ' syntactic realization , we investigate how one can obtain [A semantically motivated verb classes A] by automatic means . The 150 most frequent Hungarian verbs were clustered on the basis of their [T complementation patterns T] , yielding a set of basic classes and hints about the features that determine[A  verbal subcategorization A] . The resulting classes serve as a basis for the subsequent analysis of their alternation behavior . </abstract>
<abstract> Using [T Machine Translation Evaluation T] Techniques to Determine [A Sentence-level Semantic Equivalence A] . The task of [T machine translation ( MT ) evaluation T] is closely related to the task of [A sentence-level semantic equivalence classification A] . This paper investigates the utility of applying standard [T MT evaluation T] methods ( [T BLEU T] , [T NIST T] , [T WER T] and [T PER T] ) to building classifiers to predict [A semantic equivalence and entailment A] . We also introduce a novel classification method based on [T PER T] which leverages [T part of speech T] information of the words contributing to the word matches and non-matches in the sentence . Our results show that [T MT evaluation T] techniques are able to produce useful features for [A paraphrase classification A] and to a lesser extent [A entailment A] . Our technique gives a substantial improvement in [A paraphrase classification A] accuracy over all of the other models used in the experiments . </abstract>
<abstract> Automatic Learning Of [A Language Model Structure A] . [T Statistical T] [A language modeling A] remains a challenging task , in particular for morphologically rich languages . Recently , new approaches based on [T factored T] language models have been developed to address this problem . These models provide principled ways of including additional conditioning variables other than the preceding words , such as [T morphological T] or [T syntactic features T] . However , the number of possible choices for model parameters creates a large space of models that can not be searched exhaustively . This paper presents an entirely [T data-driven T] model selection procedure based on [T genetic search T] , which is shown to outperform both knowledge-based and random selection procedures on two di erent [A language modeling A] tasks ( Arabic and Turkish ) . </abstract>
<abstract> Learning [T Bayesian Networks T] for [A Semantic Frame Composition in a Spoken Dialog A] System . A [T stochastic T] approach based on [T Dynamic Bayesian Networks T] ( [T DBNs T] ) is introduced for [A spoken language understanding A] . [T DBN-based T] models allow to infer and then to compose [A semantic frame-based tree structures from speech transcriptions A] . Experimental results on the French MEDIA dialog corpus show the appropriateness of the technique which both lead to good tree identification results and can provide the dialog system with n-best lists of scored hypotheses . </abstract>
<abstract> [T Dependency-based T] [A Semantic Role Labeling A] of PropBank . We present a PropBank [A semantic role labeling A] system for English that is integrated with a [T dependency parser T] . To tackle the problem of joint syntactic -- semantic analysis , the system relies on a syntactic and a semantic subcomponent . The syntactic model is a [T projective parser T] using [T pseudo-projective transformations T] , and the semantic model uses [T global inference T] mechanisms on top of a pipeline of classifiers . The complete syntactic -- semantic output is selected from a candidate pool generated by the subsystems . We evaluate the system on the CoNLL2005 test sets using [T segment-based T] and [T dependency-based T] metrics . Using the [T segment-based T] CoNLL-2005 metric , our system achieves a near state-of-the-art F1 figure of 77.97 on the WSJ+B rown test set , or 78.84 if punctuation is treated consistently . Using a [T dependency-based T] metric , the F1 figure of our system is 84.29 on the test set from CoNLL-2008 . Our system is the first [T dependency-based T] [A semantic role labeler A] for PropBank that rivals constituent-based systems in terms of performance . </abstract>
<abstract> [A Machine Transliteration A] . It is challenging to translate names and technical terms across languages with different alphabets and sound inventories . These items are commonly transliterated , i.e. , replaced with approximate phonetic equivalents . For example , `` computer '' in English comes out as `` konpyuutaa '' in Japanese . Translating such items from Japanese back to English is even more challenging , and of practical interest , as transliterated items make up the bulk of text phrases not found in bilingual dictionaries . We describe and evaluate a method for performing[A  backwards transliterations A] by machine . This method uses a [T generative T] model , incorporating several distinct stages in the [A transliteration A] process . </abstract>
<abstract> Stabilizing [T Minimum Error Rate Training T] . The most commonly used method for training feature weights in [T  statistical T] [A machine translation ( SMT ) A] systems is Och 's [T minimum error rate training T] ( [T MERT T] ) procedure . Awell-knownproblemwithOch 's procedure is that it tends to be sensitive to small changes in the system , particularly when the number of features is large . In this paper , we quantify the stability of Och 's procedure by supplying different random seeds to a core component of the procedure ( Powell 's algorithm ) . We show that for systems with many features , there is extensive variation in outcomes , both on the development data and on the test data . Weanalyzethecausesofthisvariationand proposemodificationstotheMERTprocedure that improve stability while helping performance on test data . </abstract>
<abstract> A New Method for [A Sentiment Classification A] in Text Retrieval . Traditional text categorization is usually a topic-based task , but a subtle demand on information retrieval is to distinguish between positive and negative view on text topic . In this paper , a new method is explored to solve this problem . Firstly , a batch of [T Concerned Concepts T] in the researched domain is predefined . Secondly , the special knowledge representing the positive or negative [T  context T] of these concepts within sentences is built up . At last , an evaluating function based on the knowledge is defined for [A sentiment classification A] of free text . We introduce some [T linguistic knowledge T] in these procedures to make our method effective . As a result , the new method proves better compared with [T SVM T] when experimenting on Chinese texts about a certain topic . </abstract>
<abstract> [A Synonym Extraction A] Using A [T Semantic Distance On A Dictionary T] . [A Synonyms extraction A] is a difficult task to achieve and evaluate . Some studies have tried to exploit general dictionaries for that purpose , seeing them as graphs where words are related by the definition they appear in , in a complex network of an arguably semantic nature . The advantage of using a general [T dictionary T] lies in the coverage , and the availability of such resources , in general and also in specialised domains . We present here a method exploiting such a [T graph structure T] to compute a distance between words . This distance is used to isolate candidate [A synonyms A] for a given word . We present an evaluation of the relevance of the candidates on a sample of the lexicon . </abstract>
<abstract> [A Part-Of-Speech Tagging A] With [T Neural Networks T] . Text corpora which are tagged with [A part-of-speech A] information are useful in many areas of linguistic research . In this paper , a new [A part-of-speech tagging A] method hased on [T neural networks T] ( Net-Tagger ) is presented and its performance is compared to that of a llMM-tagger ( Cutting et al. , 1992 ) and a trigrambased tagger ( Kempe , 1993 ) . It is shown that the Net-Tagger performs as well as the trigram-based tagger and better than the iIMM-tagger . </abstract>
<abstract> [T  Unsupervised Learning  T] Of [A  Field Segmentation A] Models For [A Information Extraction A] . The applicability of many current [A information extraction A] techniques is severely limited by the need for supervised training data . We demonstrate that for certain [A field structured extraction A] tasks , such as classified advertisements and bibliographic citations , small amounts of prior knowledge can be used to learn effective models in a primarily unsupervised fashion . Although hidden Markov models ( HMMs ) provide a suitable generative model for field structured text , general unsupervised HMM learning fails to learn useful structure in either of our domains . However , one can dramatically improve the quality of the learned structure by exploiting simple prior knowledge of the desired solutions . In both domains , we found that [T unsupervised T] methods can attain accuracies with 400 unlabeled examples comparable to those attained by supervised methods on 50 labeled examples , and that [T semi-supervised T] methods can make good use of small amounts of labeled data . </abstract>
<abstract> Learning [T Extraction Patterns T] For [A Subjective Expressions A] . This paper presents a [T bootstrapping T] process that learns linguistically rich [T extraction patterns T] for [A subjective ( opinionated ) expressions A] . High-precision classifiers label unannotated data to automatically create a large training set , which is then given to an [T extraction pattern learning T] algorithm . The [T learned patterns T] are then used to identify more [A subjective sentences A] . The [T bootstrapping T] process learns many subjective [T patterns T] and increases recall while maintaining high precision . </abstract>
<abstract> [A Evaluation Of Semantic Clusters A] . [A Semantic clusters A] of a domain form an important feature that can be useful for performing syntactic and semantic disambiguation . Several attempts have been made to extract the semantic clusters of a domain by probabilistic or taxonomic techniques . However , not much progress has been made in evaluating the obtained semantic clusters . This paper focuses on an [A evaluation A] mechanism that can be used to [A evaluate semantic clusters A] produced by a system against those provided by human experts . </abstract>
<abstract> An Efficient [T Kernel T] for [A Multilingual Generation in Speech-to-Speech Dialogue Translation A] . We present core aspects of a fully implemented [A generation component in a multilingual speechto-speech dialogue translation A] system . Its design was particularly influenced by the necessity of real-time processing and usability for multiple languages and domains . We developed a general [T kernel T] system comprising a [T microplanning T] and a [T syntactic realizer T] module . Tile microplanner performs [T lexical and syntactic choice T] , based on [T constraint-satisfaction T] techniques . The [T syntactic realizer T] processes [T HPSG grammars T] reflecting the latest developments of the underlying linguistic theory , utilizing their pre-processing into the TAG formalism . The declarative nature of the knowledge bases , i.e. , the [T microplanning constraints T] and the [T HPSG grammars T] allowed an easy adaption to new domains and languages . The successful integration of our component into the translation system Verbmobil proved the fulfillment of the specific real-time constraints . </abstract>
<abstract> [T Unsupervised Multilingual Learning T] for [A Morphological Segmentation A] . For centuries , the deep connection between languages has brought about major discoveries about human communication . In this paper we investigate how this powerful source of information can be exploited for unsupervised language learning . In particular , we study the task of [A morphological segmentation A] of multiple languages . We present a [T nonparametric Bayesian T] model that jointly induces [A morpheme segmentations A] of each language under consideration and at the same time identifies cross-lingual morpheme patterns , or abstract morphemes . We apply our modeltothreeSemiticlanguages : Arabic , Hebrew , Aramaic , as well as to English . Our results demonstrate that learning [A morphological A] models in tandem reduces error by up to 24 % relative to monolingual models . Furthermore , we provide evidence that our joint model achieves better performance when applied to languages from the same family . </abstract>
<abstract> [T Variational Decoding T] for [T Statistical  T][A Machine Translation A] . [T Statistical T] models in [A machine translation A] exhibit spurious ambiguity . That is , the probability of an output string is split among many distinct derivations ( e.g. , trees or segmentations ) . In principle , the goodness of a string is measured by the total probability of its many derivations . However , finding the best string ( e.g. , during decoding ) is then computationally intractable . Therefore , most systems use a simple Viterbi approximation that measures the goodness of a string using only its most probable derivation . Instead , we develop a [T variational approximation T] , which considers all the derivations but still allows tractable decoding . Our particular [T variational distributions T] are parameterized as [T n-gram T] models . We also analytically show that interpolating thesen-gram models for different n is similar to minimumrisk decoding for BLEU ( Tromble et al. , 2008 ) . Experiments show that our approach improves the state of the art . </abstract>
<abstract> [T Finite-State T] [A Morphological A] Analysis Of Persian . This paper describes a two-level [A morphological A] analyzer for Persian using a system based on the [T Xerox finite state T] tools . Persian language presents certain challenges to computational analysis : There is a complex verbal conjugation paradigm which includes long-distance morphological dependencies ; phonological alternations apply at morpheme boundaries ; word and noun phrase boundaries are difficult to define since morphemes may be detached from their stems and distinct words can appear without an intervening space . In this work , we develop these problems and provide solutions in a [T finitestate T] [A morphology A] system . </abstract>
<abstract> [T Decision Tree Learning T] Algorithm With [T Structured Attributes T] : Application To [A Verbal Case Frame Acquisition A] . The [T Decision Tree Learning T] Algorithms ( [T DTLAs T] ) are getting keen attention from the natural language processing research comlnunity , and there have been a series of attempts to apply them to [A verbal case frame acquisition A] . However , a [T DTLA T] can not handle structured attributes like nouns , which are classified under a thesaurus . In this paper , we present a new [T DTLA T] that can rationally handle the structured attributes . In the process of tree generation , the algorithm generalizes each attribute optimally using a given [T thesaurus T] . We apply this algorithm to a bilingual corpus and show that it successfiflly learned a [T generalized decision tree T] for [A classifying the verb A] `` take '' and that the tree was smaller with more prediction power on the open data than the tree learned by the conventional DTLA . </abstract>
<abstract> [T Reordering Constraints T] For Phrase-Based Statistical [A Machine Translation A] . In statistical  [A machine translation A] , the generation of a translation hypothesis is computationally expensive . If arbitrary reorderings are permitted , the search problem is NP-hard . On the other hand , if we restrict the possible reorderings in an appropriate way , we obtain a polynomial-time search algorithm . We investigate different [T reordering constraints T] for phrase-based statistical [A machine translation A] , namely the [T IBM constraints T] and the [T ITG constraints T] . We present efficient [T dynamic programming T] algorithms for both constraints . We evaluate the constraints with respect to [A translation A] quality on two Japanese -- English tasks . We show that the [T reordering constraints T] improve [A translation A] quality compared to an unconstrained search that permits arbitrary [T phrase reorderings T] . The [T ITG constraints T] preform best on both tasks and yield statistically significant improvements compared to the unconstrained search . </abstract>
<abstract> Efficient [A Multilingual Phoneme-To-Grapheme Conversion A] Based On [T HMM T] . Grapheme-to-phoneme conversion ( GTPC ) has been achieved in most European languagesby dictionary look-up or using rules . The application of these methods , however , in the reverse process , ( i.e. , in [A phoneme-to-grapheme conversion A] ( [A PTGC A] ) ) creates serious problems , especially in inflectionally rich languages . In this paper the [A PTGC A] problem is approached from a completely different point of view . Instead of rules or a dictionary , the statistics of language connecting pronunciation to spelling are exploited . The novelty lies in modeling the natural language [T intraword features T] using the theory of [T hidden Markov models T] ( [T HMM T] ) and performing the conversion using the [T Viterbi T] algorithm . The [A PTGC A] system has been established and tested on various multilingual corpora . Initially , the first-order [T HMM T] and the common [T Viterbi T] algorithm were used to obtain a single transcription for each word . Afterwards , the second-order [T HMM T] and the [T N-best T] algorithm adapted to [A PTGC A] were implemented to provide one or more transcriptions for each word input ( homophones ) . This system gave an average score of more than 99 % correctly transcribed words ( overall success in the first four candidates ) for most of the seven languages it was tested on ( Dutch , English , French , German , Greek , Italian , and Spanish ) . The system can be adapted to almost any language with little effort and can be implemented in hardware to serve in real-time speech recognition systems . </abstract>
<abstract> A [T Hybrid T] Approach For The Acquisition Of [A Information Extraction Patterns A] . In this paper we present a [T hybrid T] approach for the acquisition of [A syntacticosemantic patterns A] from raw text . Our approach [T co-trains a decision list learner T] whose feature space covers the set of all [A syntactico-semantic patterns A] with an [T Expectation Maximization clustering T] algorithm that uses the text words as attributes . We show that the combination of the two methods always outperforms the [T decision list learner T] alone . Furthermore , using a [T modular architecture T] we investigate several algorithms for [A pattern ranking A] , the most important component of the [T decision list learner T] . </abstract>
<abstract> Automatic [A Slide Generation A] Based on [T Discourse Structure Analysis T] . In this paper , we describe a method of automatically [A generating summary slides A] from a text . The slides are generated by itemizing [T topic\/non-topic T] parts that are extracted from the text based on [T syntactic\/case T] analysis . The indentations of the items are controlled according to the [T discourse structure T] , which is detected by [T cue phrases T] , identi cation of [T word chain T] and similarity between two sentences . Our experiments demonstrates generated slides are far easier to read in comparison with original texts . </abstract>
